{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb0c7aa",
   "metadata": {},
   "source": [
    "### Load multiple PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "35ab3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "70ffbd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Specify folder containing PDFs\n",
    "folder_path = r\"C:\\Projects\\Agentic_AI_2.0_Course\\AGENTIC_AI_2.0\\2-Langchain_Basics\\2.6.Assignment\\Data\"\n",
    "\n",
    "chunks =[]\n",
    "# Iterate over PDF files and load them\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, file_name)\n",
    "        file_path = pdf_path\n",
    "        \n",
    "        chunk = partition_pdf(\n",
    "        filename=file_path,\n",
    "        infer_table_structure=True,            # extract tables\n",
    "        strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "        extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "        # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "        extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "        chunking_strategy=\"by_title\",          # or 'basic'\n",
    "        max_characters=10000,                  # defaults to 500\n",
    "        combine_text_under_n_chars=2000,       # defaults to 0\n",
    "        new_after_n_chars=6000,\n",
    "\n",
    "        # extract_images_in_pdf=True,          # deprecated\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6189094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\"}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get 2 types of elements from the partition_pdf function\n",
    "set([str(type(el)) for el in chunks[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4b998395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6bea9665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x2c7dadc58d0>,\n",
       " <unstructured.documents.elements.Title at 0x2c7dadc6d10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc6050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc49d0>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e9042950>,\n",
       " <unstructured.documents.elements.Text at 0x2c7e90428c0>,\n",
       " <unstructured.documents.elements.Text at 0x2c7e9042c50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc7910>,\n",
       " <unstructured.documents.elements.Text at 0x2c7e9042680>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e9042650>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc6170>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc71f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc6e30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc60e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc5ba0>,\n",
       " <unstructured.documents.elements.Header at 0x2c7dadc7670>,\n",
       " <unstructured.documents.elements.Title at 0x2c7dadc7f10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc5ea0>,\n",
       " <unstructured.documents.elements.Image at 0x2c7dadc51e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc7310>,\n",
       " <unstructured.documents.elements.Image at 0x2c7dadc5750>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e9042410>,\n",
       " <unstructured.documents.elements.Title at 0x2c7dadc40a0>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e9043d00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7e9043940>,\n",
       " <unstructured.documents.elements.Image at 0x2c7e9043580>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e90431c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7e9042e00>,\n",
       " <unstructured.documents.elements.Title at 0x2c7e9042740>,\n",
       " <unstructured.documents.elements.EmailAddress at 0x2c7e9040a90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc62f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7dadc4280>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7e90439d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7e9042d40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c7e9042080>]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all related documents that are associated to this chunk\n",
    "chunks[0][0].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e819dde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CompositeElement',\n",
       " 'element_id': '12632682f1472167ce1c81a2fdef8ed0',\n",
       " 'text': \"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nResearchGate\\n\\nInternational Journal of Computer Engineering and Technology (IJCET)\\n\\nInternational Journal of Computer Engineering and Technology (IJCET) Volume 16, Issue 1, Jan-Feb 2025, pp. 862-880, Article ID: IJCET_16_01_069 Available online at https://iaeme.com/Home/issue/IJCET?Volume=16&Issue=1 ISSN Print: 0976-6367; ISSN Online: 0976-6375; Journal ID: 5751-5249 Impact Factor (2025): 18.59 (Based on Google Scholar Citation) DOI: https://doi.org/10.34218/IJCET_16_01_069 OPEN © IAEME Publication &\\n\\n© IAEME Publication\\n\\nACCESS\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION- MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nPanneer Selvam Viswanathan\\n\\n[24]7.ai, USA.\\n\\nAGENTIC Al A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMSIN ARTIFICIAL INTELLIGENCE\\n\\nABSTRACT\\n\\nAgentic AI represents a paradigm shift in artificial intelligence systems characterized by autonomous decision-making capabilities and adaptive problem-\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nsolving mechanisms. This article comprehensively analyzes Agentic AI, examining its foundational architecture, core capabilities, and cross-industry applications. The article explores how these systems transcend traditional AI frameworks through their ability to operate independently, set goals, and adapt to complex environments without constant human intervention. The article investigates the implementation of Agentic AI across multiple sectors, including robotics, healthcare, autonomous vehicles, and financial services, while addressing the technical challenges and ethical considerations inherent in autonomous systems. The article encompasses the critical aspects of accountability, bias mitigation, and human-AI collaboration, providing insights into the potential implications for future technological development. The findings suggest that Agentic Al's evolution necessitates a balanced approach between technological advancement and ethical governance, highlighting the importance of establishing robust frameworks for responsible deployment. This article contributes to the growing literature on autonomous AI systems and provides a foundation for understanding their role in shaping future technological landscapes.\\n\\nKeywords: Agentic Artificial Intelligence, Autonomous Decision-Making, Machine Learning Systems, Human-AI Collaboration, Adaptive Intelligence.\\n\\nCite this Article: Panneer Selvam Viswanathan. Agentic Al: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence. International Journal of Computer Engineering and Technology (IJCET), 16(1), 2025, 862-880.\\n\\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_16 ISSUE_1/IJCET_16_01_069.pdf\",\n",
       " 'metadata': {'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'last_modified': '2025-06-02T21:54:41',\n",
       "  'page_number': 1,\n",
       "  'orig_elements': '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',\n",
       "  'file_directory': 'C:\\\\Projects\\\\Agentic_AI_2.0_Course\\\\AGENTIC_AI_2.0\\\\2-Langchain_Basics\\\\2.6.Assignment\\\\Data',\n",
       "  'filename': 'Agentic_AI_Autonomous_DecisionMaking.pdf'}}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8768f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Image',\n",
       " 'element_id': '48af9111-93ee-4a79-86b7-121d4e71ebe0',\n",
       " 'text': 'Ts',\n",
       " 'metadata': {'detection_class_prob': 0.39658135175704956,\n",
       "  'coordinates': {'points': ((np.float64(338.8556213378906),\n",
       "     np.float64(1384.396240234375)),\n",
       "    (np.float64(338.8556213378906), np.float64(1489.6591796875)),\n",
       "    (np.float64(476.1963195800781), np.float64(1489.6591796875)),\n",
       "    (np.float64(476.1963195800781), np.float64(1384.396240234375))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1656,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2025-06-02T21:57:35',\n",
       "  'filetype': 'PPM',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 1,\n",
       "  'image_base64': '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',\n",
       "  'image_mime_type': 'image/jpeg'}}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what an extracted image looks like.\n",
    "# It contains the base64 representation only because we set the param extract_image_block_to_payload=True\n",
    "\n",
    "elements = chunks[3][0].metadata.orig_elements\n",
    "chunk_images = [el for el in elements if 'Image' in str(type(el))]\n",
    "chunk_images[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174061ac",
   "metadata": {},
   "source": [
    "### Separate extracted elements into tables, text, and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aad30",
   "metadata": {},
   "source": [
    "##### separate tables from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "754e1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    for chunk in chunks[i]:\n",
    "        if \"Table\" in str(type(chunk)):\n",
    "            tables.append(chunk)\n",
    "\n",
    "        if \"CompositeElement\" in str(type((chunk))):\n",
    "            texts.append(chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ac4a367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables # no tables captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "84a0ed04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e11e5f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "74579459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78a530",
   "metadata": {},
   "source": [
    "#### Get the images from the CompositeElement objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2f4df868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for i in range(len(chunks)):\n",
    "        for chunk in chunks[i]:\n",
    "            if \"CompositeElement\" in str(type(chunk)):\n",
    "                chunk_els = chunk.metadata.orig_elements\n",
    "                for el in chunk_els:\n",
    "                    if \"Image\" in str(type(el)):\n",
    "                        images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59f5c9",
   "metadata": {},
   "source": [
    "#### Check what the images look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3a6a5f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "65708fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c0599c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a3e16",
   "metadata": {},
   "source": [
    "# Semantic Chunking of Text\n",
    "\n",
    " Reference : https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e8d4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nResearchGate\\n\\nInternational Journal of Computer Engineering and Technology (IJCET)\\n\\nInternational Journal of Computer Engineering and Technology (IJCET) Volume 16, Issue 1, Jan-Feb 2025, pp. 862-880, Article ID: IJCET_16_01_069 Available online at https://iaeme.com/Home/issue/IJCET?Volume=16&Issue=1 ISSN Print: 0976-6367; ISSN Online: 0976-6375; Journal ID: 5751-5249 Impact Factor (2025): 18.59 (Based on Google Scholar Citation) DOI: https://doi.org/10.34218/IJCET_16_01_069 OPEN © IAEME Publication &\\n\\n© IAEME Publication\\n\\nACCESS\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION- MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nPanneer Selvam Viswanathan\\n\\n[24]7.ai, USA.\\n\\nAGENTIC Al A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMSIN ARTIFICIAL INTELLIGENCE\\n\\nABSTRACT\\n\\nAgentic AI represents a paradigm shift in artificial intelligence systems characterized by autonomous decision-making capabilities and adaptive problem-\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nsolving mechanisms. This article comprehensively analyzes Agentic AI, examining its foundational architecture, core capabilities, and cross-industry applications. The article explores how these systems transcend traditional AI frameworks through their ability to operate independently, set goals, and adapt to complex environments without constant human intervention. The article investigates the implementation of Agentic AI across multiple sectors, including robotics, healthcare, autonomous vehicles, and financial services, while addressing the technical challenges and ethical considerations inherent in autonomous systems. The article encompasses the critical aspects of accountability, bias mitigation, and human-AI collaboration, providing insights into the potential implications for future technological development. The findings suggest that Agentic Al's evolution necessitates a balanced approach between technological advancement and ethical governance, highlighting the importance of establishing robust frameworks for responsible deployment. This article contributes to the growing literature on autonomous AI systems and provides a foundation for understanding their role in shaping future technological landscapes.\\n\\nKeywords: Agentic Artificial Intelligence, Autonomous Decision-Making, Machine Learning Systems, Human-AI Collaboration, Adaptive Intelligence.\\n\\nCite this Article: Panneer Selvam Viswanathan. Agentic Al: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence. International Journal of Computer Engineering and Technology (IJCET), 16(1), 2025, 862-880.\\n\\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_16 ISSUE_1/IJCET_16_01_069.pdf\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "389fe043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts[9].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92004a62",
   "metadata": {},
   "source": [
    "### Club all the text from texts into a single list of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f21de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data =[]\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    text_data.append(texts[i].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string= \" \".join(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eb14a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nResearchGate\\n\\nInternational Journal of Computer Engineering and Technology (IJCET)\\n\\nInternational Journal of Computer Engineering and Technology (IJCET) Volume 16, Issue 1, Jan-Feb 2025, pp. 862-880, Article ID: IJCET_16_01_069 Available online at https://iaeme.com/Home/issue/IJCET?Volume=16&Issue=1 ISSN Print: 0976-6367; ISSN Online: 0976-6375; Journal ID: 5751-5249 Impact Factor (2025): 18.59 (Based on Google Scholar Citation) DOI: https://doi.org/10.34218/IJCET_16_01_069 OPEN © IAEME Publication &\\n\\n© IAEME Publication\\n\\nACCESS\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION- MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nPanneer Selvam Viswanathan\\n\\n[24]7.ai, USA.\\n\\nAGENTIC Al A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMSIN ARTIFICIAL INTELLIGENCE\\n\\nABSTRACT\\n\\nAgentic AI represents a paradigm shift in artificial intelligence systems characterized by autonomous decision-making capabilities and adaptive problem-\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nsolving mechanisms. This article comprehensively analyzes Agentic AI, examining its foundational architecture, core capabilities, and cross-industry applications. The article explores how these systems transcend traditional AI frameworks through their ability to operate independently, set goals, and adapt to complex environments without constant human intervention. The article investigates the implementation of Agentic AI across multiple sectors, including robotics, healthcare, autonomous vehicles, and financial services, while addressing the technical challenges and ethical considerations inherent in autonomous systems. The article encompasses the critical aspects of accountability, bias mitigation, and human-AI collaboration, providing insights into the potential implications for future technological development. The findings suggest that Agentic Al\\'s evolution necessitates a balanced approach between technological advancement and ethical governance, highlighting the importance of establishing robust frameworks for responsible deployment. This article contributes to the growing literature on autonomous AI systems and provides a foundation for understanding their role in shaping future technological landscapes.\\n\\nKeywords: Agentic Artificial Intelligence, Autonomous Decision-Making, Machine Learning Systems, Human-AI Collaboration, Adaptive Intelligence.\\n\\nCite this Article: Panneer Selvam Viswanathan. Agentic Al: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence. International Journal of Computer Engineering and Technology (IJCET), 16(1), 2025, 862-880.\\n\\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_16 ISSUE_1/IJCET_16_01_069.pdf I. Introduction\\n\\nA. Background and Context of AI Evolution\\n\\nThe evolution of artificial intelligence has witnessed transformative shifts from rule- based systems to increasingly sophisticated architectures capable of complex cognitive tasks. Over the past decade, this progression has accelerated dramatically, moving from narrow AI applications to systems exhibiting broader capabilities [1]. Within this evolutionary spectrum, the emergence of Agentic AI marks a significant departure from traditional AI frameworks, representing a new paradigm in autonomous system development that focuses on realizing practical potential through structured engineering approaches.\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nB. Definition of Agentic AI\\n\\nAgentic AI can be defined as an advanced form of artificial intelligence that demonstrates autonomous decision-making capabilities, independent goal-setting mechanisms, and adaptive problem-solving behaviors without continuous human intervention. Unlike conventional AI systems that operate within predefined parameters, Agentic AI exhibits a degree of autonomy that enables it to respond dynamically to changing environments and complex scenarios [2]. This autonomy is characterized by the system\\'s ability to perceive its environment, process information, make independent decisions, and execute actions while aligning with its programmed objectives.\\n\\nC. Significance and Relevance in Current Technological Landscape\\n\\nThe significance of Agentic AI in the current technological landscape extends beyond theoretical advancement, representing a crucial step toward truly autonomous systems. As industries increasingly demand intelligent solutions capable of handling complex, real-world scenarios, Agentic AI offers a promising framework for developing systems operating with minimal human oversight while maintaining reliability and effectiveness. This capability has particular relevance in sectors requiring rapid decision-making, continuous adaptation, and autonomous operation, such as automated manufacturing, healthcare diagnostics, financial trading, and autonomous transportation systems [1]. D. Research Objectives and Scope\\n\\nThis research aims to comprehensively analyze Agentic AI systems, examining their architectural foundations, implementation challenges, and practical applications across various industries. The scope encompasses theoretical frameworks and practical implementations, focusing on the technical requirements for achieving meaningful autonomy in AI systems, the ethical considerations surrounding autonomous decision-making, and the potential implications for human-AI collaboration. Additionally, this study explores the challenges and opportunities presented by Agentic AI, including issues of accountability, safety, and system reliability.\\n\\nII. Theoretical Framework\\n\\nThe theoretical underpinnings of Agentic AI represent a complex interplay between autonomous decision-making capabilities, learning mechanisms, and architectural principles that collectively define a new paradigm in artificial intelligence systems.\\n\\nhttps://iaeme.com/Home/journal/IJCET 864\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nA. Foundational Principles of Agentic AI\\n\\nThe core principles of Agentic AI emerge from the convergence of multiple disciplines, including computer science, cognitive psychology, and systems theory, forming a comprehensive framework for autonomous system development.\\n\\n1. Autonomous Decision-Making Capabilities\\n\\nAgentic AI systems exemplify a revolutionary approach to autonomous decision- making, transcending traditional algorithmic constraints. These systems incorporate sophisticated neural architectures that enable them to process and synthesize complex information streams in real-time [3]. The decision-making framework operates on multiple levels of abstraction, from low-level reactive responses to high-level strategic planning, enabling systems to navigate complex scenarios while maintaining operational coherence. This multi-tiered approach allows for both rapid response to immediate stimuli and thoughtful consideration of long-term consequences, mirroring human cognitive processes in unprecedented ways. 2. Goal-Oriented Behavior\\n\\nThe goal-oriented architecture of Agentic AI represents a fundamental shift from reactive to proactive system behavior. These systems employ sophisticated objective hierarchies that enable them to decompose complex goals into manageable subtasks while aligning with overarching objectives [4]. The implementation includes dynamic goal prioritization mechanisms that adapt to changing circumstances while ensuring consistency with core directives. This sophisticated goal management system enables Agentic AI to demonstrate remarkable flexibility in pursuing objectives while maintaining operational coherence.\\n\\n3. Adaptive Learning Mechanisms\\n\\nAt the heart of Agentic AI lies a sophisticated learning architecture that combines multiple learning paradigms. The systems integrate supervised learning techniques with advanced reinforcement learning strategies, creating a hybrid approach that enables rapid adaptation to novel situations [3]. This learning framework is particularly evident in web navigation tasks, where systems must continuously update their understanding of dynamic environments. The adaptive mechanisms demonstrate remarkable capability in generalizing from experience, allowing systems to handle increasingly complex scenarios with growing proficiency.\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\n4. Distinction from Traditional AI Systems\\n\\nThe evolutionary leap from traditional AI to Agentic AI represents a fundamental shift in system capabilities and operational paradigms. While conventional AI systems operate within predetermined parameters, Agentic AI exhibits unprecedented levels of autonomy and adaptability [4]. This distinction manifests in the system\\'s ability to modify behavioral patterns dynamically, learn from novel experiences, and operate effectively in unstructured environments, all while maintaining alignment with intended purposes and ethical constraints.\\n\\nTable 1: Core Characteristics of Agentic AI [3, 4]\\n\\nCharacteristic Description Autonomous Decision- | Independent processing and choice execution without continuous Making human oversight Goal-Oriented Behavior Capability to set and pursue objectives with hierarchical planning Adaptive Learning Real-time learning and behavior modification based on experience Contextual Awareness Understanding and responding to environmental changes B. Technical Architecture\\n\\nThe technical architecture of Agentic AI systems represents a sophisticated integration of multiple technological components, each crucial for autonomous operation.\\n\\n1. Perception Systems\\n\\nModern Agentic AI systems employ advanced perception frameworks that simultaneously process and integrate multiple data streams. These systems utilize sophisticated sensor fusion algorithms to create comprehensive environmental models, enabling accurate situation assessment and response formulation. The perception layer incorporates immediate sensory data and historical context, enabling rich environmental understanding and supporting informed decision-making.\\n\\n2. Decision-Making Algorithms\\n\\nThe algorithmic foundation of Agentic AI represents a significant advancement in computational decision-making [3]. These systems employ layered decision architectures that combine classical reasoning with probabilistic inference, enabling nuanced response selection in complex scenarios. The decision-making framework incorporates fast, heuristic-based responses and deeper analytical processing, allowing for adaptable behavior across various operational contexts.\\n\\nhttps://iaeme.com/Home/journal/IJCET 866\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\n3. Action Execution Frameworks\\n\\nThe action execution component of Agentic AI systems demonstrates sophisticated control mechanisms that bridge the gap between decision-making and physical or virtual action implementation [4]. These frameworks ensure the precise translation of strategic decisions into tactical actions, maintaining system stability and operational effectiveness across diverse scenarios. The execution layer incorporates robust error handling and adaptive control mechanisms, ensuring reliable performance even in challenging conditions.\\n\\n4. Learning and Feedback Mechanisms\\n\\nThe learning architecture in Agentic AI systems represents a continuous improvement cycle that enables progressive enhancement of system capabilities. These systems collect and analyze performance data through sophisticated feedback mechanisms, updating internal models and decision-making processes to optimize future operations. This adaptive learning approach enables systems to evolve and improve, developing increasingly sophisticated responses to complex challenges. III. Core Capabilities and Implementation\\n\\nA. Autonomous Operation\\n\\nThe implementation of autonomous operation in Agentic AI systems represents a sophisticated integration of multiple technological components that work harmoniously to enable independent functioning and adaptive behavior.\\n\\n1. Independent Decision-Making Processes\\n\\nAgentic AI systems employ advanced decision-making architectures that combine probabilistic reasoning with deterministic rule sets to enable autonomous operation. These systems utilize multi-layered decision frameworks that process information through various stages of analysis, from rapid response mechanisms for immediate challenges to strategic planning capabilities for long-term objectives. The decision-making processes incorporate learned experiences and predefined constraints, enabling systems to make informed choices while maintaining operational boundaries [5].\\n\\n2. Environmental Perception and Analysis\\n\\nEnvironmental perception in Agentic AI systems involves sophisticated sensor fusion and data integration techniques that enable comprehensive situational awareness. These systems employ advanced processing algorithms to interpret complex environmental data\\n\\nhttps://iaeme.com/Home/journal/IJCET editor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nstreams, creating detailed environmental models that support informed decision-making. The analysis framework includes real-time processing capabilities that enable systems to identify relevant patterns and relationships within their operational context, particularly in IoT- integrated environments [5].\\n\\n3. Real-Time Adaptation Capabilities\\n\\nThe adaptive capabilities of Agentic AI systems extend beyond simple reactive responses to sophisticated real-time learning and adjustment mechanisms. These systems can modify their behavior patterns in response to changing environmental conditions, unexpected obstacles, or new requirements. The adaptation framework incorporates feedback loops that enable continuous performance optimization while maintaining operational stability. B. Machine Learning Integration\\n\\nIntegrating machine learning capabilities represents a fundamental aspect of Agentic AI systems, enabling sophisticated learning and adaptation mechanisms that support autonomous operation. 1. Reinforcement Learning Applications\\n\\nReinforcement learning is crucial in developing autonomous capabilities within Agentic AI systems. These applications enable systems to learn optimal behavior patterns through interaction with their environment. The reinforcement learning framework incorporates advanced reward mechanisms and policy optimization techniques that guide the system toward improved performance over time, particularly in complex IoT ecosystems [5].\\n\\n2. Neural Network Implementations\\n\\nThe neural network architectures employed in Agentic AI systems represent state-of- the-art implementations that enable sophisticated pattern recognition and decision-making capabilities. These implementations include deep learning models for complex pattern analysis and specialized network architectures for specific operational requirements. The neural network framework supports offline learning for baseline capability development and online learning for real-time adaptation.\\n\\n3. Data Processing and Pattern Recognition\\n\\nData processing and pattern recognition capabilities in Agentic AI systems involve sophisticated algorithms that enable systems to identify relevant patterns and relationships within complex data streams [5]. These capabilities support immediate operational decisions and long-term learning processes, enabling systems to develop increasingly sophisticated responses to environmental challenges, particularly in scenarios involving large-scale IoT sensor networks and distributed computing environments.\\n\\nhttps://iaeme.com/Home/journal/IJCET 868 editor@iaeme.com\\n\\nAgentic Al: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nKey Performance Metrics of Agentic Al Systems 100 94.8 92-1 93.4 85.2 88.9 90 80 70 60 50 40 30 20 10 Decision Accuracy Response Time Adaptation Rate System Reliability Traditional Al(%) WAgentic Al (%)\\n\\nFig. 1: Key Performance Metrics of Agentic AI Systems [7]\\n\\nIV. Applications Across Industries\\n\\nThe deployment of Agentic AI across various industries marks a transformative shift in how organizations approach automation, decision-making, and operational efficiency. This comprehensive exploration delves into the diverse applications and profound impact of Agentic Al across key industrial sectors. A. Robotics and Manufacturing\\n\\nIntegrating Agentic AI in manufacturing environments has catalyzed a fundamental transformation in industrial processes, marking the advent of truly intelligent manufacturing systems. The evolution of autonomous mobile robots (AMRs) represents a quantum leap in industrial automation capabilities [6]. These systems transcend traditional programmed movements by incorporating contextual awareness and adaptive decision-making. Modern AMRs demonstrate sophisticated capabilities in dynamic path planning, intelligent task prioritization, and adaptive response to environmental changes.\\n\\nImplementing Agentic AI in industrial automation has revolutionized manufacturing processes by introducing unprecedented levels of flexibility and intelligence [7]. Modern systems demonstrate self-optimizing production lines that adjust parameters based on real-time quality metrics while incorporating predictive maintenance capabilities that anticipate\\n\\nhttps://iaeme.com/Home/journal/IJCET 869\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nequipment failures before they occur. This proactive approach has significantly reduced downtime and improved overall operational efficiency.\\n\\nQuality control processes have evolved from reactive inspection to proactive quality management through Agentic AI integration [6]. Advanced systems now incorporate real-time defect detection with learning capabilities, enabling predictive quality analysis that prevents defects before they occur. This transformation has led to significant improvements in product quality and manufacturing consistency.\\n\\nB. Healthcare Systems\\n\\nIntegrating Agentic AI in healthcare represents a paradigm shift in patient care and medical decision-making. Smart medical devices powered by Agentic AI [7] revolutionize patient care through adaptive drug delivery systems and intelligent monitoring capabilities. These systems continuously analyze patient data to make real-time adjustments in treatment protocols, ensuring optimal therapeutic outcomes.\\n\\nDiagnostic systems enhanced by Agentic AI demonstrate unprecedented medical data analysis and pattern recognition capabilities. These systems process complex diagnostic datasets to identify subtle indicators of disease, enabling earlier intervention and more effective treatment planning. Integrating multiple data sources allows for more comprehensive and accurate diagnostic recommendations.\\n\\nPatient monitoring systems have evolved to provide continuous, intelligent oversight of patient conditions. Modern systems leverage Agentic AI to analyze vital signs and behavioral patterns, enabling predictive analytics that can identify potential health issues before they become critical. This proactive approach to patient care has significantly improved outcomes and reduced the burden on healthcare providers. C. Transportation and Logistics\\n\\nThe transportation sector has witnessed dramatic transformation through Agentic AI implementation. The evolution of autonomous vehicle technology [7] has led to sophisticated systems capable of navigating complex environments with minimal human intervention. These systems combine advanced perception capabilities with real-time decision-making algorithms to ensure safe and efficient operation in dynamic conditions.\\n\\nNavigation systems have evolved to incorporate real-time data analysis and predictive capabilities, enabling dynamic route optimization and proactive traffic management. These systems continuously analyze multiple data streams to identify optimal routes and adjust to changing conditions, significantly improving transportation efficiency.\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nTraffic management systems [6] have become increasingly sophisticated, incorporating predictive analytics and real-time optimization capabilities. These systems analyze traffic patterns and environmental conditions to make proactive adjustments that minimize congestion and improve overall system efficiency. Integrated emergency response coordination has further enhanced the capability to manage complex urban transportation networks.\\n\\nD. Financial Services\\n\\nThe financial sector has embraced Agentic AI to enhance decision-making and risk management capabilities. Modern algorithmic trading systems [7] incorporate sophisticated market analysis and pattern recognition capabilities, enabling real-time adaptation to market conditions. These systems process vast amounts of data to identify trading opportunities while maintaining strict risk management protocols.\\n\\nRisk assessment systems have evolved to provide comprehensive analysis across multiple dimensions, enabling more effective risk management and compliance monitoring. These systems continuously analyze market conditions and portfolio performance to identify potential risks before they materialize.\\n\\nPortfolio management systems powered by Agentic AI [6] demonstrate sophisticated asset allocation and strategy adaptation capabilities. These systems analyze market trends and risk factors to make autonomous investment decisions while aligning with specified investment objectives and constraints. The integration of predictive analytics has significantly improved the ability to anticipate market trends and adjust portfolios accordingly.\\n\\nTable 2: Industry-Specific Applications of Agentic AI [6, 7]\\n\\nIndustry Primary Applications Key Benefits Manufacturing | Autonomous robots, Quality control Enhanced efficiency, Reduced errors Healthcare Diagnostic systems, Patient Monitoring | Improved accuracy, 24/7 monitoring Transportation | Autonomous vehicles, Traffic | Reduced accidents, Optimized flow management Finance Algorithmic trading, Risk assessment Fast execution, Better risk management https://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nV. Ethical and Societal Implications\\n\\nA. Accountability Framework\\n\\nDeveloping and deploying Agentic AI systems necessitate robust accountability frameworks that address the complex interplay between autonomous decision-making and human oversight [8]. These frameworks must clearly delineate responsibilities while ensuring the transparent operation of AI systems.\\n\\n1. Legal Responsibilities\\n\\nEstablishing legal frameworks for Agentic AI systems presents unique challenges in determining liability and responsibility. Educational institutions and industry experts emphasize establishing clear legal guidelines for AI system deployment [8]. The legal framework must address questions of causation and responsibility in cases where autonomous decisions lead to unintended consequences.\\n\\n2. Decision-Making Transparency\\n\\nTransparency in Agentic AI decision-making processes is crucial for maintaining accountability and trust [9]. Systems must be designed with inherent explainability features that allow stakeholders to understand the reasoning behind autonomous decisions. This includes implementing mechanisms for tracking decision pathways and maintaining comprehensive audit trails of system actions.\\n\\n3. Risk Management\\n\\nComprehensive risk management strategies for Agentic AI systems must address technical and operational risks [8]. This includes developing robust testing frameworks, establishing clear operational boundaries, and implementing failsafe mechanisms that ensure system behavior remains within acceptable parameters.\\n\\nB. Societal Impact\\n\\n1. Workforce Transformation\\n\\nIntegrating Agentic AI systems catalyzes significant changes in workforce dynamics and job roles. Modern perspectives on AI integration emphasize the importance of proactive workforce development and adaptation strategies [9]. This transformation necessitates careful consideration of job displacement, skill requirements evolution, and the creation of new roles centered around AI system management and oversight. 2. Human-AlI Collaboration\\n\\nDeveloping effective human-AI collaboration frameworks represents a critical challenge in Agentic AI deployment [8]. These frameworks must balance autonomous\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\noperation with meaningful human oversight, ensuring productive interaction while maintaining appropriate control mechanisms.\\n\\n3. Social Acceptance and Trust\\n\\nBuilding and maintaining public trust in Agentic AI systems requires careful attention to transparency, reliability, and demonstrated value [9]. This includes clear communication about system capabilities and limitations and proactive engagement with stakeholder concerns about AI deployment in various societal contexts.\\n\\nC. Ethical Considerations\\n\\n1. Bias and Fairness\\n\\nAddressing bias in Agentic AI systems requires comprehensive approaches to ensure fair and equitable operation [8]. Contemporary ethical frameworks emphasize the importance of algorithmic fairness and the need for diverse representation in AI development and deployment processes.\\n\\n2. Privacy Concerns\\n\\nThe operation of Agentic AI systems often involves processing sensitive personal data, raising significant privacy concerns [9]. Modern approaches to AI ethics emphasize the importance of robust data protection frameworks while maintaining system effectiveness and user privacy.\\n\\n3. Security Implications\\n\\nSecurity considerations for Agentic AI systems extend beyond traditional cybersecurity, including protection against autonomous decision-making manipulation. Current ethical guidelines emphasize the need for comprehensive security frameworks that address the unique vulnerabilities of autonomous systems while ensuring system integrity and reliability [9].\\n\\nVI. Challenges and Limitations\\n\\nImplementing Agentic AI systems presents a complex array of challenges that span both technical and operational domains, requiring careful consideration and strategic planning for successful deployment. A. Technical Challenges\\n\\n1. System Reliability\\n\\nThe deployment of Agentic AI systems faces significant challenges in ensuring consistent and reliable operation across diverse scenarios [10]. Based on comprehensive Delphi\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nstudy findings, the complexity of autonomous decision-making processes introduces unique reliability concerns, particularly in mission-critical applications where system failures could have severe consequences. Modern implementations must address the fundamental challenge of maintaining operational stability while ensuring predictable behavior patterns. Developing effective fault detection and recovery mechanisms becomes crucial in maintaining system integrity across various operational conditions.\\n\\n2. Performance Optimization\\n\\nOptimizing the performance of Agentic AI systems requires addressing complex computational requirements while maintaining real-time response capabilities [11]. A systematic review of implementation challenges reveals that performance optimization must consider the intricate relationships between neural network architectures, decision-making algorithms, and system latency, particularly in logistics and operational environments. The challenge extends beyond basic functionality to encompass the delicate balance between processing power and energy efficiency.\\n\\n3. Integration Complexities\\n\\nIntegrating Agentic AI systems with existing infrastructure presents significant technical challenges beyond simple compatibility issues [10]. Research findings indicate that organizations struggle with seamless integration within complex technological ecosystems, often involving legacy systems and varying operational requirements. The complexity increases substantially when dealing with distributed architectures and heterogeneous systems, requiring sophisticated approaches to maintain system coherence.\\n\\nB. Implementation Barriers\\n\\n1. Regulatory Compliance\\n\\nMeeting regulatory requirements while maintaining system effectiveness represents a significant challenge in Agentic AI deployment [11]. Empirical studies in logistics operations demonstrate that organizations must navigate an increasingly complex regulatory landscape that varies across jurisdictions and industries. The dynamic nature of AI regulation requires systems to be designed with sufficient flexibility to adapt to evolving compliance requirements while maintaining operational effectiveness. 2. Infrastructure Requirements\\n\\nDeploying Agentic AI systems often necessitates significant infrastructure investments and modifications beyond initial implementation considerations [10]. Delphi\\'s study results highlight that organizations must comprehensively evaluate their existing infrastructure to determine necessary upgrades and modifications. This includes ensuring adequate\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\ncomputational resources, storage capabilities, and network infrastructure to support sophisticated AI operations.\\n\\n3. Cost Considerations\\n\\nThe financial implications of implementing Agentic AI systems represent a significant consideration that extends far beyond initial development costs [11]. Systematic review findings indicate that organizations must carefully evaluate the total cost of ownership, considering immediate implementation expenses and long-term operational costs. The challenge lies in developing sustainable implementation strategies that balance these costs against potential benefits while ensuring long-term viability.\\n\\nSuccessful implementation of Agentic AI systems requires a comprehensive understanding of these challenges and the development of strategic approaches to address them. Research findings emphasize that organizations must adopt holistic perspectives that consider both technical and operational aspects while maintaining a focus on long-term sustainability and effectiveness. Through careful planning and strategic implementation, these challenges can be effectively managed to realize the full potential of Agentic AI systems.\\n\\nVII. Future Directions and Research Opportunities\\n\\nA. Technological Advancements\\n\\n1. Enhanced Autonomy Capabilities\\n\\nThe future of Agentic AI systems points toward significantly enhanced autonomous capabilities that transcend current limitations [12]. Industry analysis strongly focuses on developing systems with improved decision-making capabilities in complex, unstructured environments. These advancements aim to enable AI systems to handle increasingly sophisticated tasks while maintaining reliability and safety. Current trends suggest breakthrough potential in self-learning and adaptive behaviors that could revolutionize how AI systems interact with their environments. 2. Improved Learning Algorithms\\n\\nResearch in learning algorithms shows promising developments in enhancing the adaptability and efficiency of Agentic AI systems [13]. According to industry experts, these advancements focus on developing more sophisticated approaches to knowledge acquisition and transfer, enabling systems to learn from smaller datasets while maintaining high accuracy.\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nIntegrating advanced neural network architectures and novel training methodologies suggests significant improvements in learning efficiency and generalization capabilities.\\n\\n3. Integration with Emerging Technologies\\n\\nThe convergence of Agentic AI with emerging technologies presents exciting opportunities for system enhancement [12]. Industry trends indicate significant potential in combining AI with quantum computing, advanced sensor technologies, and edge computing architectures. These integrations promise to overcome existing computational limitations while enabling new real-time processing and decision-making capabilities.\\n\\nB. Industry Applications\\n\\n1. New Use Cases\\n\\nExpanding Agentic AI into novel application domains presents significant opportunities for innovation and development [13]. Enterprise research indicates emerging applications in environmental monitoring, sustainable energy management, and advanced healthcare delivery. These new use cases demonstrate the versatility of Agentic AI systems and their potential to address complex societal challenges.\\n\\n2. Market Opportunities\\n\\nThe evolving landscape of AI applications reveals substantial market opportunities across various sectors [12]. Market analysis suggests a growing demand for specialized AI solutions in emerging markets and traditionally non-digital industries. The potential for market expansion is particularly evident in digital transformation sectors, where Agentic AI can provide significant competitive advantages.\\n\\n3. Innovation Potential\\n\\nThe potential for innovation within Agentic AI development continues to expand, with industry experts identifying numerous unexplored applications and capabilities [13]. Future developments suggest possibilities for breakthrough applications in areas such as personalized medicine, climate change mitigation, and space exploration. The convergence of multiple technological advances creates opportunities for revolutionary solutions to complex global challenges. https://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\nImprovement (%) - Operational Cost Reduction Customer Innovation Rate Time-to-Market Efficiency Satisfaction\\n\\nFig. 2: Impact of Agentic AI on Business Metrics [13]\\n\\nVIII. Future Research Directions\\n\\nThe field of Agentic AI presents numerous opportunities for future research and development that could significantly impact various domains. Leading technology companies predict these opportunities will span theoretical foundations, practical applications, and societal implications. The potential for breakthrough developments in autonomous capabilities and emerging technological integrations suggests a rich landscape for future innovation and advancement in this field.\\n\\nIX. Practical Implementation Considerations\\n\\nUnderstanding the practical implications of these future directions requires careful consideration of implementation challenges and opportunities. Industry insights suggest that successful adoption of advanced Agentic AI systems will depend on addressing current limitations while effectively leveraging emerging technologies. The evolution of implementation strategies must balance technological capabilities with practical considerations for deployment across different industries and applications.\\n\\nhttps://iaeme.com/Home/journal/IJCET\\n\\neditor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\nX. Conclusion\\n\\nThis comprehensive article of Agentic AI reveals its transformative potential across multiple domains while highlighting critical considerations for future development and implementation. The article demonstrates that Agentic AI represents a significant evolution in artificial intelligence, characterized by sophisticated autonomous decision-making capabilities, adaptive learning mechanisms, and broad applicability across industries. From manufacturing and healthcare to transportation and financial services, Agentic AI systems are revolutionizing traditional operational paradigms while introducing new possibilities for automation and optimization. However, successfully deploying these systems requires careful consideration of ethical implications, technical challenges, and implementation barriers. The article emphasizes the importance of developing robust accountability frameworks, addressing societal impacts, and ensuring appropriate governance structures. As the field continues to evolve, future directions indicate promising advancements in autonomous capabilities, learning algorithms, and integration with emerging technologies. The findings suggest that while Agentic AI presents significant challenges in system reliability, regulatory compliance, and cost considerations, its potential benefits in enhancing operational efficiency, decision-making processes, and innovation capabilities make it a crucial technology for future development. This article contributes to the growing body of knowledge on autonomous AI systems while providing valuable insights for practitioners, researchers, and policymakers involved in\\n\\ndeveloping and deploying Agentic AI technologies. References\\n\\n{1 Bosch, J., Holmstrém Olsson, H., Brinne, B., & Crnkovic, I. (2022). AI Engineering: Realizing the Potential of AL JEEE Software, 39(6), 23-27. https://ieeexplore.ieee.org/document/9928 1 96/citations#citations\\n\\n[2] IEEE Spectrum. \"Explainer: What Are AI Agents?\" IEEE Spectrum, 2024. [Online]. Available: _https://spectrum.ieee.org/ai-agents\\n\\n[3] Abuelsaad, T., Akkil, D., Dey, P., Jagmohan, A., & Vempaty, A. (2024). Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems. arXiv preprint arXiv:2407.13032.https://arxiv.org/abs/2407.13032\\n\\nhttps://iaeme.com/Home/journal/IJCET 878\\n\\neditor@iaeme.com\\n\\nAgentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\\n\\n[4] Martinez, D. R., & Kifle, B. M. (2024). Artificial Intelligence: A Systems Approach from Architecture Principles to Deployment. MIT Press eBooks, IEEE Xplore2. https://ieeexplore.ieee.org/book/10701030\\n\\nMohanarangan, S., Karthika, D., Moohambigai, B., & Sangeetha, R. (2024). Unleashing the Power of AI and Machine Learning: Integration Strategies for IoT Systems. International Journal of Scientific Research in Computer Science and Engineering, 12(2), 25-32.\\n\\nhttps://www.isroset.org/journal/IJSRCSE/full_paper_view.php?paper_id=3461\\n\\nRandieri, C. (2025, January 3). Agentic Al: A New Paradigm In Autonomous Artificial Intelligence. Forbes. Retrieved from\\n\\nhttps://www.forbes.com/councils/forbestechcouncil/2025/01/03/agentic-ai-a-new- paradigm-in-autonomous-artificial-intelligence/\\n\\n[7] Automation Anywhere. (n.d.). What is agentic AI? Key benefits & features. Retrieved from https://www.automationanywhere.com/rpa/agentic-ai\\n\\nThe Princeton Review. (2024). Ethical and Social Implications of AI Use. Retrieved from_https://www.princetonreview.com/ai-education/ethical-and-social-implications- of-ai-use\\n\\nStefanini. (2024). The Moral And Ethical Implications Of Artificial Intelligence. Retrieved from https://stefanini.com/en/insights/articles/the-moral-and-ethical- implications-of-artificial-intelligence\\n\\n[10] Bérubé, M., & Giannelia, T. (2021). Barriers to the Implementation of AI in Organizations: Findings from a Delphi Study. Proceedings of the 54th Hawaii International Conference on System Sciences. Retrieved from https://hdl.handle.net/10125/71425\\n\\n(11) Alsuwaidi, J., Aydin, R., & Rashid, H. (2022). Investigating Barriers and Challenges to Artificial Intelligence (AI) Implementation in Logistic Operations: A Systematic Review of Literature. 5th European International Conference on Industrial Engineering and Operations Management. Retrieved from https://index.ieomsociety.org/index.cfm/article/view/ID/10808\\n\\nhttps://iaeme.com/Home/journal/IJCET editor@iaeme.com\\n\\nPanneer Selvam Viswanathan\\n\\n[12] Rapid Canvas. (2024). Future Directions in AI Research: Trends and Predictions. Retrieved from__https://www.rapidcanvas.ai/blogs/future-directions-in-ai-research- trends-and-predictions\\n\\n[13] IBM. (2024). The Future of Artificial Intelligence. Retrieved from https://www.ibm.com/think/insights/artificial-intelligence-future\\n\\nCitation: Panneer Selvam Viswanathan. Agentic Al: A Comprehensive Framework for Autonomous Decision- Making Systems in Artificial Intelligence. International Journal of Computer Engineering and Technology (IJCET), 16(1), 2025, 862-880.\\n\\nAbstract Link: https://iaeme.com/Home/article_id/IJCET_16_01_069 Article Link:\\n\\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_16_ISSUE_1/IJCET_16_01_069.pdf\\n\\nCopyright: © 2025 Authors. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\\n\\nThis work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).\\n\\nM editor@iaeme.com\\n\\nhttps://iaeme.com/Home/journal/IJCET 880\\n\\neditor@iaeme.com Attention Is All You Need\\n\\nAshish Vaswani* Google Brain avaswani@google.com\\n\\nNoam Shazeer* Google Brain noam@google.com\\n\\nNiki Parmar* Google Research nikip@google.com\\n\\nJakob Uszkoreit* Google Research usz@google.com\\n\\nLlion Jones* Google Research llion@google.com\\n\\nAidan N. Gomez* ¢ University of Toronto aidan@cs.toronto.edu\\n\\nLukasz Kaiser* Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin*\\n\\n+\\n\\nillia.polosukhin@gmail.com\\n\\nAbstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29] [2] [5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures\\n\\n*Equal contribution. ing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n+ Work performed while at Google Brain.\\n\\n+Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h;, as a function of the previous hidden state h,_; and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2]{16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU (20}, ByteNet and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [II]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in sectionB.2]\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [L ] and {8}.\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure . Here, the encoder maps an input sequence of symbol representations (21, ...,%,,) to a sequence of continuous representations z = (z1,...,2,). Given z, the decoder then generates an output sequence (yj, ..., Ym) of symbols one element at a time. At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure[I] respectively. 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\n\\nOutput\\n\\nProbabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | -+CAgc8 Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, ee a, Positional Positional Encoding @ © @ Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\n\\nFigure 1: The Transformer - model architecture.\\n\\nwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization [I]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer() is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmode = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which per: orms multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less tl han 7.\\n\\n3.2. Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure [2). The input consists of queries and keys of dimension dx, and values of dimension d,. We compute the dot products of the\\n\\nScaled Dot-Product Attention Multi-Head Attention Linear\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nquery with all keys, divide each by \\\\/d;,, and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices kK and V. We compute the matrix of outputs as:\\n\\n. , QKT Attention(Q, K,V) = softmax(-7-)V (1) Ie\\n\\nThe two most commonly used attention functions are additive attention (2), and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of Te Additive attention computes the compatibility function using a feed-forward network with ke a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of dj, the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d;, [3]. We suspect that for large values of d;,, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients} To counteract this effect, we scale the dot products by TE:\\n\\n3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodei-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d;,, d; and d,, dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d,,-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure[2]\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\nMultiHead(Q, K,V) = Concat(head), ..., head, )W? where head; = Attention(QW2, KW, VW/’)\\n\\nWhere the projections are parameter matrices We € Rte, WH E RtwowXde, WV © Remote Xde and WO € Rl» xdosss,\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dy, = dy = dmodei/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3. Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\ne In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n\\ne The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\ne Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to —oo) all values in the input of the softmax which correspond to illegal connections. See Figure[2|\\n\\n3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0,2W, + b1)W2 + bo (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmoder = 512, and the inner-layer has dimensionality dy 5 = 2048.\\n\\n3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmode!. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by V/dmoael- 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n? - d) O(1) O(1) Recurrent O(n-d?) O(n) O(n) Convolutional O(k-n-d?) O(1) O(logx(n)) Self-Attention (restricted) O(r-n-d) ol) O(n/r)\\n\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE (pos,2i) = sin(pos/100007\"/ 4\") PE (pos,21-41) = €08(pos/ 100007#/4msaa )\\n\\nwhere pos is the position and 7 is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 27 to 10000 - 27. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+, can be represented as a linear function of PEpos-\\n\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table B] row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,%p) to another sequence of equal length (21,...,2,), with x;, 2; € R%, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table[]] a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (BI) and byte-pair representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n\\nthe input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log,(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k -n +d + n-d?). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [BI]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n5.2. Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3. Optimizer\\n\\nWe used the Adam optimizer with 3; = 0.9, 82 = 0.98 and € = 10~°. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = do8, : min(step_num~°°, step_num - warmup_steps~\\'°) (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nResidual Dropout We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Parop = 0.1.\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nBLEU Training Cost (FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet 23.75 Deep-Att + PosUnk [32] 39.2 1.0 - 107° GNMT + RL BI] 24.6 39.92 2.3-10!9 1.4-1070 ConvS2S 25.16 40.46 9.6-10\\'% 1.5-1070 MoE 26.03 40.56 2.0-10\\'9 1.2. 1079 Deep-Att + PosUnk Ensemble 40.4 8.0 - 107° GNMT + RL Ensemble (BI 26.30 41.16 1.8-1079 1.1- 1074 ConvS2S Ensemble 26.36 41.29 7.7-10!9 1.2.10?! Transformer (base model) 27.3 38.1 3.3- 1018 Transformer (big) 28.4 41.0 2.3. 1019\\n\\nLabel Smoothing During training, we employed label smoothing of value €;, = 0.1 (B0J. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results\\n\\n6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table[2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Tablef3| Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty a = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].\\n\\nTable[2]summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPup|\\n\\n6.2. Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table[3]\\n\\nIn Table[3|rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TELOPS for K80, K40, M40 and P100, respectively.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nN dyoast de Rh de dy Parop ets Game | deny dev). base | 6 5122048. 8 64 64 O01 O1 100K] 492.258 65 1 512 512 5.29 24.9 A) 4 128 128 5.00 25.5 16 32 32 491 258 32 16 16 5.01 25.4 16 5.16 251 58 (8) 32 5.01 254 60 2 611 23.7 36 4 5.19 253 50 8 488 255 80 ©) 256 32 32 5.75 245 28 1024 128 128 4.66 26.0 168 1024 5.12 254 53 4096 475 262 90 0.0 5.77 24.6 0.2 495 25.5 @) 0.0 467 253 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big | 6 1024 4096 16 03 300K | 433 264.213\\n\\nIn Table B] rows (B), we observe that reducing the attention key size dj, hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model. 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com, |tensorflow/tensor2tensor|\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\nReferences\\n\\n1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv: 1607.06450, 2016.\\n\\n2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n3 Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n4 Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv: 1601.06733, 2016.\\n\\n5 Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rmn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n6 Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv: 1610.02357, 2016.\\n\\n7 Junyoung Chung, Caglar Giilgehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n8 Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv: 1705.03 122v2, 2017.\\n\\n9 Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n\\n0 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\\n\\n1] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n\\n2] Sepp Hochreiter and Jiirgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\\n\\n3] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv: 1602.02410, 2016.\\n\\n[14] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In /nternational Conference on Learning Representations (ICLR), 2016.\\n\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n\\n6] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n\\n7| Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In JCLR, 2015.\\n\\n8] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n\\n9] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv: 1703.03130, 2017.\\n\\n20] Samy Bengio Lukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv: 1508.04025, 2015.\\n\\n[22] Ankur Parikh, Oscar Tackstrém, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv: 1705.04304, 2017.\\n\\n[24] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv: 1608.05859, 2016.\\n\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv: 1508.07909, 2015.\\n\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958, 2014.\\n\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates, Inc., 2015.\\n\\n29 Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.\\n\\n30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv: 1609.08144, 2016.\\n\\n32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n11 Journal of Imaging\\n\\nArticle\\n\\nAl vs. AI: Can AI Detect AIl-Generated Images?\\n\\nSamah S. Baraheem !\\'?*® and Tam V. Nguyen 2®\\n\\nDepartment of Computer Science, Umm Al-Qura University, Prince Sultan Bin Abdulaziz Road, Mecca 21421, Makkah, Saudi Arabia\\n\\nDepartment of Computer Science, University of Dayton, Dayton, OH 45469, USA; tamnguyen@udayton.edu\\n\\n* Correspondence: ssbaraheem@uqu.edu.sa\\n\\nAbstract: The proliferation of Artificial Intelligence (AI) models such as Generative Adversarial Net- works (GANs) has shown impressive success in image synthesis. Artificial GAN-based synthesized images have been widely spread over the Internet with the advancement in generating naturalistic and photo-realistic images. This might have the ability to improve content and media; however, it also constitutes a threat with regard to legitimacy, authenticity, and security. Moreover, implementing an automated system that is able to detect and recognize GAN-generated images is significant for image synthesis models as an evaluation tool, regardless of the input modality. To this end, we propose a framework for reliably detecting Al-generated images from real ones through Convolutional Neural Networks (CNNs). First, GAN-generated images were collected based on different tasks and different architectures to help with the generalization. Then, transfer learning was applied. Finally, several Class Activation Maps (CAM) were integrated to determine the discriminative regions that guided the classification model in its decision. Our approach achieved 100% on our dataset, i.e., Real or Synthetic Images (RSI), and a superior performance on other datasets and configurations in terms of its accuracy. Hence, it can be used as an evaluation tool in image generation. Our best detector was a pre-trained EfficientNetB4 fine-tuned on our dataset with a batch size of 64 and an initial learning rate of 0.001 for 20 epochs. Adam was used as an optimizer, and learning rate reduction along with data augmentation were incorporated. A heck f check for updates\\n\\nKeywords: GAN-generated images detection; GAN image localization; detection of computer- generated images; fake Al-generated images recognition; fake and real detection; convolutional neural networks\\n\\nCitation: Baraheem, $.S.; Nguyen, T.V. Al vs. AI: Can AI Detect Al-Generated Images? J. Imaging 2023, 9, 199. https: //doi.org/ 10.3390/jimaging9100199\\n\\nAcademic Editor: Cosimo Distante\\n\\nReceived: 10 August 2023 Revised: 13 September 2023 Accepted: 27 September 2023 Published: 28 September 2023\\n\\nAttribution (CC BY) license (https:// creativecommons.org /licenses/by/ 4.0/). Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons\\n\\n1. Introduction\\n\\nImage synthesis is the process of generating artificial images from different input modalities, i.e., text, sketch, audio, or image [1]. It is used in many applications, such as art generation [2], photo editing [3,4], photo inpainting [5,6], and computer-aided design [7]. Thus, image synthesis has received intense research, especially after Generative Adversarial Networks (GAN) [8] were developed. GAN has two networks, namely, a generator and a discriminator. The two components compete with each other. While the generator attempts to generate realistic images to fool the discriminator, the discriminator attempts to differentiate between artificial and real images. In the beginning, the generator produces obviously fake data, and the discriminator easily and quickly classifies the generated data as a fake and penalizes the generator for generating implausible data. As the training progresses over the time, the generator learns to produce plausible data that can fool the discriminator, and the discriminator becomes unable to distinguish between synthetic and real data, leading to classifying synthetic data as real. This contest yields in generating high-quality photo-realistic images. Therefore, many researchers have incorporated GANs into image synthesis, which has led to a significant enhancement in generated images. This tremendous progress over the past decade in generating images has led to the provision of synthetic media with a great level of photorealism and quality. This, of course, opens\\n\\nJ. Imaging 2023, 9, 199. https: / /doi.org /10.3390/jimaging9100199\\n\\nhttps: / /www.mdpi.com/journal /jimaging\\n\\nJ. Imaging 2023, 9,199\\n\\nup a great opportunity for artists, designers, movie makers, game designers, and creative users to create new content of interest and enhance their media. However, it may constitute serious threat regarding legitimacy, authenticity, and security, especially in the hands of malicious users. Malicious users may exploit this advantage to create and disseminate fake media that is well-suited to their attacks. Thus, the media forensic community faces new difficulties as a result of not being able to detect Al-generated images in the real world. Images are considered to be important evidence in forensic investigations that can help in determining the authenticity and origin of sources. Thus, image integrity is critical in forensic investigations. One example of misusing synthetic images in a way that may affect forensic investigations is when generated images are planted at a crime scene or sent to investigation offices as a false lead. Furthermore, being able to create images that may be used for criminal or malicious goals raises a concern. For instance, criminal and malicious users could create fake websites on the internet or fake profiles on social media to widely spread false information or advertise false products that can be used for fraud purposes. a\\n\\nAs shown in Figure 1, it is hard to distinguish synthetic images from genuine ones with he rapid improvement in the image synthesis field. Hence, an automated tool for detecting and localizing GAN-generated images is necessary, since most synthetic images contain races invisible to the human eye. In fact, in spite of the high-quality images resulting rom GAN models, synthesized images bear explicit or implicit artifacts resulting from he generation process. Figure 2 shows both explicit artifacts in terms of pixel anomalies and implicit artifacts, which are considered as artificial signatures/fingerprints based on the generative model architecture. Different GAN architectures may have different ingerprints seen in the spatial or frequency domains. Thus, these artifacts could be exploited to distinguish between synthetic and real images. Since each GAN architecture produces a specific pattern that is inserted automatically into each generated image as a distinct fingerprint [9,10], it is hard to generalize detectors on GAN-generated images rom other GAN architectures. Other studies have worked on the Fourier domain to detect GAN-generated images from real ones [11-13]. These studies have shown that a distinct signature is observed in the frequency domain as a peak, which does not appear in the Fourier spectrum of the real image. This artificial fingerprint depends on the GAN architecture and is caused by the up-sampling operations. Thus, it fails to generalize.\\n\\nod Real Synthetic\\n\\nFigure 1. Examples of real images and their corresponding generated ones produced by various GAN-models based on different tasks.\\n\\nAccordingly, in this paper, we conducted an extensive experimental study that led to creating an automated model that is able to reliably detect GAN-generated images from real ones. Our approach was based on fine-tuning a pre-trained convolutional neural networks (CNN) model on a newly collected dataset. Our dataset was based on different modalities of image synthesis and different GAN architectures to help with the generalization ability across various tasks and architectures. We further visualized where the model looked at the image during the classification process by incorporating various CAMs.\\n\\n2 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nThe remainder of this paper is organized as follows. We briefly introduce the related works in Section 2. The proposed data collection and method are presented in Section 3. We conduct the experiments in Section 4 and discuss the results and limitations of our approach in Section 5. Finally, the paper is concluded, and the future work is introduced in Section 6.\\n\\nGAN-generated Image\\n\\n_\\n\\neos pe : ES\\n\\nFigure 2. Examples of GAN-synthesized images along with their explicit and implicit artifacts. From left to right: pixel anomalies, artificial fingerprints, and spectral artifacts. 2. Related Work\\n\\nImage generation is the task of synthesizing a high-quality realistic image from an image [14-17], sketch [18], or text [19-22]. It is a challenging and interesting problem in the computer vision area. Research on image synthesis has been actively conducted over the past decade, especially after the advent of GANs [8]. The GAN architecture and its variants have improved these generated images. Sometimes, these artificial images are hardly distinguishable from real images. This could help in improving the media and content in the digital world; nonetheless, it likewise comprises dangers with respect to authenticity, genuineness, and security.\\n\\n2.1. Image-to-Image Synthesis\\n\\nThis is the process of translating images from a source domain to another domain. This mapping works by modifying some characteristics in the input image from the source domain to have the same characteristics in the target domain while maintaining its content. For instance, segmentation mask maps are transformed into color images [14-17].\\n\\n2.1.1. Image-to-Image Synthesis through Conditional GAN (cGAN)\\n\\nConditional GAN (cGAN) [23] is a conditional version of GAN, where an additional input, e.g., class labels, is fed into the generator along with the noise input to generate synthetic data. This condition extension, e.g., class labels, is also fed into the discriminator along with the real and synthetic data so it can differentiate between them. OASIS [14] uses a simplified version of the GAN architecture. The discriminator is re-designed as a semantic segmentation network, where label maps are directly used as ground truth. The discriminator’s architecture is based on U-Net [24], which is an encoder—decoder model connected by skip connections. This helps in generating images that are well-aligned with their corresponding semantic label maps. To promote the discriminator to concentrate on the semantic/content and differences in the structural information of the generated and real images, LabelMix regularization is leveraged, leading to re-designing the discriminator. Hence, the generator is re-designed to allow for the synthesis of multi-modal data through 3D noise sampling. This enables partially or completely changing the images, which leads to enhancing the diversity of the generated images. CC-FPSE [15] uses a conditional GAN for semantic image synthesis. It works by exploiting the semantic layout during the generation process. The generator predicts the convolutional kernels restricted on the semantic label maps to produce feature maps. The resulting feature maps from the noise are used to generate the image. To improve the alignment of the synthesized images with their label maps and generate fine details, the discriminator is designed based on\\n\\n3 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nfeature pyramid semantic-embedding discriminator. This predicts not only real/fake scores, but also the semantic alignment scores to the semantic label maps. SPADE [16] is proposed upon a pix2pix model [25]. It uses a spatially adaptive normalization layer with learned parameters in the generator to maintain the semantic information, leading to the generation of photo-realistic images. The reason for this is that SPADE normalizes only the activations from the prior layer. Thus, it preserves the input information, as opposed to InstanceNorm [26], which tends to wash away the semantic information during the generation process. a\\n\\n2.1.2. Image-to-Image Synthesis through Transformers\\n\\nTaming-transformers [17] incorporates a transformer with the inductive bias of a CNN. In the generator, the CNN, which consists of encoder—decoder architecture, learns a discrete spatial codebook. To learn this context-rich codebook and generate local realism, the model uses VQ-GAN, which is a type of VQVAE, along with perceptual loss. Then, the transformer learns the global composition within the image to generate high-resolution images.\\n\\n2.2. Sketch-to-Image Synthesis\\n\\nThis is a task that converts an input sketch into a photo-realistic image. Thus, it maps simple and rough sketch that only contains basic structural information to a color image that consists of rich features. S2I-DetectoRS [18], S2I-HTC [18], S2I-QueryInst [18], and S2I-MaskRCNN [18] share the same framework and consist of four main stages. In the first stage, a pre-trained instance segmentation model on the MS-COCO dataset [27] is leveraged. Specifically, DetectoRS [28], HTC [29], QueryInst [30], or Mask R-CNN [31] are fine-tuned on different types of edge maps [32-35] to segment 92 classes of the MS- COCO dataset [27]. Then, the semantic segmentation stage takes place. Next, the resultant semantic mask map is fed into an image-to-image translation model, namely SPADE [16], to generate a photo-realistic image. Finally, a post-processing stage is incorporated to enhance the background further and refine human faces.\\n\\na\\n\\n2.3. Text-to-Image Synthesis\\n\\nThis is the process that maps a natural language description into a realistic image that conveys the same semantic information as the input text [18-21]. Hence, it substitutes a long sentence into one still image.\\n\\n2.3.1. Text-to-Image Synthesis though Attention Module\\n\\nIn AttnGAN [19], an attention module is integrated to generate fine details. The words in the input text are encoded into word vectors along with a sentence vector that results from encoding the whole sentence. The generation process occurs over two stages. In the first stage, based on the sentence vector, a low-quality image is generated. Then, multi-stage refinement takes place to successively refine the image by focusing on specific regions of the low-quality image based on the related word vectors. In ControlGAN [22], an attention module is used. In particular, a word-level spatial and channel-wise attention module is integrated into the generator to concentrate on subregions each time, depending on the more related words. Additionally, to obtain fine-grained feedback, a word-level discriminator is introduced to correlate subregions of the image with words. Therefore, this enables the generator to change the visual attributes of specific subregions without impacting the other regions.\\n\\n2.3.2. Text-to-Image Synthesis though Contrastive Learning\\n\\nSince the text is created by humans during annotation, different captions might be assigned to one image based on the annotators’ point of view. This discrepancy could lead to generated images which are different from their corresponding ground truth. To tackle this problem, DM-GAN+CL [20] is proposed. It is a contrastive learning method for learning consistent representations of the input text. Hence, this improves the semantic consistency.\\n\\n4 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\n2.3.3. Text-to-Image Synthesis though Deep Fusion Block (DFBlock)\\n\\nDF-GAN [21] is a one-stage, simple, yet effective model that is able to directly generate high-quality images from input text. The generator incorporates a Deep text-image Fusion Block (DFBlock) to enhance the semantic consistency between the synthesized images and their corresponding texts. The discriminator consists of Matching-Aware Gradient Penalty (MA-GP) to encourage the generator to produce more photo-realistic images. 3. Data Collection and Methodology\\n\\nIn this section, we first introduce the Real or Synthetic Images (RSI) dataset that was used for the training and testing. Then, our proposed framework is discussed in detail. An overview of our proposed method is illustrated in Figure 3.\\n\\noo : 1000 classes CNN model (Head removed) | Transfer learning ImageNet ; Real — \\' 2 classes Synthetic (New head)\\n\\nimage\\n\\nimage\\n\\nFigure 3. The pipeline of our proposed framework for GAN-generated image recognition.\\n\\n3.1. Data Collection: Real or Synthetic Images (RSI)\\n\\nTo enable our method to generalize better, a dataset was collected based on 12 different image synthesis models from several tasks, i.e., image-to-image, sketch-to-image, and text-to-image. Having a dataset of various tasks and diverse GAN architectures helped not only with the generalization capability, but also in boosting the accuracy. Our dataset was compiled based on the COCO-Stuff dataset [36] through (1) image-to-image syn- thesis models: OASIS [14], CC-FPSE [15], SPADE [16], and Taming-transformers [17], (2) sketch-to-image synthesis models: S2I-DetectoRS [18], S2I-HTC [18], S2I-QueryInst [18], and S2I-MaskRCNN [18], and (3) text-to-image synthesis models: AttnGAN [19], DM- GANG+CL [20], DF-GAN [21], and ControlGAN [22]. The COCO-Stuff dataset [36] is an extension of the MS COCO dataset [27]. The COCO dataset [27] contains only instance-level annotations for classes of things. In total, 164k images of the MS COCO dataset [27] were augmented with pixel-level annotations corresponding to stuff. Then, the dataset was split into 118k, 5k, and 40k images for the training, validation, and testing sets, respectively. Indeed, the COCO-Stuff dataset [36] is composed of 172 classes: 80 thing classes, 91 stuff classes, and 1 ‘unlabeled’ class. Thus, it can be used in many tasks, such as classification, detection, segmentation, scene understanding, captioning, and image generation, just to name a few. Examples of synthesized images generated by different tasks conditioned on different input types are shown in Figure 4.\\n\\n5 of 18\\n\\n— Imaging 2023, 9,199\\n\\nOASIS, S2I-De- Att- Sushko tectoRS, nGAN, at al., Bara- Xu et al., 2020 heem et 2017 al., 2023 cc- S2I- DM- FPSE, HTC, GAN+C Liu et Bara- L, Ye et al., 2019 heem et al., 2021 al., 2023 SPADE, S2I-Que- DF- Park, rylnst, GAN, 2019 Bara- Tao et heem et al., 2022 al., 2023 Taming- S2I- Control- trans- MaskRC GAN, Li formers, NN, etal. Esser et Bara- 2019 al., 2021 cs heem et Image-to-image synthesis all, 0p Sketch-to-image synthesis Text-to-image synthesis\\n\\nFigure 4. A sample of synthesized images generated by different tasks and different GAN models [14-22].\\n\\nOur dataset consisted of 24k, 12k, and 12k images for the training, validation, and testing, respectively. For each set, the number of images was split evenly between real and synthetic images. More details about our data collection are provided in Table 1.\\n\\nTable 1. Specification of our dataset.\\n\\nTask Model Input Training Set Validation Set Testing Set Total OASIS [14 2000 000 1000 4000 Image-to-image CC-FPSE [15] Semantic 2000 000 1000 4000 synthesis SPADE [16 mask map 2000 000 1000 4000 Taming-transformers [17] 2000 000 1000 4000 $2I-DetectoRS [18] 2000 000 1000 4000 Sketch-to- S2I-HTC [18] 2000 000 1000 4000 image Sketch synthesis $2I-QueryInst [18] 2000 000 1000 4000 S2I-MaskRCNN [18] 2000 000 1000 4000 AttnGAN [19] 2000 000 1000 4000 Text-to-image DM-GAN+CL [20] Text 2000 000 1000 4000 . ex synthesis DF-GAN [21] 2000 000 1000 4000 ControlGAN [22] 2000 000 1000 4000 24,000 12,000 12,000 48,000\\n\\nThe process of collecting our dataset was as follows. We first collected real images based on the COCO-Stuff dataset [36]. Then, for each real image, we generated differ- ent inputs in terms of text, sketch, and semantic segmentation mask maps depending on [36]. Specifically, for the tasks of text-to-image synthesis and image-to-image synthe- sis, we collected the captions and semantic mask maps from [36]. However, for the task sketch-to-image synthesis, we generated the sketches based on a dodging and burning algorithm [32], which produced pencil sketches and maintain structural details. Next, for\\n\\n6 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\neach of the 12 image synthesis models and based on the input modality, we ran the pre- trained model on the generated inputs to obtain synthetic artificial images. The resolution of the real and synthetic images was rescaled to 256 x 256, regardless the image synthesis models’ resolution.\\n\\n3.2. Methodology\\n\\nWith the recent improvements in image synthesis, it is significant to develop an automated tool to dependably and timely recognize GAN-generated images. Thus, after the dataset was collected, we trained a classifier by fine-tuning the classifier that was pre-trained on the ImageNet dataset [37] on our newly collected RSI dataset. The reason for this was that training a classifier from scratch is not only time consuming and costly, but also less efficient in terms of classification performance. Therefore, transfer learning is a better approach. Since there were only two classes in our dataset, namely, real or synthetic, the training process was conducted in binary classification setting. A variety of classifiers were leveraged through an experimental study to reach the best model that was able to reliably and precisely detect and localize GAN-generated images.\\n\\nThe details of the training procedure were as follows. We fine-tuned our dataset on VGG19 [38], ResNet [39] with different number of layers: (50, 101, and 152), Incep- tionV3 [40], Xception [41], DenseNet121 [42], InceptionResNetV2 [43], MixConv [44], MaxViT [45], and EfficientNetB4 [46]. The head of the classifier pre-trained on Ima- geNet [37] was removed, and a new head was placed on top of the classifier. The new head for all our classifiers consisted of a global average pooling, a dense layer with a ReLU [47] activation function, a batch normalization layer [48] to reduce the internal co-variant shift and stabilize the training process, a dropout layer [49] to overcome the overfitting problem, and finally, a dense layer with a sigmoid activation function. For all the classifiers, a batch size of 64 was used, and the initial learning rate was 0.001. The training lasted for 20 epochs; however, checkpoints were saved whenever there was an improvement in the validation loss, so that the best model could be loaded later during testing. Moreover, for all the classifiers, the learning rate was reduced automatically when the validation loss stopped improving. Furthermore, data augmentation, in particular, horizontal flip, was applied on the training set during the training process. Adam [50] was incorporated as an optimizer for all the classifiers except ResNet101, where RMSprop [51] was used. 4. Results and Analysis\\n\\nIn this section, we first introduce the evaluation metrics used to evaluate performance. Then, the experimental results are reported based on the metrics adopted on our RSI dataset to evaluate the efficiency of our proposed methods in detecting and localizing GAN-generated images from real ones. Next, an ablation study was conducted to show the effectiveness of our proposed approach in recognizing GAN-synthesized images on different modalities of image synthesis (text, sketch, or image) and different GAN-based image synthesis models.\\n\\n4.1. Evaluation Metrics\\n\\nTo evaluate the performance of our proposed methods in detecting GAN-generated images from real ones, eight common evaluation metrics were adopted. Specifically, Precision, Recall, F1 score, Accuracy, Average Precision (AP), Area Under Curve of Receiver Operating Characteristic (ROC-AUC), False Positive Rate (FPR), and False Negative Rate (FNR) were measured and recorded.\\n\\n4.2. Experimental Results on RSI\\n\\nA comprehensive performance evaluation was conducted on our RSI dataset to val- idate the performance of our approach in detecting GAN-generated images. In particu- lar, the testing set of RSI, which consisted of 12,000 images split equally between GAN- generated and real images, based on the COCO-Stuff dataset [36], was used during the\\n\\n7 of 18\\n\\nJ. Imaging 2023, 9, 199\\n\\nevaluation. The eight evaluation metrics were computed on the testing set of our dataset and are reported in Table 2. As can be seen, EfficientNetB4 achieved the best performance in recognizing and localizing GAN-generated images, with a 100% accuracy, followed by InceptionV3 with 98% in terms of accuracy on this particular dataset, i.e., the RSI dataset. Instances of GAN-generated and real images from the RSI dataset that were classified properly via our best model (EfficientNetB4), with a 100% accuracy, are illustrated in Figure 5.\\n\\nTable 2. Performance of different classifiers on testing set.\\n\\nPrecision Recall Fl Accuracy AP ROC-AUC FPR FNR VGG19 0.94 0.94 0.94 0.94 0.9819 0.9803 0.053 0.064 ResNet50 0.93 0.91 0.91 0.91 0.9933 0.9927 0.0035 0.168 ResNet101 0.95 0.95 0.95 0.95 0.9879 0.9877 0.028 0.08 ResNet152 0.92 0.92 0.92 0.92 0.9743 0.9718 0.042 0.118 InceptionV3 0.98 0.98 0.98 0.98 0.9976 0.9974 0.016 0.03 Xception 0.97 0.97 0.97 0.97 0.9995 0.9994 0.0003 0.054 DenseNet121 0.97 0.97 0.97 0.97 0.9969 0.9966 0.012 0.044 InceptionResNetV2 0.96 0.96 0.96 0.96 0.9942 0.9943 0.037 0.036 MixConv 0.94 0.94 0.94 0.94 0.9411 0.9412 0.057 0.056 MaxViT 0.92 0.86 0.89 0.89 0.9375 0.9375 0.087 0.137 EfficientNetB4 1.00 1.00 1.00 1.00 1.00 1.00 0.0 0.0\\n\\nDetection of GAN-generated images\\n\\nDetection of real images\\n\\nFigure 5. Examples of detecting real and GAN-generated images with 100% accuracy using our best model on the testing set.\\n\\nFurthermore, to locate the region where our model was looking at during the classifica- tion process, different types of Class Activation Maps (CAM) were integrated. Specifically, GradCAM [52], AblationCAM [53], LayerCAM [54], and Faster ScoreCAM [55] were adopted to inspect the images by identifying which parts of the image contributed more to the classification decision of our model. A visualization of the four different CAM methods is demonstrated in Figure 6.\\n\\n8 of 18\\n\\nJ. Imaging 2023, 9, 199 Category\\n\\nSynthetic\\n\\nReal\\n\\n9 of 18\\n\\nImage\\n\\nGradCAM, Selvaraju et al., 2020\\n\\nAblationCAM, Desai et al., 2020\\n\\nLayerCAM, Jiang et al., 2021\\n\\nFaster ScoreCAM, Wang et al. 2020,\\n\\n)\\n\\nFigure 6. Visualization of variants of CAM on the testing set using our best model EfficientNetB4 [52-55].\\n\\nAs shown, the model looked mostly to the background of the GAN-generated images. However, it also looked at some regions where distortion and anomalies appeared in the synthetic images. To faithfully explain the predictions of our method, an explanation technique called LIME [56] was leveraged, as shown in Figure 7.\\n\\nJ. Imaging 2023, 9,199\\n\\nSynthetic image\\n\\nSuper-pixels- based explanation\\n\\nRegions-based explanation\\n\\nReal image\\n\\nSuper-pixels- based explanation\\n\\nRegions-based explanation\\n\\nFigure 7. Visualization of LIME interpretability technique on detecting GAN-generated images from real ones based on our method. The first and fourth columns represent the input image whether synthetic or real. The second and fifth columns illustrate LIME explanation based on super-pixels, where only the super-pixels that contribute to the final classification decision are presented. The third and sixth columns demonstrate LIME explanation based on the two regions. While the green-colored regions of super-pixels correspond to regions that increase the probability of the classified label, the red-colored regions of super-pixels correspond to regions that decrease the probability of the classified label. (a) GAN-generated images classified as “Synthetic”; (b) Real images classified as “Real”.\\n\\n4.3. Effectiveness of Our Model\\n\\nTo show the effectiveness of our model in recognizing GAN-generated images for other models that were not trained on them, we further conducted one more experiment. In this experiment, we trained three models separately based on the input modality. Specifically, we trained a model on sketch-to-image and text-to-image models, named (S2I_T2I). In total, eight models were used for both modalities. Then, the trained model was tested on an image-to-image modality which consisted of four models. The second experiment (I21_T2I) was that the model was trained on image-to-image and text-to-image models, and then tested on sketch-to-image modality models. The same specification was followed in the third and last experiment, where we trained the model on image-to-image and sketch-to-image models (I2I_S2I), and then tested the trained model on text-to-image models. The results of these experiments are reported in Table 3. As can be seen, while the performance of the S2I_T2I model, when excluding I2I models, achieved 0.99 in terms o: accuracy, the I2I_S2I model reached 0.95. In the meantime, the I2]_T2I model achieved less accuracy with 0.83 due to the image improvement step used in sketch-to-image models. This image improvement step attempts to enhance the background by replacing the generated background with a real background consistent and aligned with the context/objects in the\\n\\n10 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nimage. Even though this step enhances the generated results, our GAN detection model was still able to detect GAN-generated images with a high accuracy of 0.83.\\n\\nTable 3. Performance of our model trained on two modalities and tested on the third and excluded modality to show the effectiveness of our model in detecting GAN-generated images.\\n\\nPrecision Recall F1 Accuracy AP ROC-AUC FPR FNR S21_T2I 0.99 0.99 0.99 0.99 0.9998 0.9997 0.026 0.001 I21_T2I 0.87 0.83 0.82 0.83 0.9684 0.9801 0.347 0.0 I21_S21 0.96 0.95 0.95 0.95 0.99997 0.99997 0.093 0.0\\n\\nConsequently, our current model proved its capability in detecting GAN-generated images from real ones with a high accuracy, even though the model and its modality were not trained on. Thus, our GAN detection model is able to recognize GAN-generated images if anew GAN model is proposed in the future.\\n\\n4.4, Ablation Study\\n\\nTo demonstrate the effectiveness of our method in detecting GAN-based synthetic images, we evaluated our second-best model, InceptionV3, on the testing set of differen! input modalities separately. We further assessed the performance of our second-best model on the testing set of image synthesis models individually. The reason that the second-best model, InceptionV3, was used during the ablation study was due to the fact that our bes model EfficientNetB4 achieved 100% accuracy. Thus, there were no differences in the performance when part of the dataset was evaluated.\\n\\nWhile Table 4 reports the performance based on the eight common evaluation metrics, Figure 8 presents the confusion matrices of our second-best model on the testing se generated from the image-to-image, sketch-to-image, and text-to-image synthesis models. The highest accuracy was accomplished with the GAN-based synthetic images generated from the texts, at 98.35%. On the contrary, the lowest accuracy was achieved when the inputs to the image synthesis models were semantic segmentation maps, at 96.90%.\\n\\nMoreover, we studied the performance of our second-best model on the GAN-synthesized images separately produced by various synthesis images models. Table 5 provides a com- prehensive performance evaluation of the models in detecting GAN-generated images produced by a single image synthesis model. From Table 5, we can conclude that it might be more challenging to detect GAN-generated images obtained from semantic segmentation mask maps than ones produced by natural language descriptions. Since the layout, shape, size, and semantic information are maintained in image-to-image synthesis, the generated images are more photo-realistic and naturalistic than the ones produced from text-to-image synthesis.\\n\\nTable 4. Performance of our second-best model in terms of accuracy (InceptionV3) on GAN-generated images individually produced by different input modalities.\\n\\nPrecision Recall F1 Accuracy AP ROC-AUC FPR FNR Image-to-image (I2I) 0.97 0.97 0.97 0.9690 0.9961 0.9957 0.032 0.03 Sketch-to-image (S21) 0.98 0.98 0.98 0.9790 0.9977 0.9974 0.012 0.03 Text-to-image (T2I) 0.98 0.98 0.98 0.9835 0.9991 0.9990 0.003 0.03\\n\\n11 of 18\\n\\nJ. Imaging 2023, 9, 199\\n\\n1750 1500 1250 1000 True Class -750 -500 - 250 1 Predicted Class\\n\\n1750 1500 1250 1000 True Class - 750 - 500 -250 1 Predicted Class\\n\\nPredicted Class Image-to-image synthesis Sketch-to-image synthesis\\n\\n1750 1500 + 1250 - 1000 True Class - 750 - 500 - 250 1 Predicted Class Text-to-image synthesis\\n\\nFigure 8. Confusion matrices produced by our second-best model in terms of accuracy (InceptionV3) on the testing set of different input modalities.\\n\\nTable 5. Performance of our second-best model in terms of accuracy (InceptionV3) on GAN-generated images individually produced by different models.\\n\\nInput ROC- Modality Precision _ Recall F1 Accuracy AP AUC FPR FNR OASIS [14] 0.96 0.96 0.96 0.964 0.9958 0.9951 0.042 0.03 CC-FPSE [15] pI 0.98 0.98 0.98 0.983 0.9983 0.9981 0.004 0.03 SPADE [16] 0.98 0.98 0.98 0.978 0.9979 0.9977 0.014 0.03 Taming-transformers [17] 0.95 0.95 0.95 0.951 0.9928 0.9920 0.068 0.03 S2I-DetectoRS [18] 0.98 0.98 0.98 0.981 0.9977 0.9974 0.008 0.03 S2I-HTC [18] sol 0.98 0.98 0.98 0.977 0.9977 0.9973 0.016 0.03 S2I-QueryInst [18] 0.98 0.98 0.98 0.978 0.9977 0.9975 0.014 0.03 S2I-MaskRCNN [18] 0.98 0.98 0.98 0.980 0.9978 0.9974 0.010 0.03 AttnGAN [19] 0.98 0.98 0.98 0.984 0.9990 0.9989 0.002 0.03 DM-GAN+CL [20] TI 0.98 0.98 0.98 0.984 0.9996 0.9996 0.002 0.03 DF-GAN [21] 0.98 0.98 0.98 0.982 0.9986 0.9985 0.006 0.03 ControlGAN [22] 0.98 0.98 0.98 0.984 0.9991 0.9990 0.002 0.03\\n\\n4.5. Experimental Results on Other Datasets\\n\\nTo show our model’s efficacy in detecting generated images from real ones, we further ran one more experiment. In this experiment, the first model in each modality from Table 1 was selected. More specifically, OASIS [14], S2I-DetectoRS [18], and AttnGAN [19] were\\n\\n12 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nused as image-to-image, sketch-to-image, and text-to-image synthesis models, respectively. Then, different datasets, which our model was not trained on, were utilized to generate synthetic images. Particularly, the ADE20K [57], Sketchy [58], and Caltech-UCSD Birds- 200-2011 (CUB-200-2011) [59] datasets were leveraged. Following this, both generated and real images were fed into our best detector, i.e., EfficientNetB4, to detect synthetic images from real ones. Finally, eight common evaluation metrics were leveraged to measure our model’s performance. Mainly, Precision, Recall, Fl score, Accuracy, Average Precision (AP), Area Under Curve of Receiver Operating Characteristic (ROC-AUC), False Positive Rate (FPR), and False Negative Rate (FNR) were adopted. The details are explained as follows. With regard to the OASIS image-to-image synthesis model [14], the testing set of the ADE20K dataset [57], which consisted of 2000 semantic segmentation images, was fed into the pre-trained OASIS model [14]. This step output synthesized images from the semantic segmentation images. Then, the generated and real images of the ADE20K testing set [57] were input into our best model, and the performance was evaluated and recorded.\\n\\nAs for S2I-DetectoRS [18], a subset of the Sketchy dataset [58], which contained 2470 sketches, was used as an input into the S2I-DetectoRS sketch-to-image synthesis model [18]. This subset of the Sketchy dataset [58] was used because S2I-DetectoRS [18] was trained on the MS COCO dataset [27], and only 35 classes from the Sketchy dataset [58] matched the classes in the MS COCO dataset [27]. These classes were airplane, alarm_clock, apple, banana, bear, bench, bicycle, car_(sedan), cat, chair, couch, cow, cup, dog, door, elephant, eyeglasses, giraffe, hat, horse, hotdog, knife, motorcycle, pickup_truck, pizza, sailboat, scissors, sheep, shoe, spoon, table, teddy_bear, umbrella, window, and zebra. In the generation phase, sketches were converted into colored images. After that, both generated and real images were fed into our best model, and the eight-evaluation metrics were measured.\\n\\nWith respect to the AttnGAN text-to-image synthesis model [19], the same procedure was followed. The only difference was the dataset, where the testing set of Caltech-UCSD Birds-200-2011 (CUB-200-2011) [59] was used. The testing set was composed of 5794 captions/descriptions. After generating the corresponding images for the input text de- scriptions, both synthetic and real images were fed into our best model, i.e., EfficientNetB4. In the final step, the performance was assessed based on our adopted evaluation metrics.\\n\\nThe experimental results were recorded and are shown in Table 6. As can be seen, our model was able to detect and recognize the GAN-generated images from real ones, even with other datasets that our classifier was not trained on. While the highest accuracy roughly reached 98% with the CUB-200-2011 dataset [59] and the AttnGAN [18] text-to- image synthesis model, our model achieved 89% in terms of accuracy with the ADE20K dataset [57] and the OASIS [14] image-to-image synthesis model. This may be attributed to the dataset complexity and generator capability. Hence, our model could be used as an evaluation tool regardless of the input modality.\\n\\nTable 6. Performance of our best model on different datasets that our model was not trained on.\\n\\nInput sos ROC- Modality Used Dataset Precision Recall F1 Accuracy AP AUC FPR ENR OASIS [14] 121 ADE20K [57] 0.91 0.89 0.89 0.889 0.9839 0.9826 0.008 0.22 anal S21 Sketchy [58] 0.95 0.94 094 0.943 0.9998 09998 0.0 0.13 AttnGAN [19] T2I CUB-200-2011 0.98 0.98 098 0.978 0.9991 0.9988 0.0 0.04 [59] 5. Discussion and Limitations\\n\\nThis paper provides a comprehensive performance evaluation to assess the perfor- mance of our method in GAN-generated images detection. We first evaluated several classifiers that were fine-tuned and re-trained on our newly collected dataset, namely Real\\n\\n13 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nor Synthetic Images (RSI). The evaluation process was based on the testing set of RSI that contained 12k images. Based on the evaluation metrics used in this study, EfficientNetB4 achieved the highest accuracy, with 100% on this particular dataset.\\n\\nTo show the efficiency of our model in detecting GAN-generated images, we further conducted two different experiments. The first experiment was based on the input modality, where we fine-tuned and re-trained EfficientNetB4 on two input modalities of the RSI raining set. Then, we tested the modality-based trained model on the third excluded input modality of the RSI testing set. Our model achieved a superior performance in terms of accuracy with 99%, 83%, and 95% when I2I, S2I, and T2I were excluded during the raining process, respectively. The reason behind the accuracy reduction in our model to 83% when the S2I input modality was excluded could be attributed to the background improvement and face refinement steps used in sketch-to-image models [18]. While the background improvement step works by replacing the synthetic background with a real, high-quality background well-suited and aligned with the foregrounds in each image, the ‘ace refinement step works by reconstructing and aligning faces. These post-processing steps improve the synthetic images further. However, our model was still capable o recognizing these images as generated images, even when our model was not trained on he S2I modality, at 83% in terms of accuracy.\\n\\nIn the second experiment, different datasets were leveraged in the evaluation phase ‘o show the ability of our model in detecting GAN-generated images, even with other datasets that our model was not trained on. Based on this experiment, it was clear that our model was able to detect generated images from real ones with a high accuracy. Thus, our model can be used in synthetic images detection, which could help in forensic investigations, mitigating the misuse of AI image generation, the alleviation of cyberattacks, and confronting criminal and malicious goals.\\n\\nTo demonstrate the discriminative region(s) of the image that highly influenced the classifier to make a decision, several types of Class Activation Maps (CAM) were adopted. As seen in Figure 6, our model mostly looked at the background of the generated im- ages. This was because generators usually concentrate more on the foreground than the background, leading to generating explicit or implicit artifacts during the generation pro- cess of the background. However, in some cases, our model looked at some parts of the foreground(s) in the generated images. This was because of anomalies and distortions generated in the foreground(s) during the generation process.\\n\\nRegardless of the high accuracy produced by our model, it sometimes failed to properly classify images. A visualization of these failure cases is illustrated in Figure 9. As can be seen from this figure, our model might fail in classifying authentic and genuine images as real when the background or foreground(s) is blurry, when the image looks vintage and old, when the image is of low quality, and/or when motion exists in the image. Furthermore, our model could misclassify GAN-generated images as fake when fine-grained details are represented, whether in the foreground(s) or background, and when the textures are sharper, since GAN-generated images tend to have smoother textures.\\n\\nTo overcome these limitations, the aforementioned aspects of misclassifying the images should be more integrated into the training dataset. Thus, our classifier can learn more about these aspects and hence properly classify the images.\\n\\n14 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nGT Fake Real P Real Fake T2I P(Real): 63.91 P(Real): 73.69 (Real): 99.98 P(Real): 63.91 P(Real): 94.56 P(Real): 89.28 P(Fake): 36.09 P(Fake): 26.31 P(Fake): 0.02 P(Fake): 36.09 P(Fake): 10.72 a P(Real): 7.80 P(Real):7.63 P(Real): 23.08 —-P(Real): 0.03 ~—- P(Real): 0.82. -—- P(Real): 4.11 P(Fake): 92.20 P(Fake): 92.37 P(Fake): 76.92 P(Fake):99.97 P(Fake):99.18 P(Fake): 95.89\\n\\nFigure 9. Failure cases classification visualization of our proposed method on different testing datasets, i.e., ADE20K, Sketchy, and CUB-200-2011. While GT indicates the ground truth, P refers to the misclassified prediction obtained by our model, represented in red. 6. Conclusions and Future Work\\n\\nWith the rapid and continuous evolution of Al-based image synthesis, generated im- ages are coming much closer to being photo-realistic and deceiving human eyes. This may improve media and content; however, it could pose challenges to security and authenticity. Therefore, an automated tool to detect and localize AI-based generated images is necessary. Hence, this paper proposed a machine model for recognizing Al-generated images from real ones. A large dataset, namely, RSI, was compiled based on a variety of GAN-based image synthesis tasks and models, and then a CNN model was well-trained on RSI Exten- sive experiments led to a method that is able to recognize GAN-generated images from real ones with an outstanding accuracy, even with other datasets that our model was not trained on. Thus, our detector can be used to detect synthetic images from genuine ones. Hence, it can aid against malicious or criminal ends by helping the multimedia forensics research community to confront threats that may emerge from the advancements in AI image synthesis technologies. Our findings suggest that GAN-generated images contain some common flaws, distortions, and artifacts that can be exploited to detect synthetic images. These traces are not visible to human eyes; however, a well-trained model can easily detect these flaws and classify the images as synthetic/fake images. Therefore, researchers should pay more attention to these traces during the image generation process to generate not only high-quality photo-realistic images, but also to conceal all fingerprints and traces.\\n\\nIn the future, we plan to incorporate different image synthesis tasks with different architectures and datasets, including facial and biological images, e.g., Western blot and microscopic images. Moreover, we aim to integrate the frequency domain of the dataset along with the spatial domain. Finally, to enhance our GAN-generated image detector, we plan to include proper techniques and strategies to overcome possible adversarial attacks added to the synthetic images, where tiny and imperceptible adversarial perturbation added to generated images may fool the detector.\\n\\nAuthor Contributions: Conceptualization, S.S.B.; methodology, S.S.B. and T.V.N.; software, S.S.B.; data curation, S.S.B.; validation, S.S.B.; formal analysis, S.S.B.; investigation, S.S.B. and T.V.N.; writing—original draft preparation, S.S.B. and T.V.N.; writing—review and editing; S.S.B. and T.V.N.; visualization, S.S.B.; supervision, T.V.N. All authors have read and agreed to the published version of the manuscript.\\n\\n15 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\nFunding: Funding for Open Access provided by the University of Dayton Libraries Open Access Fund. The second author is supported by NSF grant # 2025234. Institutional Review Board Statement: Not applicable.\\n\\nInformed Consent Statement: Not applicable.\\n\\nData Availability Statement: The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request.\\n\\nAcknowledgments: The first author would like to thank Umm Al-Qura University, in Saudi Arabia, for the continuous support. This work has been supported in part by the University of Dayton Office for Graduate Academic Affairs through the Graduate Student Summer Fellowship Program.\\n\\nConflicts of Interest: The authors declare no conflict of interest.\\n\\nReferences\\n\\n1.\\n\\nBaraheem, S.S.; Le, T.-N.; Nguyen, T.V. Image synthesis: A review of methods, datasets, evaluation metrics, and future outlook. Artif. Intell. Rev. 2023, 56, 10813-10865. [CrossRef]\\n\\n2.\\n\\nElgammal, A.; Liu, B.; Elhoseiny, M.; Mazzone, M. CAN: Generating ‘art’ by learning about styles and deviating from style norms. arXiv 2017, arXiv:1706.07068.\\n\\n3.\\n\\nChen, J.; Shen, Y.; Gao, J.; Liu, J.; Liu, X. Language-Based Image Editing with recurrent attentive models. arXiv 2017, arXiv:1711.06288.\\n\\nYan, Z.; Zhang, H.; Wang, B.; Paris, S.; Yu, Y. Automatic photo adjustment using deep neu-ral networks. arXiv 2014, arXiv:1412.7725.\\n\\n4.\\n\\nYu, J.; Lin, Z.; Yang, J.; Shen, X.; Lu, X.; Huang, T.S. Generative image inpainting with con-textual attention. arXiv 2018, arXiv:1801.07892.\\n\\n5.\\n\\n6.\\n\\nNazeri, K.; Ng, E.; Joseph, T.; Qureshi, F.Z.; Ebrahimi, M. EdgeConnect: Generative image inpainting with adversarial edge learning. arXiv 2019, arXiv:1901.00212.\\n\\n7.\\n\\nThaung, L. Advanced Data Augmentation: With Generative Adversarial Networks and Computer-Aided Design. 2020, Dissertation. Available online: http: / /liu.diva-portal.org/smash/record jsf?pid=diva2%3A 1484523 &dswid=6768 (accessed on 1 April 2022).\\n\\nGoodfellow, LJ.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative adversarial nets. In Proceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8-13 December 2014; Volume 2, pp. 2672-2680.\\n\\n8.\\n\\nMarra, F.; Gragnaniello, D.; Verdoliva, L.; Poggi, G. Do GANs Leave Artificial Fingerprints? In Proceedings of the 2019 IEEE Conference on Multimedia Information Processing and Retrieval, San Jose, CA, USA, 28-30 March 2019; pp. 506-511.\\n\\n9.\\n\\nYu, N.; Davis, L.; Fritz, M. Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints. In Proceedings of the\\n\\n10.\\n\\nEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, 27 October-2 November 2019.\\n\\n11.\\n\\nZhang, X.; Karaman, S.; Chang, S.-F. Detecting and Simulating Artifacts in GAN Fake Images. In Proceedings of the 2019 IEEE\\n\\nnternational Workshop on Information Forensics and Security (WIFS), Delft, The Netherlands, 9-12 December 2019; pp. 1-6.\\n\\nDzanic, T.; Shah, K.; Witherden, F. Fourier spectrum discrepancies in deep network generated images. arXiv 2019, arXiv:1911.06465.\\n\\n12.\\n\\nFrank, J.; Eisenhofer, T.; Sch, L.; Fischer, A.; Kolossa, D.; Holz, T. Leveraging Frequency Analysis for Deep Fake Image Recognition. n Proceedings of the 37th International Conference on Machine Learning, Virtual, 13-18 July 2020.\\n\\n13.\\n\\nSushko, V.; Schénfeld, E.; Zhang, D.; Gall, J.; Schiele, B.; Khoreva, A. You only need adversarial supervision for semantic image synthesis. arXiv 2020, arXiv:2012.04781.\\n\\n14.\\n\\nLiu, X.; Yin, G.; Shao, J.; Wang, X.; Li, H. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. arXiv 2019, arXiv:1910.06809.\\n\\n15.\\n\\n16. Park, T.; Liu, M.-Y.; Wang, T.-C.; Zhu, J.-Y. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15-20 June 2019.\\n\\nEsser, P.; Rombach, R.; Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June 2021.\\n\\n17.\\n\\nBaraheem, S.S.; Nguyen, T.V. Sketch-to-image synthesis via semantic masks. In Multimedia Tools and Applications; Springer: Berlin/ Heidelberg, Germany, 2023; pp. 1-20. [CrossRef]\\n\\n18.\\n\\nXu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; He, X. AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks. arXiv 2017, arXiv:1711.10485.\\n\\n19.\\n\\n20. Ye, H.; Yang, X.; Takac, M.; Sunderraman, R.; Ji, S. Improving text-to-image synthesis using contrastive learning. arXiv 2021, arXiv:2107.02423.\\n\\n21. Tao, M.; Tang, H.; Wu, S.; Sebe, N.; Wu, F,; Jing, X.-Y. DF-GAN: Deep fusion generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 18-24 June 2022; pp. 16515-16525. [CrossRef]\\n\\n16 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\n22. Li, B.; Qi, X.; Lukasiewicz, T.; Torr, P-H.S. Controllable Text-to-Image Generation. arXiv 2019, arXiv:1909.07083.\\n\\n23. Mirza, M.; Osindero, S. Conditional generative adversarial nets. arXiv 2014, arXiv:1411.1784.\\n\\nRonneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv 2015, arXiv:1505.04597.\\n\\n24.\\n\\nsola, P.; Zhu, J.-Y.; Zhou, T.; Efros, A.A. Image-to-image translation with conditional adversarial networks. arXiv 2016, arXiv:1611.07004.\\n\\n25.\\n\\nUlyanov, D.; Vedaldi, A.; Lempitsky, V. Instance normalization: The missing ingredient for fast stylization. arXiv 2016, arXiv:1607.08022.\\n\\n26.\\n\\n27. Lin, T-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; Zitnick, C.L. Microsoft COCO: Common objects in context. In Proceedings of the Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, 6-12 September 2014; Springer International Publishing: Cham, Switzerland, 2014; pp. 740-755.\\n\\n28. Qiao, S.; Chen, L.-C.; Yuille, A. DetectoRS: Detecting objects with recursive feature pyramid and switchable atrous convolution. n Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June 2021.\\n\\n29. Chen, K.; Pang, J.; Wang, J.; Xiong, Y.; Li, X.; Sun, S.; Feng, W.; Liu, Z.; Shi, J; Ouyang, W.; et al. Hybrid Task Cascade for Instance Segmentation. arXiv 2019, arXiv:1901.07518.\\n\\n30. Fang, Y.; Yang, S.; Wang, X.; Li, Y.; Fang, C.; Shan, Y.; Feng, B.; Liu, W. Instances as Queries. arXiv 2021, arXiv:2105.01928.\\n\\nHe, K.; Gkioxari, G.; Dollar, P.; Girshick, R. Mask R-CNN. In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22-29 October 2017. 31.\\n\\n32. Beyeler, M. Design and develop advanced computer vision projects using OpenCV with Python. In OpenCV with Python Blueprints, Packt Publishing Ltd.: London, UK, 2015; ISBN 978-178528269-0.\\n\\n33. Xie, S.; Tu, Z. Holistically-nested edge detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 7-13 December 2015.\\n\\n34. Ding, L.; Goshtasby, A. On the Canny edge detector. Pattern Recognit. 2001, 34, 721-725. [CrossRef]\\n\\nanopoulos, N.; Vasanthavada, N.; Baker, R.L. Design of an image edge detection filter using the Sobel operator. IEEE J. Solid-State Circuits 1988, 23, 358-367. [CrossRef]\\n\\n35.\\n\\nCaesar, H.; Uijlings, J.; Ferrari, V. COCO-stuff: Thing and stuff classes in context. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18-22 June 2018; pp. 1209-1218.\\n\\n36.\\n\\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; Li, E-F. ImageNet: A large-scale hierarchical image database. In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20-25 June 2009; pp. 248-255.\\n\\n37.\\n\\nSimonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv 2014, arXiv:1409.1556.\\n\\nHe, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. arXiv 2015, arXiv:1512.03385.\\n\\nSzegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the inception architecture for computer vision. arXiv 2015, arXiv:1512.00567.\\n\\nChollet, F. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21-26 July 2017; pp. 1251-1258.\\n\\nHuang, G.; Liu, Z.; van der Maaten, L.; Weinberger, K.Q. Densely connected convolutional networks. arXiv 2016, arXiv:1608.06993.\\n\\nSzegedy, C.; loffe, S.; Vanhoucke, V.; Alemi, A. Inception-v4, Inception-ResNet and the impact of residual connections on learning. arXiv 2016, arXiv:1602.07261. [CrossRef]\\n\\nTan, M.; Le, Q.V. MixConv: Mixed Depthwise Convolutional Kernels. arXiv 2019, arXiv:1907.09595.\\n\\nTu, Z.; Talebi, H.; Zhang, H.; Yang, F.; Milanfar, P.; Bovik, A.; Li, Y. Maxvit: Multi-axis vision transformer. In European Conference on Computer Vision; Springer Nature: Cham, Switzerland, 2022; pp. 459-479.\\n\\nTan, M.; Le, Q.V. EfficientNet: Rethinking model scaling for convolutional Neural Networks. arXiv 2019, arXiv:1905.11946.\\n\\nAgarap, A.F. Deep Learning using Rectified Linear Units (ReLU). arXiv 2018, arXiv:1803.08375.\\n\\nloffe, S.; Szegedy, C. Batch Normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015, arXiv:1502.03167.\\n\\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 2014, 15, 1929-1958.\\n\\n50. Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980.\\n\\n51. Graves, A. Generating sequences with recurrent neural networks. arXiv 2013, arXiv:1308.0850.\\n\\nSelvaraju, R.R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; Batra, D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. Int. J. Comput. Vis. 2020, 128, 336-359. [CrossRef]\\n\\n52.\\n\\n53. Desai, S.; Ramaswamy, H.G. Ablation-CAM: Visual explanations for deep convolutional network via gradient-free localization. In Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), Snowmass Village, CO, USA, 1-5 March 2020; pp. 972-980.\\n\\n54, Jiang, P.-T.; Zhang, C.-B.; Hou, Q.; Cheng, M.-M.; Wei, Y. LayerCAM: Exploring hierarchical class activation maps for localization. IEEE Trans. Image Process. 2021, 30, 5875-5888. [CrossRef]\\n\\n17 of 18\\n\\nJ. Imaging 2023, 9,199\\n\\n55. Wang, H.; Wang, Z.; Du, M.; Yang, F.; Zhang, Z.; Ding, S.; Mardziel, P.; Hu, X. Score-CAM: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Seattle, DC, USA, 14-19 June 2020.\\n\\n56. Ribeiro, M.T,; Singh, S.; Guestrin, C. Why should I trust you? Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 13-17 August 2016.\\n\\n57. Zhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; Torralba, A. Scene Parsing through ADE20K Dataset. In Proceedings of the\\n\\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21-26 July 2017; pp. 5122-5130.\\n\\n58. Sangkloy, P.; Burnell, N.; Ham, C.; Hays, J. The sketchy database: Learning to retrieve badly drawn bunnies. ACM Trans. Graph. 2016, 35, 1-12. [CrossRef]\\n\\n59. Wah, C.; Branson, S.; Welinder, P.; Perona, P.; Belongie, S.J. The Caltech-UCSD Birds-200-2011 Dataset; California Institute of Technology: Pasadena, CA, USA, 2011.\\n\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\\n\\n18 of 18 MASTERING GENERATIVE AI AND PROMPT ENGINEERING: PRACTICAL GUIDE FOR Data SCIENTISTS\\n\\nA\\n\\nTs\\n\\nAs the AI-driven economy continues to evolve and expand, the role\\n\\nof prompt engineering will become increasingly significant and\\n\\ntransformative across diverse sectors and domains.\\n\\nB&B\\n\\nDATA SCIENCE HORIZONS\\n\\nContents\\n\\nIntroduction\\n\\nChapter 1: Understanding Generative AI\\n\\n1.1, EvoLUTION oF AI: FROM RULE-BASED TO GENERATIVE MODELS 1.2, KEY GENERATIVE AI MODELS: RNNs, LSTMs, GPT, AND MORE 1.3. POPULAR USE CASES FOR GENERATIVE AI\\n\\nChapter 2: Introduction to Prompt Engineering\\n\\n2.1. WHAT IS PROMPT ENGINEERING AND WHY IT MATTERS\\n\\n2.2. PROMPT TYPES: EXPLICIT, IMPLICIT, AND CREATIVE PROMPTS\\n\\n2.3. THE ROLE OF PROMPTS IN GUIDING AI MODELS\\n\\nChapter 3: Designing Effective Prompts\\n\\n3.1. UNDERSTANDING YOUR AI MODEL: CAPABILITIES AND LIMITATIONS\\n\\n3.2, CRAFTING CLEAR AND CONCISE PROMPTS\\n\\n3.3. USING TOKENS, TEMPERATURE, AND OTHER PARAMETERS\\n\\n3.4, ITERATIVE PROMPT DESIGN: TESTING AND REFINING\\n\\nChapter 4: Advanced Prompt Engineering Techniques\\n\\n4.1, CONDITIONAL PROMPTS FOR CONTEXT-SENSITIVE Al\\n\\n4.2, MULTI-STEP PROMPTS FOR COMPLEX TASKS\\n\\n4.3. LEVERAGING TRANSFER LEARNING FOR PROMPT ENGINEERING\\n\\nChapter 5: Ethical Considerations in Generative AI and Prompt Engineering\\n\\n5.1, ADDRESSING AI BIASES AND FAIRNESS\\n\\n5.2, ENSURING TRANSPARENCY AND EXPLAINABILITY\\n\\n5.3. DATA PRIVACY AND SECURITY CONCERNS\\n\\nChapter 6: Real-world Applications and Case Studies\\n\\n6.1. CONTENT GENERATION AND CREATIVE WRITING\\n\\n6.2. DATA ANALYSIS AND VISUALIZATION\\n\\n6.3. CHATBOTS AND CONVERSATIONAL AI\\n\\n6.4, ANOMALY DETECTION AND PATTERN RECOGNITION\\n\\nConclusion\\n\\nAppendices\\n\\nA. RECOMMENDED BOOKS, ARTICLES, AND BLOGS\\n\\nB: ONLINE COMMUNITIES AND FORUMS FOR DISCUSSIONS AND COLLABORATION\\n\\nIntroduction\\n\\nThe field of artificial intelligence (AI) has come a long way since its inception, with generative AI and prompt engineering playing crucial roles in its advancement. As data scientists, it\\'s essential to stay updated with the latest trends and techniques to unlock the full potential of AI in various applications. This short ebook aims to provide a comprehensive guide on generative AI and prompt engineering, equipping readers with the knowledge and tools necessary to excel in the realm of data science.\\n\\nGenerative AI encompasses a range of models and techniques designed to generate new data based on existing input data. These models have demonstrated significant capabilities in natural language processing, image generation, and more. By understanding the mechanics and intricacies of generative AI, data scientists can harness its power to create innovative solutions for a multitude of problems.\\n\\nPrompt engineering, on the other hand, deals with the art of crafting effective prompts to guide AI models in generating desired outputs. As AI models become more sophisticated, the need for efficient and precise prompt engineering has grown more critical. By mastering this skill, data scientists can better direct AI models to produce targeted results, ultimately enhancing the efficacy of their applications.\\n\\nThis ebook will delve into the key concepts, best practices, and real-world applications of generative AI and prompt engineering. It will explore the capabilities and limitations of popular AI models, detail the process of designing effective prompts, and discuss the ethical considerations that arise when working with these technologies. To further support your learning, the book will also present a series of case studies, demonstrating the practical applications of generative AI and prompt engineering in various industries.\\n\\nBy the end of this ebook, you will have gained a solid understanding of generative AI and prompt engineering, enabling you to apply these techniques to your own projects effectively. As AI continues to evolve and impact our world in unprecedented ways, the knowledge you gain from this guide will prove invaluable in your journey as a data scientist.\\n\\nChapter 1: Understanding Generative AI Tl\\n\\nBy understanding the evolution of AI and the mechanics of generative models, data scientists can better harness these cutting-edge technologies to create innovative solutions for a diverse range of challenges.\\n\\n1.1. Evolution of AI: From Rule-Based to Generative Models\\n\\nThe history of artificial intelligence can be traced back to the mid-2oth century when the first AI concepts emerged. Over the years, AI has evolved through several stages, each marked by significant advancements in technology and methodology. One of the most notable transitions in AI has been the shift from rule-based systems to generative models.\\n\\nIn the early days of AI, rule-based systems were the predominant approach. These systems relied on a set of predefined rules and decision trees to process input data and produce output. While rule-based systems were effective for simple tasks and scenarios with limited variability, they struggled to scale and adapt to more complex and dynamic situations. The rigidity of these systems made it challenging to account for the vast array of possibilities and nuances found in real-world problems.\\n\\nAs AI research progressed, machine learning emerged as a more flexible and adaptive approach. Machine learning models learn patterns from training data and apply those patterns to make predictions or decisions. This data-driven approach enabled AI systems to tackle increasingly complex tasks and better generalize to new situations.\\n\\nGenerative AI represents the next leap in AI\\'s evolution, building upon machine learning\\'s foundation. Unlike traditional machine learning models that focus on discriminative tasks—determining the most likely output given an input— generative models aim to generate new data based on the patterns and distributions observed in the training data. This ability to create novel data allows generative AI to excel in a wide range of applications, including natural language processing, image synthesis, and more.\\n\\nSome of the key generative AI models include:\\n\\ne Restricted Boltzmann Machines (RBMs)\\n\\ne Variational Autoencoders (VAEs)\\n\\ne Generative Adversarial Networks (GANs)\\n\\ne Recurrent Neural Networks (RNNs)\\n\\ne Long Short-Term Memory (LSTMs)\\n\\ne Transformers\\n\\nThese models have been developed and refined over the years, leading to increasingly powerful and sophisticated AI systems. By understanding the evolution of AI and the mechanics of generative models, data scientists can better harness these cutting-edge technologies to create innovative solutions for a diverse range of challenges. 1.2. Key Generative AI Models: RNNs, LSTMs, GPT, and More\\n\\nAs generative AI has evolved, several key models have emerged, each with its own unique capabilities and strengths. In this section, we\\'ll explore some of the most prominent generative AI models, including RNNs, LSTMs, and GPT.\\n\\nRESTRICTED BOLTZMANN MAcuINEs (RBMs)\\n\\nRBMs are a type of unsupervised learning model that can learn a probability distribution over input data. They consist of two layers, a visible layer that represents input data and a hidden layer that captures latent features. RBMs have been used for various generative tasks, such as image synthesis and feature learning.\\n\\nVARIATIONAL AUTOENCODERS (VAEs)\\n\\nVAEs are a type of generative model that combines aspects of deep learning and Bayesian inference to learn complex data distributions. They consist of an encoder, which maps input data to a latent space, and a decoder, which reconstructs data from the latent space. VAEs have been used for various generative tasks, including image synthesis, text generation, and style transfer.\\n\\nGENERATIVE ADVERSARIAL NETworKs (GANSs)\\n\\nGANs consist of two neural networks, a generator and a discriminator, that compete against each other in a zero-sum game. The generator creates synthetic data, while the discriminator\\n\\n4\\n\\nattempts to distinguish between real and generated data. GANs have shown remarkable success in generating high-quality images, music, and even text.\\n\\nRECURRENT NEuRAL NETworKS (RNNs)\\n\\nRNNs are a class of neural networks designed to process sequences of data. Unlike feedforward networks, RNNs have a feedback loop that allows them to maintain an internal state, making them well-suited for tasks involving time series or sequential data. RNNs have been used for a variety of generative tasks, such as text generation, speech synthesis, and music composition.\\n\\nLone SHort-TERM Memory (LSTMs)\\n\\nLSTMs are a type of RNN specifically designed to address the vanishing gradient problem, which can occur when training RNNs on long sequences. By incorporating memory cells and gating mechanisms, LSTMs can effectively learn long-range dependencies in sequential data. They have been widely used in natural language processing, speech recognition, and other generative tasks. TRANSFORMERS\\n\\nTransformers are a type of neural network architecture that utilizes self-attention mechanisms to process input data. Unlike RNNs and LSTMs, transformers can process sequences in parallel, making them highly efficient for large-scale tasks. GPT (Generative Pre-trained Transformer) is a popular transformer-based model developed by OpenAI, known for its impressive capabilities in natural language processing and generation.\\n\\nThese generative AI models offer data scientists a powerful toolbox for tackling a wide range of applications and challenges. By understanding the strengths and limitations of each model, you can choose the most suitable approach for your specific data science project and harness the full potential of generative AI.\\n\\n1.3. Popular Use Cases for Generative AI\\n\\nGenerative AI has demonstrated significant potential in a wide range of applications, thanks to its ability to create novel data based on existing patterns. In this section, we\\'ll explore some popular use cases for generative AI, showcasing its versatility and impact across various domains.\\n\\nNaturAL LaNGuaGE ProcessInc (NLP)\\n\\nGenerative AI has revolutionized NLP by enabling the generation of coherent and contextually relevant text. Applications include chatbots, summarization, translation, content generation, and more. Models like GPT have demonstrated impressive capabilities in generating human-like text, making them invaluable for NLP tasks.\\n\\nIMAGE SYNTHESIS AND EDITING\\n\\nGenerative models like GANs and VAEs have made significant strides in image synthesis, enabling the creation of realistic images from scratch or based on specific attributes. These models can also be used for image-to-image translation, style transfer, and inpainting, offering numerous possibilities for artists, designers, and researchers.\\n\\nMusic AND AUDIO GENERATION\\n\\nGenerative AI has been used to create original music compositions, sound effects, and even speech synthesis. RNNs, LSTMs, and transformers have shown promise in capturing the structure and patterns of music, allowing for the generation of new melodies, harmonies, and rhythms. Druc DiscovERY AND MATERIAL SCIENCE\\n\\nGenerative models can be employed to generate novel molecular structures and materials with desired properties, accelerating the drug discovery and material design processes. By exploring the vast space of chemical and material compositions, generative AI can help identify promising candidates for further experimentation and testing.\\n\\nANOMALY DETECTION AND PATTERN RECOGNITION\\n\\nGenerative AI can be used to model the underlying distribution of data, making it possible to identify outliers and anomalies. This capability has valuable applications in fraud detection, network security, and quality control, among others.\\n\\nData AUGMENTATION\\n\\nIn situations where data is scarce or imbalanced, generative models can help create additional training samples, enhancing the performance of machine learning algorithms. This is particularly useful in domains where data collection is expensive, time-consuming, or ethically challenging.\\n\\nSIMULATION AND SCENARIO PLANNING\\n\\nGenerative AI can create synthetic data to simulate various scenarios, enabling businesses and researchers to test hypotheses, evaluate strategies, and make informed decisions. This can be particularly useful in fields like finance, logistics, urban planning, and environmental studies.\\n\\nThese use cases represent just a glimpse of the potential applications for generative AI. As the technology continues to advance and mature, it is likely that even more innovative and transformative applications will emerge, further solidifying the importance of generative Al in the realm of data science.\\n\\nChapter 2: Introduction to Prompt Engineering\\n\\nT The importance of prompt engineering is further underscored by the rapid growth and adoption of transformer-based models like GPT, which are heavily reliant on prompts to generate outputs.\\n\\n2.1. What Prompt Engineering is and Why it Matters\\n\\nPrompt engineering is the process of crafting effective prompts to guide AI models, particularly generative models, in generating the desired outputs. A prompt is an input given to an AI model that sets the context, goal, or constraints for the model\\'s response. The quality of a prompt can significantly influence the quality, relevance, and accuracy of the Al-generated output.\\n\\nAs generative AI models become more sophisticated and complex, the need for efficient and precise prompt engineering grows more critical. A well-crafted prompt can help maximize the potential of AI models by ensuring that they produce targeted, meaningful, and contextually appropriate responses. In contrast, an ineffective prompt may lead to ambiguous, irrelevant, or even nonsensical outputs.\\n\\nPrompt engineering plays a crucial role in the success of AI applications across various domains, from content generation and natural language processing to data analysis and visualization. By mastering the art of prompt engineering, data scientists can better direct AI models to achieve specific objectives, optimize system performance, and enhance the overall user experience.\\n\\nThe importance of prompt engineering is further underscored by the rapid growth and adoption of transformer-based models like GPT, which are heavily reliant on prompts to generate outputs. Given the vast capabilities and potential applications of these models, developing a deep understanding of prompt engineering is essential for data scientists looking to harness the power of generative Al effectively. 2.2. Prompt Types: Explicit, Implicit, and Creative Prompts\\n\\nPrompts can be categorized into different types based on their structure, purpose, and the level of guidance they provide to the AI model. Understanding the various prompt types can help data scientists craft effective prompts that yield the desired results. In this section, we will discuss three common prompt types: explicit, implicit, and creative prompts.\\n\\nEXPLiciT PROMPTS\\n\\nExplicit prompts provide clear and direct instructions to the AI model, specifying the exact format or information required in the generated output. These prompts often include keywords or phrases that guide the model towards a specific response. For example, an explicit prompt for a translation task might be, \\'\\'Translate the following English text to French: \\'The weather is nice today.\\'\" Explicit prompts are generally easier for AI models to interpret and can lead to more accurate and relevant results. However, they may sometimes limit the model\\'s ability to generate creative or nuanced outputs.\\n\\nIMPLICIT PROMPTS\\n\\nImplicit prompts are less direct in their instructions, allowing the AI model more freedom to interpret the desired outcome. These prompts rely on the model\\'s understanding of context, relationships, or conventions to generate an appropriate response. For example, an implicit prompt for a translation task might be, \\'How would you say \\'The weather is nice today\\' in French?\" Implicit prompts can encourage AI models to think more creatively and generate more diverse outputs. However, they may also increase the risk of generating ambiguous or off-topic responses.\\n\\nCREATIVE PROMPTS\\n\\nCreative prompts are designed to encourage AI models to generate novel, imaginative, or unconventional outputs. These prompts often involve open-ended questions, scenarios, or challenges that require the model to think beyond its training data and explore new ideas or perspectives. For example, a creative prompt for a storytelling task might be, \\'\\'Write a short story about a world where the weather changes based on people\\'s emotions.\" Creative prompts can help data scientists tap into the full potential of generative AI, enabling the creation of unique and engaging content. However, they may also require more iteration and fine-tuning to achieve the desired results.\\n\\nBy understanding the different prompt types and their respective strengths and limitations, data scientists can choose the most appropriate approach for their specific AI application. Additionally, mastering the art of crafting various prompt types can help data scientists harness the full potential of generative AI models and optimize their performance across a diverse range of tasks. 2.3. Best Practices for Crafting Effective Prompts\\n\\nCreating effective prompts is an essential skill for data scientists working with generative AI models. The following best practices can help you craft prompts that yield accurate, relevant, and meaningful outputs while minimizing the risk of generating ambiguous or off-topic responses.\\n\\nBE CLEAR AND CONCISE\\n\\nEnsure that your prompt is easy to understand and provides clear instructions for the Al model. Avoid using overly complex language or unnecessary jargon that may confuse the model. Keeping your prompt concise can also help the model focus on the essential information and generate more accurate results.\\n\\nPROVIDE CONTEXT\\n\\nIncluding context in your prompt can help guide the AI model towards a more relevant and accurate output. For example, if you\\'re asking the model to generate a summary of an article, providing the article\\'s title, author, and publication date can help the model understand the context and generate a more appropriate summary.\\n\\nSPECIFY THE DESIRED FORMAT\\n\\nIf you have a specific format or structure in mind for the generated output, be sure to include this information in your prompt. For example, if you want the model to generate a bulleted list or a numbered sequence, explicitly mention this in your prompt to guide the model accordingly.\\n\\nENCOURAGE MULTIPLE ATTEMPTS\\n\\nGenerative AI models can sometimes produce unexpected or undesirable outputs. If the initial output does not meet your expectations, try rephrasing your prompt or adjusting its parameters to encourage the model to generate a different response. Iterating on your prompt can help you fine-tune its effectiveness and achieve the desired results.\\n\\n10\\n\\nBALANCE GUIDANCE AND FREEDOM\\n\\nStriking the right balance between providing guidance and allowing the AI model creative freedom is crucial for crafting effective prompts. Overly restrictive prompts can limit the model\\'s ability to generate creative or nuanced outputs, while overly open-ended prompts may lead to ambiguous or off-topic results. Experiment with different levels of guidance to find the sweet spot that best aligns with your project\\'s objectives. EVALUATE AND ITERATE\\n\\nRegularly evaluate the effectiveness of your prompts by reviewing the Al-generated outputs and comparing them against your desired outcomes. Use this feedback to refine and improve your prompts, ensuring that they consistently yield high-quality results. Iterative prompt engineering is an essential part of the process, as it allows you to continuously optimize the performance of your generative AI models.\\n\\nBy following these best practices, you can create effective prompts that guide generative AI models towards generating accurate, relevant, and meaningful outputs. Mastering the art of prompt engineering is crucial for data scientists looking to harness the full potential of generative AI and optimize its performance across a wide range of applications.\\n\\n11\\n\\nChapter 3: Practical Applications of Prompt Engineering\\n\\nTT\\n\\nCustom prompts not only help guide AI models towards generating more accurate and contextually relevant outputs, but they also unlock the full potential of generative AI to tackle complex language- based challenges.\\n\\n3.1. Improving NLP Tasks with Custom Prompts\\n\\nPrompt engineering plays a critical role in enhancing the performance of generative AI models, particularly in the realm of natural language processing (NLP). Custom prompts can help guide AI models to generate more accurate, contextually relevant, and engaging outputs across a variety of NLP tasks. In this section, we\\'ll explore some common NLP tasks where prompt engineering can make a significant impact.\\n\\nTEXT SUMMARIZATION\\n\\nCrafting an effective prompt for text summarization involves specifying the desired length, format, and key points that the summary should cover. By providing clear instructions and context, you can guide the AI model to generate concise and informative summaries that accurately capture the essence of the source text.\\n\\nExamples of effective prompts for text summarization include:\\n\\ne \"Write a concise summary of this news article about the latest advancements in artificial intelligence, focusing on the main breakthroughs and their potential impact on various industries.\"\\n\\ne \"Summarize this research paper on the effects of climate change on biodiversity, highlighting the key findings and the implications for conservation efforts.\"\\n\\ne \"Provide a brief summary of this book chapter on the history of the internet, covering the major developments and their significance for the way we communicate and access information today.\" SENTIMENT ANALYSIS\\n\\nFor sentiment analysis, prompts should be designed to encourage the AI model to focus on the relevant aspects of the text and evaluate the underlying sentiment. A well-crafted prompt can help the model identify positive, negative, or neutral sentiments more accurately, improving the overall performance of sentiment analysis tasks.\\n\\nExamples of effective prompts for sentiment analysis include:\\n\\ne \"What emotions do customers express in their online reviews of our new product?\" This prompt asks for sentiment analysis on customer reviews of a specific product. It is well-crafted because it is specific and provides a clear context for the sentiment analysis task.\\n\\ne \"How do Twitter users feel about the latest political controversy?\"\\n\\nThis prompt asks for sentiment analysis on a specific topic being discussed on Twitter. It is well-crafted because it is specific and provides a clear context for the sentiment analysis task.\\n\\ne \"What is the overall sentiment of movie reviews for the latest blockbuster film?\" This prompt asks for sentiment analysis on a collection of movie reviews for a specific film. It is well-crafted because it is specific and provides a clear context for the sentiment analysis task.\\n\\nText GENERATION\\n\\nWhether you\\'re generating creative content, news articles, or product descriptions, crafting an effective prompt is crucial for guiding the AI model towards generating contextually relevant and engaging text. Including specific details, themes, or keywords in your prompt can help the model generate more targeted and coherent outputs.\\n\\nExamples of effective prompts for text generation include:\\n\\ne \\'\"Write a short story about a person who discovers a mysterious object in their backyard and is transported to a different dimension.\"\\n\\ne \"Imagine a future world where technology has advanced to the point where humans can implant memories in their brains. Write a news article describing the controversy surrounding this new technology.\"\\n\\ne \\'Write a poem about the changing of the seasons, exploring the different emotions and sensations that come with each season.\" QUESTION-ANSWERING\\n\\nIn question-answering tasks, prompts should be designed to convey the exact information required in the answer. Providing context, such as the source text or relevant background information, can help the AI model generate more accurate and comprehensive answers to user questions.\\n\\nExamples of effective prompts for question-answering include:\\n\\ne ‘What was the main cause of World War II and how did it affect the global political landscape?\"\\n\\ne \"Can you explain the concept of quantum entanglement and how it relates to the theory of relativity?\"\\n\\ne ‘What are some of the most effective methods for reducing carbon emissions and mitigating climate change, and how have these strategies been implemented in different parts of the world?\"\\n\\nText CLASSIFICATION\\n\\nFor text classification tasks, prompts should be crafted to guide the AI model towards identifying the relevant category or label for a given text. Including examples of texts belonging to different categories or providing explicit instructions can help the AI model better understand the classification criteria and improve its performance.\\n\\nExamples of effective prompts for text classification include:\\n\\ne Topic Classification: \"Given a set of news articles, classify each article into one of several categories such as Politics, Sports, Business, or Entertainment.\"\\n\\ne Intent Classification: \"Given a set of customer queries, classify each query into one of several categories such as Sales, Support, Technical Issues, or Feedback.\"\\n\\ne Spam Detection: \"Given a set of email messages, classify each message as spam or not spam.\"\\n\\nMACHINE TRANSLATION\\n\\nIn machine translation tasks, prompts should be designed to convey the desired language and context for the translation. Providing clear instructions and specifying any specific formatting or style requirements can help the AI model generate more accurate and fluent translations.\\n\\nExamples of effective prompts for machine translation include:\\n\\ne Translate the following sentence from English to French: \\'\\'The quick brown fox jumps over the lazy dog.\"\\n\\ne Convert the following document from Spanish to English: \\'\\'El cambio climatico es uno de los mayores desafios que enfrenta la humanidad en la actualidad.\"\\n\\ne Translate the following Chinese text to Spanish: \"ix Maki AVES i SEAN SCL BR, vie FEY A Se BU PAR TEA SCE ok FB AT ED HG BBS\"\\n\\nBy mastering the art of prompt engineering, data scientists can improve the performance of generative AI models across a wide range of NLP tasks. Custom prompts not only help guide AI models towards generating more accurate and contextually relevant outputs, but they also unlock the full potential of generative AI to tackle complex language-based challenges. 3.2. Enhancing Creativity and Diversity in Al-Generated Content\\n\\nOne of the key advantages of generative AI models is their ability to create novel and diverse content, whether it\\'s text, images, or audio. Prompt engineering can play a crucial role in encouraging AI models to generate more creative and diverse outputs, enhancing the overall quality and appeal of the generated content. In this section, we\\'ll discuss how prompt engineering can be used to boost creativity and diversity in Al- generated content.\\n\\nOPEN-ENDED PROMPTS\\n\\nUsing open-ended prompts that encourage exploration and imagination can help AI models generate more creative content. For example, asking the model to Write a story set in a world where time flows backward\" can inspire the model to come up with unique ideas and scenarios that go beyond its training data.\\n\\nCONSTRAINTS AND CHALLENGES\\n\\nIntroducing constraints or challenges in your prompts can push AI models to think more creatively and find innovative solutions. For example, asking the model to \"Write a story without using the letter \\'e\\'\\'\\' can force the model to use unusual words and phrasing, resulting in more inventive content.\\n\\nComBINING IDEAS AND THEMES\\n\\nCrafting prompts that combine multiple ideas or themes can encourage AI models to generate more diverse and engaging content. For example, asking the model to \"Write a story that combines elements of science fiction and romance\" can lead to the creation of unique and unexpected narratives.\\n\\nENCOURAGING VARIABILITY\\n\\nPrompting the AI model to generate multiple versions of the same content can help increase diversity and provide a broader range of options to choose from. For example, you can ask the model to \"Generate three different endings for the following story\" to explore various narrative possibilities.\\n\\nMIXING STYLES AND FORMATS\\n\\nExperimenting with different styles, formats, or genres in your prompts can lead to more creative and diverse outputs. For example, you can prompt the model to \"Write a poem in the style of anews report\" or \\'\\'Reimagine a classic fairy tale as a modern-day thriller.\" ITERATIVE PROMPTING\\n\\nPrompting the AI model to build upon or refine its previous outputs can lead to more nuanced and sophisticated content. For example, you can use the model\\'s initial output as a starting point and prompt it to \"Expand on the story by adding more details and depth to the characters and plot.\"\\n\\nBy employing these strategies in prompt engineering, data scientists can harness the full creative potential of generative AI models and generate more diverse and engaging content. This not only enhances the overall quality of Al-generated content but also expands the range of applications and possibilities for generative AI in various domains, from entertainment and marketing to education and research.\\n\\n3-3. Addressing AI Ethics and Bias through Thoughtful Prompt Engineering\\n\\nGenerative AI models have the potential to perpetuate biases present in their training data, which can lead to ethical concerns and unintended consequences. Prompt engineering can play a critical role in addressing these issues by guiding AI models to generate more balanced, fair, and responsible outputs. In this section, we\\'ll discuss how thoughtful prompt engineering can help address AI ethics and bias concerns.\\n\\n16\\n\\nENCOURAGING FAIRNESS AND INCLUSIVITY\\n\\nCrafting prompts that emphasize fairness and inclusivity can help guide AI models towards generating more balanced and representative content. For example, you can ask the model to \"Generate a story that features a diverse cast of characters, each with unique backgrounds and perspectives.\"\\n\\nAVOIDING STEREOTYPES AND DISCRIMINATION\\n\\nTo prevent AI models from perpetuating stereotypes or discrimination, design prompts that explicitly discourage such behavior. For example, instruct the model to \"Write a character description that avoids stereotypes and showcases the individual\\'s unique qualities and experiences.\"\\n\\nPROMOTING POSITIVE AND RESPONSIBLE CONTENT\\n\\nCreating prompts that encourage AI models to generate positive, uplifting, and responsible content can help counteract negative biases and promote more ethical Al-generated outputs. For example, you can prompt the model to \"Write an inspiring story about individuals overcoming adversity through collaboration and empathy.\" FACT-CHECKING AND VERIFICATION\\n\\nIn cases where the Al-generated content involves factual information or claims, crafting prompts that emphasize the importance of accuracy and truthfulness can help mitigate the risk of misinformation. For example, you can instruct the model to \"Provide a well-researched and fact-checked summary of the key events and developments related to climate change in the past decade.\"\\n\\nITERATIVE REFINEMENT\\n\\nContinuously refining and iterating on your prompts can help identify and address potential biases or ethical concerns in the Al-generated outputs. Regularly evaluate the generated content for fairness, accuracy, and responsibility, and use this feedback to improve your prompts and guide the AI model towards more ethical outcomes.\\n\\nUsER FEEDBACK AND COLLABORATION\\n\\nInvolving users or stakeholders in the prompt engineering process can provide valuable insights into potential biases or ethical concerns, leading to more thoughtful and responsible Al-generated content. Encourage users to provide feedback on the generated outputs, and collaborate with them to refine and improve your prompts.\\n\\n17\\n\\nBy adopting these strategies in prompt engineering, data scientists can proactively address AI ethics and bias concerns and guide generative AI models towards generating more responsible, fair, and balanced outputs. Thoughtful prompt engineering not only contributes to the ethical use of AI technologies but also enhances the overall value and impact of Al-generated content in various applications and domains.\\n\\n3-4. Personalization and Adaptability in Al-Generated Content\\n\\nGenerative AI models hold immense potential for creating personalized and adaptable content tailored to individual users\\' needs, preferences, or contexts. Prompt engineering plays a vital role in enabling AI models to generate such customized content by incorporating user-specific information and guiding the model to adapt its outputs accordingly. In this section, we\\'ll discuss how prompt engineering can be employed to create personalized and adaptable Al-generated content. INCORPORATING USER PREFERENCES\\n\\nDesign prompts that account for the user\\'s preferences, such as their favorite topics, genres, or styles, to generate content that aligns with their interests. For example, you can prompt the model to \"Write a science fiction story based on the user\\'s favorite themes: time travel and alternate realities.\"\\n\\nApjusTING LANGUAGE AND TONE\\n\\nCraft prompts that consider the user\\'s preferred language, tone, or level of formality to create content that resonates with their communication style. For example, you can instruct the model to \"Write a summary of the latest technology news using simple, non-technical language for a general audience.\"\\n\\nADAPTING TO CONTEXT\\n\\nDesign prompts that take into account the user\\'s context, such as their location, cultural background, or current situation, to generate more relevant and engaging content. For example, you can prompt the model to \\'\\'Generate a list of fun weekend activities tailored to the user\\'s current city and interests.\"\\n\\nLEARNING FROM USER INTERACTIONS\\n\\nLeverage user feedback and interactions to iteratively refine your prompts and guide the AI model to generate content that better aligns with the user\\'s needs and preferences over time.\\n\\n18\\n\\nFor example, you can update your prompts based on the user\\'s feedback on previous Al-generated content or incorporate their content consumption patterns.\\n\\nPERSONALIZED RECOMMENDATIONS\\n\\nCraft prompts that enable AI models to generate personalized recommendations, such as books, movies, or products, based on the user\\'s preferences, browsing history, or demographic information. For example, you can prompt the model to \\'\\'Recommend a list of five books in the mystery genre that the user might enjoy based on their reading history.\"\\n\\nADAPTIVE LEARNING AND TUTORING\\n\\nUse prompt engineering to create personalized learning experiences tailored to individual learners\\' needs, interests, and skill levels. For example, you can prompt the AI model to \"Generate a customized lesson plan on Python programming for a beginner-level student with a strong interest in data analysis.\"\\n\\nBy employing these strategies in prompt engineering, data scientists can create personalized and adaptable Al-generated content that caters to individual users\\' needs and preferences. This not only enhances user satisfaction and engagement but also unlocks new possibilities for leveraging generative AI in diverse applications, from content recommendation and personalization to adaptive learning and tutoring.\\n\\nChapter 4: Challenges and Limitations of Prompt Engineering\\n\\nT\\n\\nBy acknowledging and addressing these challenges and limitations, data scientists can develop more effective and responsible prompt engineering strategies. 4.1. Understanding AI Model Limitations and Inherent Biases\\n\\nWhile prompt engineering can significantly enhance the performance and usability of generative AI models, it\\'s essential to acknowledge the inherent limitations and biases present in these models. Understanding these limitations can help data scientists set realistic expectations, make informed decisions, and develop more robust and reliable AI solutions. In this section, we\\'ll discuss some of the key challenges and limitations associated with prompt engineering.\\n\\nMobet LIMITATIONS\\n\\nGenerative AI models, like any other machine learning model, have limitations stemming from their training data, architecture, and other factors. These limitations can sometimes lead to unexpected, irrelevant, or nonsensical outputs, even with well-crafted prompts. Recognizing these limitations can help data scientists develop strategies to mitigate their impact or explore alternative approaches.\\n\\nINHERENT BIASES\\n\\nAI models may inherit biases present in their training data, which can inadvertently perpetuate stereotypes, discrimination, or misinformation. While prompt engineering can help address some of these biases, it\\'s essential to be aware that biases may still emerge in the Al-generated content.\\n\\nUNPREDICTABILITY\\n\\nGenerative AI models can sometimes produce outputs that are surprising or unexpected, even with carefully engineered prompts. Managing this unpredictability can be challenging, and\\n\\n20\\n\\ndata scientists may need to iterate and refine their prompts multiple times to achieve the desired results.\\n\\nOVERFITTING\\n\\nCrafting overly specific or complex prompts can lead to overfitting, where the AI model generates outputs that are too narrowly focused or adherent to the prompt\\'s constraints, limiting its creativity or usefulness. Striking the right balance between guidance and freedom is crucial for effective prompt engineering.\\n\\nEVALUATION AND FEEDBACK\\n\\nEvaluating the effectiveness of prompts and Al-generated outputs can be challenging, particularly for subjective or creative tasks. Developing robust evaluation methods and incorporating user feedback can help data scientists iterate on their prompts and enhance the performance of their AI solutions. ETHICAL CONSIDERATIONS\\n\\nPrompt engineering raises ethical concerns, such as the potential to manipulate users, spread misinformation, or reinforce biases. Data scientists should approach prompt engineering with a sense of responsibility and consider the potential consequences of their Al-generated content.\\n\\nBy acknowledging and addressing these challenges and limitations, data scientists can develop more effective and responsible prompt engineering strategies. This understanding not only helps improve the performance of generative AI models but also contributes to the development of more robust, reliable, and ethical AI solutions across various applications and domains.\\n\\n4.2. Striking the Right Balance between Guidance and Flexibility\\n\\nOne of the key challenges in prompt engineering is finding the right balance between providing enough guidance to the AI model and allowing for sufficient flexibility to generate diverse and creative outputs. Striking this balance is crucial for maximizing the potential of generative AI models while ensuring that their outputs are relevant, accurate, and engaging. In this section, we\\'ll discuss some strategies for finding the right balance between guidance and flexibility in prompt engineering.\\n\\n21\\n\\nSTART WITH BROAD PROMPTS\\n\\nBegin with broad, open-ended prompts that allow the AI model to explore a wide range of ideas and possibilities. This approach can help you gauge the model\\'s capabilities and identify any limitations or biases in its outputs. From there, you can incrementally refine the prompt to provide more guidance and structure.\\n\\nEXPERIMENT WITH DIFFERENT LEVELS OF DETAIL\\n\\nTry using different levels of detail in your prompts, ranging from very specific to more general, and observe how the AI model responds. This experimentation can help you identify the optimal level of guidance required to generate the desired outputs without sacrificing creativity or diversity.\\n\\nITERATE AND REFINE\\n\\nPrompt engineering is often an iterative process, involving trial and error as you refine and adjust your prompts to achieve the best results. Regularly evaluate the Al-generated outputs, and use this feedback to fine-tune your prompts and strike the right balance between guidance and flexibility. ENCOURAGE VARIABILITY\\n\\nDesign prompts that encourage the AI model to generate multiple variations or interpretations of the same content. This can help ensure that the model remains creative and diverse while still adhering to the overall constraints and goals of the prompt.\\n\\nLEVERAGE CONSTRAINTS AS A CREATIVE TOOL\\n\\nIntroducing constraints or challenges in your prompts can be an effective way to guide the AI model while encouraging creative problem-solving. Instead of viewing constraints as limitations, use them as tools to inspire the AI model to generate more inventive and engaging content.\\n\\nCOLLABORATE WITH USERS\\n\\nIncorporate user feedback and preferences into the prompt engineering process, and collaborate with users to identify the right balance between guidance and flexibility. This can help ensure that the Al-generated content is relevant, engaging, and tailored to the user\\'s needs and expectations.\\n\\n22\\n\\nBy employing these strategies, data scientists can strike the right balance between guidance and flexibility in prompt engineering, maximizing the potential of generative AI models while ensuring that their outputs are relevant, accurate, and engaging. This delicate balance not only enhances the effectiveness of Al-generated content but also opens new opportunities for leveraging generative AI in diverse applications and domains.\\n\\n4.3. Ensuring Quality and Reliability in AI-Generated Content\\n\\nAs Al-generated content becomes more prevalent, ensuring its quality and reliability is crucial for maintaining user trust and delivering value. Prompt engineering plays a vital role in guiding AI models to generate high-quality content, but it also presents challenges in terms of consistency, accuracy, and relevance. In this section, we\\'ll discuss some strategies for overcoming these challenges and ensuring the quality and reliability of Al-generated content through prompt engineering.\\n\\nRigorous TESTING AND EVALUATION\\n\\nRegularly test and evaluate the Al-generated content using a combination of quantitative metrics and qualitative assessments. Establish clear benchmarks and performance criteria to measure the quality, relevance, and accuracy of the content and use this feedback to refine your prompts and AI models. UsER FEEDBACK AND COLLABORATION\\n\\nIncorporate user feedback into the prompt engineering process to ensure that the Al-generated content meets their needs, preferences, and expectations. Collaborate with users to identify areas for improvement and implement changes to your prompts and models accordingly.\\n\\nContinuous MopDEL IMPROVEMENT\\n\\nStay up to date with the latest advancements in AI research and incorporate new techniques, models, or approaches to improve the performance and reliability of your generative AI solutions. Continuously iterate on and refine your AI models based on user feedback, evaluation results, and industry best practices.\\n\\nRosust Error HANDLING AND MONITORING\\n\\nDevelop robust error handling and monitoring mechanisms to detect and address potential issues in the Al-generated content, such as inaccuracies, inconsistencies, or offensive content.\\n\\n23\\n\\nImplement safeguards and filters to prevent the dissemination of low-quality or harmful content and ensure that Al-generated outputs meet established quality standards.\\n\\nCLEAR COMMUNICATION AND TRANSPARENCY\\n\\nClearly communicate the limitations and potential risks associated with Al-generated content to users, and be transparent about how the content is created, evaluated, and refined. This transparency can help manage user expectations, build trust, and promote responsible use of Al-generated content.\\n\\nMULTIMODAL VALIDATION\\n\\nWhen possible, leverage multimodal validation approaches, such as combining text, images, and other data sources, to enhance the reliability and accuracy of Al-generated content. This can help provide additional context and support for the generated content, increasing its overall quality and trustworthiness.\\n\\nBy adopting these strategies, data scientists can overcome the challenges associated with ensuring quality and reliability in Al- generated content and deliver more consistent, accurate, and relevant outputs through prompt engineering. This commitment to quality and reliability not only enhances user trust and satisfaction but also contributes to the broader adoption and success of generative AI technologies across various applications and domains.\\n\\n24\\n\\nChapter 5: Future Directions and Emerging Trends in Prompt Engineering TT\\n\\nThese emerging trends and future directions not only promise to enhance the capabilities and performance of generative AI systems but also contribute to the broader adoption and success of AI technologies across various applications and domains.\\n\\n5.1. Leveraging Advanced AI Models and Techniques\\n\\nAs AI research continues to advance, new models, techniques, and approaches are being developed that can enhance the capabilities and performance of generative AI systems. These advancements hold the potential to revolutionize prompt engineering and enable even more sophisticated, creative, and effective Al-generated content. In this section, we\\'ll explore some of the emerging trends and future directions in prompt engineering that leverage these advanced AI models and techniques.\\n\\nFINE-TUNING AND TRANSFER LEARNING\\n\\nFine-tuning pre-trained AI models on domain-specific datasets can help improve their performance and relevance for specific tasks or industries. This approach can enhance the effectiveness of prompt engineering by enabling AI models to generate content that is more tailored to the user\\'s needs and context.\\n\\nMutti-mopat AI MopELs\\n\\nThe integration of multi-modal AI models, capable of processing and generating content across different modalities (e.g., text, images, audio), can expand the scope and capabilities of prompt engineering. These models can generate more engaging and immersive content by combining text with visuals, audio, or other forms of media.\\n\\n25\\n\\nCoNnTEXTUAL AI AND Memory MECHANISMS\\n\\nIncorporating contextual AI models and memory mechanisms can help generative AI systems better understand and adapt to the user\\'s context, history, and preferences. This can lead to more personalized, relevant, and engaging Al-generated content through prompt engineering.\\n\\nAcTIVE LEARNING AND USER INTERACTION\\n\\nEmploying active learning techniques, where the AI model iteratively refines its understanding and performance based on user feedback and interactions, can help improve the effectiveness of prompt engineering. This approach enables AI models to generate content that is better aligned with user needs and preferences over time. COLLABORATIVE AI SysTEMS\\n\\nDeveloping AI systems that can collaborate with human users in real-time, such as co-writing or co-designing, can unlock new possibilities for prompt engineering. These collaborative AI systems can help users generate content more effectively, efficiently, and creatively by leveraging the strengths of both humans and AI models.\\n\\nEruicat AI AND FAIRNESS-AWARE PROMPT ENGINEERING\\n\\nAs ethical considerations become increasingly important in AI research and development, incorporating fairness-aware and ethical principles into prompt engineering will be crucial. This can help ensure that Al-generated content is not only accurate and engaging but also fair, inclusive, and responsible.\\n\\nBy embracing these advanced AI models and techniques, data scientists can drive the evolution of prompt engineering and unlock new possibilities for Al-generated content. These emerging trends and future directions not only promise to enhance the capabilities and performance of generative AI systems but also contribute to the broader adoption and success of AI technologies across various applications and domains.\\n\\n5.2. The Convergence of Human and AI Creativity\\n\\nAs Al-generated content becomes more sophisticated and capable, the convergence of human and AI creativity presents exciting opportunities for the future of prompt engineering. The synergy between human intuition and AI\\'s computational power can lead to innovative and transformative applications in content generation, creativity, and problem-solving. In this\\n\\n26\\n\\nsection, we\\'ll discuss some of the ways in which human and AI creativity can converge in the context of prompt engineering.\\n\\nHuman-AI CoLlaBorATION\\n\\nDeveloping AI systems that work alongside humans as partners can enable new forms of creative collaboration. By leveraging the complementary strengths of human intuition, expertise, and creativity with AI\\'s computational power and pattern recognition, this partnership can lead to novel and innovative ideas and solutions. AUGMENTING HUMAN CREATIVITY\\n\\nAl-generated content can serve as a valuable resource for inspiring and augmenting human creativity. By providing diverse ideas, perspectives, and suggestions, AI models can help humans overcome creative blocks, explore new directions, or refine their ideas.\\n\\nCREATIVE EDUCATION AND TRAINING\\n\\nGenerative AI models can play a pivotal role in creative education and training by providing personalized learning experiences, interactive simulations, and adaptive feedback. Prompt engineering can be employed to design educational content that nurtures and enhances human creativity, critical thinking, and problem-solving skills.\\n\\nTHE DEMOCRATIZATION OF CREATIVITY\\n\\nAs Al-generated content becomes more accessible and affordable, it can help democratize creativity by empowering individuals and organizations with limited resources to generate high-quality content, explore new ideas, or solve complex problems.\\n\\nETHICAL AND RESPONSIBLE CREATIVITY\\n\\nThe convergence of human and AI creativity raises ethical questions and challenges. By fostering open dialogue, collaboration, and responsible AI practices, we can ensure that the combined potential of human and AI creativity is harnessed for the greater good and the development of a more inclusive and equitable society.\\n\\nCROSS-DISCIPLINARY INNOVATIONS\\n\\nThe integration of Al-generated content and prompt engineering across various disciplines, such as art, science, technology, and humanities, can lead to groundbreaking innovations and discoveries. By breaking down traditional boundaries and fostering cross-disciplinary collaboration, human and AI creativity can together drive transformative progress.\\n\\n27\\n\\nBy embracing the convergence of human and AI creativity, we can unlock new possibilities for the future of prompt engineering and the broader landscape of content generation, creativity, and problem-solving. This synergy not only promises to enhance the capabilities and performance of generative AI systems but also contributes to the development of more innovative, inclusive, and equitable solutions across various applications and domains. 5.3. The Role of Prompt Engineering in the Al-Driven Economy\\n\\nAs AI technologies continue to advance and reshape various aspects of the global economy, the role of prompt engineering is poised to grow in importance and impact. From content generation to decision-making, Al-driven solutions are increasingly becoming a key component of businesses, governments, and organizations worldwide. In this section, we\\'ll discuss how prompt engineering can play a crucial role in the Al-driven economy and enable more effective, efficient, and innovative solutions across diverse sectors.\\n\\nENHANCING BusINESS COMMUNICATIONS\\n\\nPrompt engineering can revolutionize business communications by generating personalized, engaging, and high-quality content tailored to specific audiences, channels, or objectives. From marketing and advertising to customer support and internal communications, prompt engineering can help organizations optimize their messaging and reach.\\n\\nACCELERATING RESEARCH AND DEVELOPMENT\\n\\nPrompt engineering can be utilized to synthesize vast amounts of information, generate insights, and identify trends, patterns, or opportunities. This can enable researchers and organizations to accelerate their R&D processes, make more informed decisions, and drive innovation across various domains.\\n\\nPERSONALIZING CUSTOMER EXPERIENCES\\n\\nBy generating personalized content and recommendations based on user preferences, behavior, or context, prompt engineering can help businesses and organizations deliver more relevant, engaging, and valuable customer experiences across diverse channels and touchpoints.\\n\\n28\\n\\nDEMocrRATIZING ACCESS TO EXPERTISE\\n\\nPrompt engineering can be employed to generate expert knowledge, guidance, or advice ina wide range of fields, making specialized expertise more accessible and affordable for individuals, businesses, and communities worldwide.\\n\\nSUPPORTING POLICY-MAKING AND GOVERNANCE\\n\\nPrompt engineering can play a critical role in policy-making and governance by generating evidence-based insights, forecasts, or recommendations. This can help governments and organizations make more informed, effective, and equitable decisions that better serve the needs and interests of their constituents. FosTERING CREATIVITY AND INNOVATION\\n\\nThe combination of human and AI creativity can lead to groundbreaking innovations and discoveries across various disciplines, as discussed in Section 5.2. Prompt engineering can act as a catalyst for creativity, enabling individuals and organizations to explore new ideas, challenge assumptions, and push the boundaries of what\\'s possible.\\n\\nADDRESSING GLOBAL CHALLENGES\\n\\nPrompt engineering can be harnessed to generate solutions, strategies, or ideas to address complex global challenges, such as climate change, poverty, or inequality. By leveraging Al-generated content and insights, we can develop more effective, scalable, and sustainable approaches to tackling pressing societal issues.\\n\\nAs the Al-driven economy continues to evolve and expand, the role of prompt engineering will become increasingly significant and transformative across diverse sectors and domains. By embracing prompt engineering and its potential, we can enable more effective, efficient, and innovative solutions that drive progress, enhance well-being, and create a more prosperous and equitable future for all.\\n\\n29\\n\\nChapter 6: Practical Tips and Best Practices for Prompt Engineering\\n\\nT By following these practical tips and best practices, you can get started with prompt engineering and harness its potential to create high-quality, engaging, and effective Al-generated content.\\n\\n6.1. Getting Started with Prompt Engineering\\n\\nFor data scientists and practitioners looking to harness the power of prompt engineering for generative AI applications, getting started can seem daunting. However, by following some practical tips and best practices, you can begin leveraging prompt engineering effectively and efficiently. In this section, we\\'ll provide guidance on how to get started with prompt engineering and ensure a successful implementation.\\n\\nUNDERSTAND Your AI MoDEL\\n\\nBefore diving into prompt engineering, familiarize yourself with the AI model you\\'re working with, its capabilities, and its limitations. Understanding the model\\'s architecture, pre-training data, and fine-tuning methods can help you craft better prompts and anticipate potential issues. DEFINE CLEAR OBJECTIVES\\n\\nEstablish clear objectives for the Al-generated content, considering factors such as the target audience, intended purpose, and desired outcome. These objectives will guide your prompt engineering efforts and help ensure the generated content is relevant, engaging, and effective.\\n\\nSTART SIMPLE\\n\\nBegin with simple, open-ended prompts to gauge the AI model\\'s capabilities and identify any limitations or biases in its outputs. From there, you can incrementally refine the prompt to provide more guidance and structure based on the model\\'s performance and your objectives.\\n\\n30\\n\\nITERATE AND EXPERIMENT\\n\\nPrompt engineering is often an iterative process, involving trial and error as you refine and adjust your prompts to achieve the best results. Experiment with different prompt structures, guidance levels, and context to identify the optimal approach for your specific application.\\n\\nEVALUATE AND MoNnITOR\\n\\nRegularly evaluate the Al-generated content using a combination of quantitative metrics and qualitative assessments. Establish clear benchmarks and performance criteria to measure the quality, relevance, and accuracy of the content, and use this feedback to refine your prompts and AI model.\\n\\nCOLLABORATE AND LEARN FROM OTHERS\\n\\nPrompt engineering is a growing field, with a vibrant community of researchers and practitioners sharing insights, techniques, and best practices. Engage with the community, learn from others\\' experiences, and collaborate on projects to continuously improve your prompt engineering skills and knowledge.\\n\\nBe MINDFUL OF ETHICS AND RESPONSIBILITY\\n\\nAs you develop Al-generated content, always be mindful of the ethical implications and potential risks associated with your work. Ensure that your prompts and AI models adhere to ethical guidelines and responsible AI practices, and strive to create content that is fair, inclusive, and respectful.\\n\\nBy following these practical tips and best practices, you can get started with prompt engineering and harness its potential to create high-quality, engaging, and effective Al-generated content. As you gain experience and expertise, you\\'ll be better equipped to tackle more complex applications and challenges, driving innovation and progress in the rapidly evolving field of generative AI. 6.2. Building an Effective Prompt Engineering Workflow\\n\\nDeveloping a structured and efficient prompt engineering workflow is essential for creating high-quality Al-generated content consistently. In this section, we\\'ll discuss the key components of an effective prompt engineering workflow and provide practical tips on how to streamline and optimize your processes.\\n\\n31\\n\\nPLANNING AND RESEARCH\\n\\nStart by conducting thorough research on your topic, target audience, and the AI model you\\'ll be using. This research will help you identify the most relevant information, trends, and insights, and lay the groundwork for crafting effective prompts.\\n\\nOBJECTIVE SETTING\\n\\nDefine clear objectives for the Al-generated content, taking into consideration factors such as purpose, audience, and desired outcomes. Use these objectives as a guide throughout the prompt engineering process to ensure that the generated content is aligned with your goals.\\n\\nDrafFTING INITIAL PROMPTS\\n\\nBased on your research and objectives, draft a set of initial prompts that provide guidance and context for the AI model. Start simple, and iteratively refine the prompts as needed to improve the quality and relevance of the Al-generated content.\\n\\nTESTING AND EVALUATION\\n\\nTest your initial prompts using the AI model and evaluate the generated content based on established benchmarks and performance criteria. Identify any issues, biases, or inaccuracies in the content, and use this feedback to refine your prompts and improve the AI model\\'s performance.\\n\\nITERATION AND REFINEMENT\\n\\nPrompt engineering is an iterative process, so continually refine and adjust your prompts based on the Al-generated content\\'s performance and feedback. Experiment with different prompt structures, guidance levels, and context to optimize the quality and relevance of the generated content.\\n\\nMoNITORING AND MAINTENANCE\\n\\nRegularly monitor the performance of your Al-generated content and the effectiveness of your prompts. Keep track of any changes in the target audience, topic, or AI model, and adjust your prompts accordingly to maintain content quality and relevance. COLLABORATION AND CONTINUOUS LEARNING\\n\\nCollaborate with other data scientists, researchers, and practitioners to share insights, techniques, and best practices. Stay up to date with the latest advancements in AI research and\\n\\n32\\n\\nprompt engineering, and continuously improve your skills and knowledge to stay ahead in the field.\\n\\nBy building an effective prompt engineering workflow, you can streamline your processes, enhance the quality and relevance of your Al- generated content, and ensure that your projects are aligned with your objectives and goals. This structured approach not only helps you create engaging and effective content but also contributes to your long-term success and growth in the rapidly evolving field of generative AI.\\n\\n6.3. Overcoming Common Challenges in Prompt Engineering\\n\\nPrompt engineering can present a variety of challenges, from addressing AI model limitations to crafting effective prompts that generate the desired content. In this section, we\\'ll discuss some common challenges in prompt engineering and provide practical tips on how to overcome them.\\n\\nDEALING WITH AI MobEt LIMITATIONS\\n\\nAI models may have limitations in their understanding of certain topics, contexts, or nuances. To address these limitations, you can experiment with different prompt structures, fine-tune the AI model on domain-specific data, or combine the outputs of multiple AI models to enhance the generated content\\'s quality.\\n\\nHANDLING AI Mope1 Bias\\n\\nAI models can sometimes exhibit biases present in their training data. To mitigate these biases, you can employ techniques such as bias-aware prompt engineering or leverage external tools and resources to identify and correct biased outputs.\\n\\nSTRIKING THE RIGHT BALANCE OF GUIDANCE\\n\\nFinding the right balance between providing too much or too little guidance in prompts can be challenging. To strike the right balance, experiment with varying levels of specificity, context, and constraint in your prompts, and iteratively refine them based on the Al-generated content\\'s performance and your objectives. ENSURING CONTENT QUALITY AND RELEVANCE\\n\\nEnsuring that the Al-generated content meets your quality and relevance criteria can be a daunting task. Establish clear benchmarks and performance criteria, and use a combination of\\n\\n33\\n\\nquantitative metrics and qualitative assessments to evaluate the content. Refine your prompts and AI model accordingly to improve the content\\'s quality and relevance.\\n\\nOVERCOMING CREATIVE BLOCKS\\n\\nAl-generated content may sometimes lack creativity or fall short of your expectations. In such cases, consider experimenting with more open-ended prompts, leveraging multi-modal AI models, or combining the outputs of multiple AI models to generate more creative and engaging content.\\n\\nADDRESSING ETHICAL CONCERNS\\n\\nPrompt engineering raises ethical questions and challenges, such as fairness, inclusivity, and responsibility. To address these concerns, ensure that your prompts and AI models adhere to ethical guidelines and responsible AI practices, and strive to create content that is fair, inclusive, and respectful.\\n\\nBy understanding and addressing these common challenges in prompt engineering, you can create more effective, engaging, and high-quality Al-generated content. As you continue to refine your skills and overcome these challenges, you\\'ll be better equipped to harness the full potential of prompt engineering and drive innovation in the rapidly evolving field of generative Al.\\n\\n6.4. Measuring the Success of Your Prompt Engineering Efforts\\n\\nTo ensure that your prompt engineering efforts are successful, it\\'s crucial to establish clear metrics and benchmarks for evaluating the performance of your Al-generated content. In this section, we\\'ll discuss some key performance indicators (KPIs) and best practices for measuring the success of your prompt engineering efforts.\\n\\nConTENT QuaLity MEtrRIcs\\n\\nAssess the quality of the Al-generated content by considering factors such as coherence, consistency, grammatical correctness, and readability. Utilize tools like readability scores, grammar checkers, or human evaluation to ensure that your content meets your quality standards.\\n\\n34 ConTENT RELEVANCE METRICS\\n\\nEvaluate the relevance of your Al-generated content by determining how well it aligns with your objectives, target audience, and intended purpose. Use a combination of qualitative assessments, user feedback, and content engagement metrics (e.g., click-through rates, conversions, or time spent) to gauge relevance.\\n\\nContTENT NoVELTY AND CREATIVITY METRICS\\n\\nMeasure the novelty and creativity of your Al-generated content by considering factors such as uniqueness, originality, and unexpectedness. You can use tools like plagiarism checkers, similarity scores, or human evaluation to assess the novelty and creativity of your content.\\n\\nContent Accuracy Metrics\\n\\nEvaluate the accuracy of the Al-generated content by comparing it to reliable sources, ground truth, or expert knowledge. Use a combination of quantitative metrics (e.g., fact-checking scores) and qualitative assessments to ensure your content is accurate and trustworthy.\\n\\nUser SATISFACTION METRICS\\n\\nAssess user Satisfaction with your Al-generated content by collecting feedback through surveys, ratings, or comments. Analyze this feedback to identify areas for improvement and refine your prompts and AI model accordingly.\\n\\nEFFICIENCY METRICS\\n\\nEvaluate the efficiency of your prompt engineering efforts by considering factors such as the time taken to generate content, the number of iterations required, and the cost of AI model usage. Use these metrics to optimize your prompt engineering workflow and ensure that you\\'re generating high-quality content in a cost-effective and timely manner.\\n\\nADAPTABILITY METRICS\\n\\nAssess the adaptability of your Al-generated content by considering how well it can be repurposed or reused across different channels, formats, or contexts. Track the performance of your content across various applications to gauge its versatility and adaptability.\\n\\nBy establishing clear KPIs and benchmarks for your prompt engineering efforts, you can effectively measure the success of your Al-generated content and ensure that it meets your quality, relevance, and performance criteria. This data-driven approach not only helps you\\n\\n35\\n\\noptimize your prompts and AI model but also enables you to continuously improve your prompt engineering skills and drive innovation in the rapidly evolving field of generative AI.\\n\\n36 Conclusion\\n\\nIn this short ebook, we have explored the fascinating world of generative AI and prompt engineering, delving into key concepts, best practices, and potential future trends in this rapidly evolving field. As AI technologies continue to advance and reshape various aspects of our lives, the role of prompt engineering in enhancing the quality, relevance, and creativity of Al-generated content will grow increasingly significant.\\n\\nRecap of key concepts and best practices:\\n\\ne We have covered the fundamentals of generative AI and the importance of prompt engineering in guiding AI models to produce high-quality content\\n\\ne We have discussed different types of prompts, techniques for prompt engineering, and practical tips to create an effective prompt engineering workflow\\n\\ne We have examined common challenges and ethical considerations, as well as various metrics for measuring the success of prompt engineering efforts\\n\\nFuture trends in generative AI and prompt engineering:\\n\\ne As generative AI models become more sophisticated and capable, prompt engineering will continue to evolve, unlocking new possibilities and applications across diverse sectors\\n\\ne From enhancing business communications and personalizing customer experiences to fostering creativity and addressing global challenges, prompt engineering will play a critical role in leveraging Al- generated content to drive progress and innovation\\n\\nIn conclusion, we encourage you, the reader, to experiment with and innovate in the realm of prompt engineering. As you delve deeper into this field, harness your creativity, collaborate with others, and push the boundaries of what\\'s possible. By doing so, you will not only contribute to the advancement of generative AI and prompt engineering but also create a more prosperous, equitable, and sustainable future for all.\\n\\nArmed with the knowledge and insights shared in this ebook, we hope you are inspired to explore the incredible potential of prompt engineering and unlock new opportunities for innovation and growth in the ever-expanding world of generative AI.\\n\\n37 Appendix A: Recommended Books, Articles, and Blogs\\n\\nTo further expand your knowledge and understanding of generative AI and prompt engineering, we have compiled a list of recommended books, articles, and courses that can provide additional insights, practical examples, and guidance.\\n\\nBooks:\\n\\n\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\n\\n\"Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play\" by David Foster\\n\\n\"Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow\" by Aurélien Géron\\n\\n\"Pattern Recognition and Machine Learning\" by Christopher M. Bishop\\n\\n\"Human Compatible: Artificial Intelligence and the Problem of Control\" by Stuart Russell\\n\\nArticles:\\n\\n\"The Illustrated GPT-2 (Visualizing Transformer Language Models)\" by Jay Alammar\\n\\n\"The Annotated GPT-3\" by Jonathan Hui\\n\\n\"Better Language Models and Their Implications\" by OpenAI\\n\\n\"Language Models are Few-Shot Learners\" by OpenAI\\n\\n\"Building AI Applications with OpenAI\\'s GPT-3: A Practical Guide\" by Harrison Kinsley\\n\\nOnline Courses:\\n\\nCoursera: \\'\\'Deep Learning Specialization\" by Andrew Ng\\n\\nCoursera: \"Natural Language Processing Specialization by National Research University Higher School of Economics\\n\\nfast.ai: \"Practical Deep Learning for Coders\"\\n\\nedX: \\'\"\\'CS224N: Natural Language Processing with Deep Learning\" by Stanford\\n\\nMIT OpenCourseWare: \\'\\'6.S191: Introduction to Deep Learning\" by MIT\\n\\nOnline Resources and Blogs:\\n\\nOpenAl Blog (https://openai.com/blog/)\\n\\nGoogle AI Blog (https://ai.googleblog.com/)\\n\\nDistill (https://distill.pub/)\\n\\nMachine Learning Mastery (https: //machinelearningmastery.com/)\\n\\n38\\n\\nThese resources will help you deepen your understanding of generative AI and prompt engineering, stay updated on the latest advancements and breakthroughs in the field, and enhance your skills and expertise. By continually learning, experimenting, and collaborating, you will be well-equipped to thrive in the exciting and rapidly evolving world of generative Al.\\n\\n39\\n\\nAppendix B: Online Communities and Forums for Discussions and Collaboration\\n\\nEngaging with online communities and forums is an excellent way to learn, share ideas, and collaborate with others who are interested in generative AI and prompt engineering. Here is a list of popular online platforms where you can discuss, ask questions, and connect with like-minded individuals and experts in the field. AI Stack Exchange (https://ai.stackexchange.com/)\\n\\nA question and answer platform for people interested in AI concepts, research, and applications. Ask questions, provide answers, or browse through existing discussions to enhance your knowledge.\\n\\nMachine Learning Subreddit (https: //www.reddit.com/r/MachineLearning/)\\n\\nA popular subreddit for discussing topics related to machine learning, deep learning, and generative AI. Share your projects, ask questions, and engage with a diverse community of learners, researchers, and professionals.\\n\\nAI section on arXiv (https://arxiv.org/list/cs.Al/recent)\\n\\nA repository of preprints for AI research papers. Browse the latest research, provide feedback, and engage in discussions with other researchers and practitioners.\\n\\nDeep Learning Subreddit (https://www.reddit.com/r/deeplearning/)\\n\\nA subreddit dedicated to deep learning and its applications, including generative AI. Share resources, ask questions, and collaborate with a vibrant community of enthusiasts and professionals.\\n\\nOpenAI Community (https://community.openai.com/)\\n\\nAn official forum for OpenAI, where users can discuss various topics related to AI research and applications, including GPT models and prompt engineering. Connect with other AI enthusiasts, share your projects, and learn from experts.\\n\\nData Science Stack Exchange (https://datascience.stackexchange.com/)\\n\\nA question and answer platform for data science professionals, researchers, and enthusiasts. Ask questions, provide answers, or browse through existing discussions to deepen your understanding of data science, machine learning, and Al.\\n\\n40\\n\\nBy participating in these online communities and forums, you can expand your network, share your knowledge, and collaborate with others who share your passion for generative AI and prompt engineering. These platforms provide invaluable opportunities to learn from experts, stay updated on the latest advancements, and contribute to the growth and innovation of the field.\\n\\n41 ears\\n\\nTable of Contents\\n\\nPrefaCe oo... eeccescesceceseeceseeseeeseeeesecseecseacsecseacsesseaesecaesassecaeeesaeeaesesaesaesesaesaeseacsesseeaeaeeasasaeeaseaesesaseesaeeataes 3 GIOSSALY .....ecescescescesccssesccsseesecesecseesececesecsecsecssesessesessecsecssseeesaeceesessaecseseesesecaecesseseusesaeseasesesaseesecaesaesersones 5 INtrOCUCTION tO LLMS......cceeeeeeseeeeseeeeetseeeeseescseesesaesecseeecseeassecsesacseaesaesesaesaeseeacsecseaeaecaseassesasesaeeesetaeeetaes 8 What Are Large Language Models (LLMS)? ..........c:csccssssscsssesscescsseesecseeseecsecseseesseseeseseaecsesesesaeeeeseasenss 8 Foundation Language Models vs. Fine-Tuned Language Models ...........:csccssessscsseseceseeseeseeeesesseeeeees 11 Evolution of Large Language Models ...........:ccsscsccessssessscesceseesecsseesesseceseseesesaeceeecsecaeseeeseseeseeseaeeneesees 11 Neural Networks ........:ccsscssseeseseeseeceseeseeeseeeesesecsecsessesessesecsecacseeseeaesecaeeecsesassesaesassesaeeesaeeaseeaeeeeaeeas 12 TLAMSFOFMELS oo... eeeeceeeceseeseseeseeceseescseeseescuecsseceseeaesecsecaesecsesaesecsesaesecsesecseeseeecaeseseseaeassesaetessetaeeetaees 14 How Enterprises Can Benefit From Using Large Language MOodels............:ccsccsseesseseesecssesseeeeeeeseseeseeeenee 20 Challenges of Large Language Models ..........:csccsscsssessesscesceseesecseeesececesecsecsecesesesseseseeesaesaeseesesesaeeees 21 Ways to Build LLMS.........cecessesccseesscsssesecsscesessecseceecaecsecsecesesseseesesecsesseseescseceesasseseseeseeseaseseseeseeseaeentees 21 How to Evaluate LLMS uc ecsceessesseseeseeecseeseecseeseseeseeeceeseesesaesaesecseecseeacsesaeeeesesaseeseseeesaeeasetaeeaseees 22 Notable Companies in the LLM Field...........cccscscccssssessscsecsecsscesecsecseceeseseesesaesceesaesseseeesseseeseeeeaeeneeaees 23 Popular Startup-developed LLM AppS.........cccsccsccsssssessscseceeceecesessesseceecesecsecsesseecsecseseesssessesesesseneeeees 23\\n\\nA Beginner’s Guide to Large Language Models\\n\\n2\\n\\nPreface\\n\\nLanguage has been integral to human society for thousands of years. a long-prevailing theory, laryngeal descent theory or LDT, suggests that speech and, thus, language, may have evolved about 200,000 or 300,000 years ago, while newer research shows it could’ve happened even sooner.\\n\\nRegardless of when it first appeared, language remains the cornerstone of human communication. It has taken on an even greater role in today’s digital age, where an unprecedented portion of the population can communicate via both text and speech across the globe.\\n\\nThis is underscored by the fact that 347.3 billion email messages are sent and received worldwide every day, and that five billion people — or over 63% of the entire world population — send and receive text messages.\\n\\nLanguage has therefore become a vast trove of information that can help enterprises extract valuable insights, identify trends, and make informed decisions. As an example, enterprises can analyze texts like customer reviews to identify their products’ best-selling features and fine-tune their future product development.\\n\\nSimilarly, language production — as opposed to language analysis — is also becoming an increasingly important tool for enterprises. Creating blog posts, for example, can help enterprises raise brand awareness to a previously unheard-of extent, while composing emails can help them attract new stakeholders or partners at an unmatched speed.\\n\\nHowever, both language analysis and production are time-consuming processes that can distract employees and decision-makers from more important tasks. For instance, leaders often need to sift through vast amounts of text in order to make informed decisions instead of making them based on extracted key information.\\n\\nEnterprises can minimize these and other problems, such as the risk of human error, by employing large language models (LLMs) for language-related tasks. LLMs can help enterprises accelerate and largely automate their efforts related to both language production and analysis, saving valuable time and resources while improving accuracy and efficiency.\\n\\nUnlike previous solutions, such as rule-based systems, LLMss are incredibly versatile and can be easily adapted to a wide range of language-related tasks, like generating content or summarizing legal documentation.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n3\\n\\nThe goal of this book is to help enterprises understand what makes LLMs so groundbreaking compared to previous solutions and how they can benefit from adopting or developing them. It also aims to help enterprises get a head start by outlining the most crucial steps to LLM development, training, and deployment.\\n\\nTo achieve these goals, the book is divided into three parts:\\n\\n> Part 1 defines LLMs and outlines the technological and methodological advancements over the years that made them possible. It also tackles more practical topics, such as how enterprises can develop their own LLMs and the most notable companies in the LLM field. This should help enterprises understand how adopting LLMs can unlock cutting-edge possibilities and revolutionize their operations.\\n\\nPart 2 discusses five major use cases of LLMs within enterprises, including content generation, summarization, and chatbot support. Each use case is exemplified with real-life apps and case studies, so as to show how LLMs can solve real problems and help enterprises achieve specific objectives.\\n\\nPart 3 is a practical guide for enterprises that want to build, train, and deploy their own LLMs. provides an overview of necessary pre-requirements and possible trade-offs with different development and deployment methods. ML engineers and data scientists can use this as a reference throughout their LLM development processes.\\n\\nHopefully, this will inspire enterprises that have not yet adopted or developed their own LLMs to do so soon in order to gain a competitive advantage and offer new SOTA services or products. The most benefits will be, as usual, reserved for early adopters or truly visionary innovators.\\n\\nA Beginner’s Guide to Large Language Models It\\n\\nGlossary\\n\\nTerms Deep learning systems Generative Al Large language models (LLMs) Natural language processing (NLP) Long short-term memory neural network (LSTM) Natural language generation (NLG) Natural language understanding (NLU) Neural network (NN) Perception Al Recurrent neural network (RNN) Description Systems that rely on neural networks with many hidden layers to learn complex patterns. Al programs that can generate new content, like text, images, and audio, rather than just analyze it. Language models that recognize, summarize, translate, predict, and generate text and other content. They’re called large because they are trained on large amounts of data and have many parameters, with popular LLMs reaching hundreds of billions of parameters. The ability of a computer program to understand and generate text in natural language. A special type of RNNs with more complex cell blocks that allow it to retain more past inputs. A part of NLP that refers to the ability of a computer program to generate human-like text. A part of NLP that refers to the ability of a computer program to understand human-like text. A machine learning algorithm in which the parameters are organized into consecutive layers. The learning process of NNs is inspired by the human brain. Much like humans, NNs “learn” important features via representation learning and require less human involvement than most other approaches to machine learning. Al programs that can process and analyze but not generate data, mainly developed before 2020. Neural network that processes data sequentially and can memorize\\n\\npast inputs.\\n\\nA Beginner’s Guide to Large Language Models\\n\\nTerms\\n\\nDescription\\n\\nRule-based system Traditional machine learning Transformer Structured data Unstructured data Fine-tuning Customization Parameter-efficient techniques (PEFT) Prompt learning Adapter tuning A system that relies on human-crafted rules to process data. Traditional machine learning uses a statistical approach, drawing probability distributions of words or other tokens based on a large annotated corpus. It relies less on rules and more on data. A type of neural network architecture designed to process sequential data non-sequentially. Data that is quantitative in nature, such as phone numbers, and can be easily standardized and adjusted to a pre-defined format that ML algorithms can quickly process. Data that is qualitative in nature, such as customer reviews, and difficult to standardize. Such data is stored in its native formats, like PDF files, before use. A transfer learning method used to improve model performance on selected downstream tasks or datasets. It’s used when the target task is similar to the pre-training task and involves copying the weights of a PLM and tuning them on desired tasks or data. A method of improving model performance by modifying only one or a few selected parameters of a PLM instead of updating the entire model. It involves using parameter-efficient techniques (PEFT). Techniques like prompt learning, LoRa, and adapter tuning which allow researchers to customize PLMs for downstream tasks or datasets whil preserving and leveraging existing knowledge of PLMs. These techniques are used during model customization and allow for quicker training and often more accurate predictions. An umbrella term for two PEFT techniques, prompt tuning and p-tuning, which help customize models by inserting virtual token embeddings among discrete or real token embeddings. A PEFT technique that involves adding lightweight feed-forward layers, called adapters, between existing PLM layers and updating only their weights during customization while keeping the original PLM weights frozen.\\n\\nOpen-domain question answering\\n\\nAnswering questions from a variety of different domains, like legal, medical, and financial, instead of just one domain.\\n\\nExtractive question answering\\n\\nAnswering questions by extracting the answers from existing texts or databases.\\n\\nA Beginner’s Guide to Large Language Models\\n\\nTerms Throughput\\n\\nLatency\\n\\nData Readiness\\n\\nA Beginner’s Guide to Large Language Models\\n\\nDescription\\n\\nA measure of model efficiency and speed. It refers to the amount of data or the number of predictions that a model can process or generate within a pre-defined timeframe.\\n\\nThe amount of time a model needs to process input and generate output.\\n\\nThe suitability of data for use in training, based on factors such as data quantity, structure, and quality.\\n\\nIntroduction to LLMs\\n\\nA large language model is a type of artificial intelligence (Al) system that is capable of generating human-like text based on the patterns and relationships it learns from vast amounts of data. Large language models use a machine learning technique called deep learning to analyze and process large sets of data, such as books, articles, and web pages.\\n\\nLarge language models unlocked numerous unprecedented possibilities in the field of NLP and Al. This was most notably demonstrated by the release of OpenAl’s GPT-3 in 2020, the then-largest language model ever developed.\\n\\nThese models are designed to understand the context and meaning of text and can generate text that is grammatically correct and semantically relevant. They can be trained on a wide range of tasks, including language translation, summarization, question answering, and text completion.\\n\\nGPT-3 made it evident that large-scale models can accurately perform a wide — and previously unheard-of — range of NLP tasks, from text summarization to text generation. It also showed that LLMs could generate outputs that are nearly indistinguishable from human-created text, all while learning on their own with minimal human intervention.\\n\\nThis presented an enormous improvement from earlier, mainly rule-based models that could neither learn on their own nor successfully solve tasks they weren’t trained on. It is no surprise, then, that many other enterprises and startups soon started developing their own LLMss or adopting existing LLMs in order to accelerate their operations, reduce expenses, and streamline workflows.\\n\\nPart 1 is intended to provide a solid introduction and foundation for any enterprise that is considering building or adopting its own LLM. What Are Large Language Models (LLMs)?\\n\\nLarge language models (LLMs) are deep learning algorithms that can recognize, extract, summarize, predict, and generate text based on knowledge gained during training on very large datasets.\\n\\nThey’re also a subset of a more general technology called language models. All language models have one thing in common: they can process and generate text that sounds like natural language. This is known as performing tasks related to natural language processing (NLP).\\n\\nA Beginner’s Guide to Large Language Models\\n\\n8\\n\\nAlthough all language models can perform NLP tasks, they differ in other characteristics, such as their size. Unlike other models, LLMs are considered /arge in size because of two reasons:\\n\\n1. They’re trained using large amounts of data.\\n\\n2. They comprise a huge number of learnable parameters (i.e., representations of the underlying structure of training data that help models perform tasks on new or never-before-seen data).\\n\\nTable 1 showcases two large language models, MT-NLG and GPT-3 Davinci, to help clarify what’s considered large by contemporary standards.\\n\\nTable 1. Comparison of MT-NLG and GPT-3\\n\\nLarge Language Model Number of Number of tokens in parameters the training data NVIDIA Model: Megatron-Turing Natural Language 530 billion 270 billion Generation Model (MT-NLG OpenAl Model: GPT-3 Davinci Model 175 billion 499 billion\\n\\nSince the quality of a model heavily depends on the model size and the size of training data, larger language models typically generate more accurate and sophisticated responses than their smaller counterparts.\\n\\nA Beginner’s Guide to Large Language Models\\n\\nFigure 1. Answer Generated by GPT-3.\\n\\nPlayground\\n\\nChat\\n\\nHuman: Why are large language models useful?\\n\\nAI: Large language models are useful because they enable machines to better understand and generate more accurate natural language text. They also help with tasks such as question answering, machine translation, and summarization. Additionally, they can be used to generate new, creative text, which can be useful for applications like music generation and story telling. Human:\\n\\n9\\n\\n£¢\\n\\n©)\\n\\n87\\n\\n6\\n\\nHowever, the performance of large language models doesn’t just depend on the model size or data quantity. Quality of the data matters, too.\\n\\nFor example, LLMs trained on peer-reviewed research papers or published novels will usually perform better than LLMs trained on social media posts, blog comments, or other unreviewed content. Low- quality data like user-generated content may lead to all sorts of problems, such as models picking up slang, learning incorrect spellings of words, and so on.\\n\\nIn addition, models need very diverse data in order to perform various NLP tasks. However, if the model is intended to be especially good at solving a particular set of tasks, then fine-tune it using a more relevant and narrower dataset. By doing so a foundation language model is transformed — from one that’s good at performing various NLP tasks across a broad set of domains — into a fine-tuned model that specializes in performing tasks in a narrowly scoped domain.\\n\\nA Beginner’s Guide to Large Language Models\\n\\nSave\\n\\ng\\n\\n72\\n\\n10\\n\\nFoundation Language Models vs. Fine-Tuned Language Models\\n\\nFoundation language models, such as the aforementioned MT-NLG and GPT-3, are what is usually referred to when discussing LLMs. They’re trained on vast amounts of data and can perform a wide variety of NLP tasks, from answering questions and generating book summaries to completing and translating sentences.\\n\\nThanks to their size, foundation models can perform well even when they have little domain-specific data at their disposal. They have good general performance across tasks but may not excel at performing any one specific task.\\n\\nFine-tuned language models, on the other hand, are large language models derived from foundation LLMs. They’re customized for specific use cases or domains and, thus, become better at performing more specialized tasks.\\n\\nApart from the fact that fine-tuned models can perform specific tasks better than foundation models, their biggest strength is that they are lighter and, generally, easier to train. But how does one actually fine-tune a foundation model for specific objectives?\\n\\nCurrently, the most popular method is customizing a model using parameter-efficient customization techniques, such as p-tuning, prompt tuning, adapters, and so on. Customization is far less time- consuming and expensive than fine-tuning the entire model, although it may lead to somewhat poorer performance than other methods. Customization methods are further discussed in Part 3. Evolution of Large Language Models\\n\\nAl systems were historically about processing and analyzing data, not generating it. They were more oriented toward perceiving and understanding the world around us rather than on generating new information. This distinction marks the main difference between Perceptive and Generative Al, with the latter becoming increasingly prevalent since around 2020, or after companies started adopting transformer models and developing increasingly more robust LLMs at a large scale.\\n\\nThe advent of large language models further fueled a revolutionary paradigm shift in the way NLP models are designed, trained, and used. To truly understand this, it may be helpful to compare large language models to previous NLP models and how they worked. For this purpose, let’s briefly explore three regimes in the history of NLP: pre-transformers NLP, transformers NLP, and LLM NLP.\\n\\n1. Pre-transformers NLP was mainly marked by models that relied on human-crafted rules rather than machine learning algorithms to perform NLP tasks. This made them suitable for simpler tasks that didn’t require too many rules, like text classification, but unsuitable for more complex tasks, such as machine translation. Rule-based models also performed poorly in edge-case scenarios because they couldn’t make accurate predictions or classifications for never-before-seen data for which no clear rules were set. This problem was somewhat solved with simple neural networks, such as RNNs and LSTMs, developed during the later phases of this period. RNNs and LSTMs could memorize past data to a certain extent and, thus, provide context-dependent predictions and\\n\\nA Beginner’s Guide to Large Language Models\\n\\n11\\n\\nclassifications. However, RNNs and LSTMs could not make predictions over long spans of text, limiting their effectiveness.\\n\\n2. Transformers NLP was set in motion by the rise of the transformer architecture in 2017. Transformers could generalize better than the then-prevailing RNNs and LSTMs, capture more context, and process more data at once. These improvements enabled NLP models to understand longer sequences of data and perform a much wider range of tasks. However, from today’s point of view, models developed during this period had limited capabilities, mainly due to the general lack of large-scale datasets and adequate computational resources. They also mainly sparked attention among researchers and experts in the field but not the general public, as they weren’t user-friendly nor accurate enough to become commercialized.\\n\\n3. LLM NLP was mainly initiated by the launch of OpenAl’s GPT-3 in 2020. Large language models like GPT-3 were trained on massive amounts of data, which allowed them to produce more accurate and comprehensive NLP responses compared to previous models. This unlocked many new possibilities and brought us closer to achieving what many consider “true” Al. Also, LLMs made NLP models much more accessible to non-technical users who could now solve a variety of NLP tasks just by using natural-language prompts. NLP technology was finally democratized.\\n\\nThe switch from one methodology to another was largely driven by relevant technological and methodological advancements, such as the advent of neural networks, attention mechanisms, and transformers and developments in the field of unsupervised and self-supervised learning. The following sections will briefly explain these concepts, as understanding them is crucial for truly understanding how LLMs work and how to build new LLMs from scratch. Neural Networks\\n\\nNeural networks (NNs) are machine learning algorithms loosely modeled after the human brain. Like the biological human brain, artificial neural networks consist of neurons, also called nodes, that are responsible for all model functions, from processing input to generating output.\\n\\nThe neurons are further organized into layers, vertically stacked components of NNs that perform specific tasks related to input and output sequences.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n12\\n\\nEvery neural network has at least three layers:\\n\\n> The input layer accepts data and passes it to the rest of the network.\\n\\n> The hidden layer, or multiple hidden layers, performs specific functions that make the final output of an NN possible. These functions can include identifying or classifying data, generating new data, and other functions depending on the specific NLP task in question.\\n\\n> The output layer generates a prediction or classification based on the input.\\n\\nWhen LLMs were first developed, they were based on simpler NN architectures with fewer layers, mainly recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). Unlike other neural networks, RNNs and LSTMs could take into account the context, position, and relationships between words even if they were far apart in a data sequence. Simply put, this meant they could memorize and consider past data when generating output, which resulted in more accurate solutions to many NLP tasks, especially sentiment analysis and text classification.\\n\\nThe biggest advantage that neural networks like RNNs and LSTMs had over traditional, rule-based systems was that they were capable of learning on their own with little to no human involvement. They analyze data to create their own rules, rather than learn the rules first and apply them to data later. This is also known as representation learning and is inspired by human learning processes.\\n\\nRepresentations, or features, are hidden patterns that neural networks can extract from data. To exemplify this, let’s imagine we’re training an NN-based model on a dataset containing the following tokens:\\n\\n“ cat,” “cats,” dog,” “dogs”\\n\\nAfter analyzing these tokens, the model may identify a representation that one could formulate as:\\n\\nPlural nouns have the suffix “-s.”\\n\\nThe model will then extract this representation and apply it to new or edge-case scenarios whose data distribution follows that of training data. For example, the assumption can be made that the model will correctly classify tokens like “chairs” or “table” as plural or singular even if it had not encountered them before. Once it encounters irregular nouns that don’t follow the extracted representation, the model will update its parameters to reflect new representations, such as:\\n\\nPlural nouns are followed by plural verbs.\\n\\nThis approach enables NN-based models to generalize better than rule-based systems and successfully perform a wider range of tasks.\\n\\nHowever, their ability to extract representations is very much dependent on the number of neurons and layers comprising a network. The more neurons neural networks have, the more complex representations they can extract. That’s why, today, most large language models use deep learning neural networks with multiple hidden layers and, thus, a higher number of neurons.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n13\\n\\nFigure 2 shows a side-by-side comparison of a single-layer neural network and a deep learning neural network.\\n\\nFigure 2. Comparison of Single-Layer vs. Deep Learning Neural Network Neural Network\\n\\nDeep Learning Neural Network\\n\\nInputs Inputs NX Output XY Output ere ©. eee e- Output Output — — Layer ‘© R Layer Input Hidden Input Layer Layer Layer Hidden Layers\\n\\nWhile this may seem like an obvious choice today, consider that developing deep neural networks did not make sense before the hardware evolved to be able to handle massive workloads. This only became possible after ~1999 when NVIDIA introduced “the world’s first GPU,” or graphics processing unit, to the wider market or, more precisely, after a wildly successful CNN called AlexNet popularized their use in deep learning in 2012.\\n\\nGPUs had a highly parallelizable architecture which enabled the rapid advances in deep learning systems that are seen today. Among other advancements, the advent of GPUs ushered in the development of a new type of neural network that would revolutionize the field of NLP: transformers.\\n\\nTransformers\\n\\nWhile RNNs and LSTMs have their advantages, especially compared to traditional models, they also have some limitations that make them unsuitable for more complex NLP tasks, such as machine translation. Their main limitation is the inability to process longer data sequences and, thus, consider the overall context of the input sequence. Because LSTMs and RNNs cannot handle too much context well, their outputs are prone to being inaccurate or nonsensical. This and other challenges have been largely overcome with the advent of new, special neural networks called transformers.\\n\\nTransformers were first introduced in 2017 by Vaswani et al. in a paper titled \"Attention is All You Need.\" The title alluded to attention mechanisms, which would become the key component of transformers.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n14\\n\\n“We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.” - Vaswani et. al, “Attention is All You Need”\\n\\nThe paper proposed that attention mechanisms would render recurrence and convolutions obsolete in regard to sequential data and make transformers more suitable for machine translation than RNNs and CNNs. This prophecy would soon come true, as transformers would go on to become the dominant architecture not only for machine translation but for all NLP tasks.\\n\\nAttention mechanisms solved the problem of inadequate context handling by allowing models to selectively pay attention to certain parts of the input while processing it. Instead of having to capture the entire context at once, models could now focus on the most important tokens regarding a specific task.\\n\\nTo demonstrate this, let’s imagine the desired model is a transformer-based model to predict the next words for the following input sentence:\\n\\nMary had a little lamb.\\n\\nAttention mechanisms -— or, rather, self-attention layers that are based on attention mechanisms — would first calculate attention weights for each word in our input. Attention weights represent the importance of each token, so the more weight a token is assigned, the more important it\\'s deemed. For example, the attention mechanism might give more weight to the word \"amb\" than the word \"a,\" as it’s likely to have more influence on the final output.\\n\\nThe model would then use these weights to dynamically emphasize or downplay each word as it generates output. If one assumes that the most weight was assigned to the word “lamb,” the model may produce a continuation such as:\\n\\n\"whose fleece was white as snow\"\\n\\nTo determine how important each token is, self-attention layers examine its relationships with other tokens in a sequence:\\n\\n1. Ifatoken has many relevant relationships with other tokens with respect to the task being performed, then that token is deemed as important and, potentially, more important than other tokens in the same sequence.\\n\\n2. Ifa token doesn’t have many relationships with other tokens, or if they are irrelevant to a specific task, that token is considered less important or completely unimportant. This means the model will virtually ignore it when generating the output.\\n\\nSo, by enabling models to handle context more effectively, attention mechanisms allowed them to generate more accurate outputs than models based on RNNs and LSTMs. Simultaneously, this new approach to data processing also allowed transformer-based models to generate outputs more quickly than RNN- and LSTM-based models.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n15\\n\\nLSTMs and RNNs need more time to generate output because they process input sequentially. To clarify what this means, let’s explore how LSTMs would approach processing our original input sentence: Mary had a little lamb.\\n\\nSince LSTMs process data sequentially, they would need to process one word in our sequence at a time: Mary, had, a, little, lamb. This significantly slows down inference, especially with longer data sequences. For example, just imagine how long it would take LSTMs and RNNs to process a single Wikipedia page. Too long.\\n\\nTransformers, on the other hand, process data in parallel, which means they “read” all input tokens at once instead of processing one at a time. It also means they are able to perform NLP tasks faster than LSTMs and RNNs.\\n\\nHowever, despite being slow, sequential data processing has one big advantage. By processing one word at a time, LSTMs and RNNs are always able to tell which word came first, second, and so on. They know the word order of the input sequence because they use that same order to process it. Conversely, transformers are not initially “aware” of the original word order because they process data non-sequentially. While this may seem like only a minor problem at first, analyzing the sentences below may illustrate otherwise:\\n\\n1. Mary had alittle lamb.\\n\\n2. Alittle lamb had Mary.\\n\\n3. Hada little lamb Mary.\\n\\nSentence (2) shows how a slight change in word order can distort the intended meaning, while sentence (3) exemplifies an even bigger issue — how changes in word order can result in completely nonsensical and grammatically incorrect variations.\\n\\nTo overcome this challenge, transformers use positional encodings that help them retain position information. Positional encodings are additional inputs, or vectors, associated with each token. They can be fixed or trainable, depending on whether the desire is for the model to refine them during training or not.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n16\\n\\nFigure 3. Sequential Data Processing in a LSTM\\n\\nSource: Attention Is All You Need\\n\\nOutput\\n\\nProbabilities Nx Positional Encoding & A) Positional ee Encoding Inputs Outputs (shifted right)\\n\\nResearchers and companies would soon start implementing these new mechanisms and building new transformer-based models, with Google releasing its famous BERT in 2018. BERT\\n\\nGoogle’s BERT (Bidirectional Encoder Representations from Transformers) is one of the first transformer-based language models. It’s a masked language model (MLM), which means it’s trained on sentences containing masked tokens. The model needs to predict the masked token by considering its surrounding context. To illustrate this, let’s imagine that a model is given the following input sentence:\\n\\n\"| [have] a mask.\"\\n\\nBERT’s task is to predict the masked word “have.” It does so by analyzing the tokens on both of its sides, namely \"I,\", \"a,\" and “mask.” This is what makes it bidirectional, as well as more accurate than previous language models that could only consider the context on the left of the masked token. In this case, unidirectional models would only consider the word “I” when predicting the masked word, which provides little context. The chances of a unidirectional model generating the right predictions are smaller.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n17\\n\\nBERT was the first model to show how bidirectionality can model performance NLP tasks. It has been used for various purposes, including improving the accuracy of Google\\'s search results by allowing for a better understanding of the context and meaning of queries.\\n\\nOther Large Language Models\\n\\nGoogle’s BERT and new advancements in the field inspired other companies to start building their own large language models. In the table below, we’ve listed some models developed prior to the truly groundbreaking GPT-3, released in June 2020.\\n\\nTable 2 shows a timeline of consequential LLM releases.\\n\\nTable 2. Large Language Model Release Timeline\\n\\nModel Company Year Brief Description GPT-2 OpenAl 2019 A transformer-based language model designed to generate human-like text and perform various NLP tasks, including language translation, summarization, and question answering. RoBERTa Facebook 2019 A BERT-based model designed to improve the performance of NLP tasks by training the model on a larger dataset and using a more efficient training method. DeBERTa Microsoft 2020 A BERT-based model designed to improve the performance of NLP tasks by decoupling the encoder and decoder components of the model. GPT-3 OpenAl 2020 An upgraded model of GPT-2 trained on a more massive dataset and capable of generating higher-quality outputs.\\n\\nBy proving that LLMs can be used for few-shot learning and excel without “large-scale task-specific data collection or model parameter updating,” GPT-3 would inspire companies to build even larger models, like Megatron-Turing Natural Language Generation with 530 billion parameters, PaLM with 540 billion, and WuDao 2.0 with impressive 1.75 trillion parameters. Unsupervised and Self-Supervised Learning\\n\\nBERT wasn’t revolutionary just because it was a bidirectional model, but also because it was trained using unsupervised learning. Unsupervised learning refers to machine learning algorithms finding patterns in unlabeled datasets with no human intervention. In BERT’s case, the model had to extract patterns from plain-language Wikipedia pages on its own during training. This is often considered to be Al in its purest form.\\n\\nUnsupervised learning models use feedback loops to learn and improve their performance. This involves getting feedback on whether a prediction or classification was right or wrong, which the model uses to guide its future decisions.\\n\\nFeedback loops are the reason why some differentiate between unsupervised and self-supervised learning. Self-supervised learning models don’t have feedback loops but rather use supervisory signals\\n\\nA Beginner’s Guide to Large Language Models\\n\\n18\\n\\nto get feedback during training. These signals are generated automatically from data without human annotation.\\n\\nBoth unsupervised and self-supervised learning techniques have one key advantage over supervised learning: they rely on the model to create labels and extract features on its own, rather than demanding human intervention. This helps companies train models without time-consuming data labeling processes or providing human feedback on the model’s outputs.\\n\\nSelf-supervised learning is currently the dominant approach to pre-training large language models and is often recommended to enterprises that want to build their own.\\n\\nBenefits of GPT Vs Bert\\n\\nGPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both highly advanced and widely used natural language processing (NLP) models. However, they differ in their architectures and use cases.\\n\\nGPT is a generative model that is trained to predict the next word in a sentence given the previous words. This pre-training enables GPT to generate coherent and fluent sentences from scratch, making it ideal for language generation tasks such as text completion, summarization, and question answering.\\n\\nIn distinction, BERT is a discriminative model that is trained to classify sentences or tokens into different categories such as sentiment analysis, named entity recognition, and text classification. It is a bidirectional model that considers both the left and right contexts of a sentence to understand the meaning of a word, making it highly effective for tasks such as sentiment analysis and question answering.\\n\\nIn terms of architecture, GPT uses a unidirectional transformer, whereas BERT uses a bidirectional transformer. This means that GPT can only consider the left context of a word when making predictions, while BERT considers both the left and right contexts.\\n\\nBoth GPT and BERT are powerful models that have revolutionized the field of NLP. Their choice depends on the specific task at hand, and researchers and practitioners often use a combination of both models to achieve optimal results.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n19 How Enterprises Can Benefit From Using Large Language Models\\n\\nneed to tackle\\n\\ntasks\\n\\nEnterprises language-related every day. This includes more obvious text tasks, such as writing emails or generating content, but also tasks like analyzing patient data for health risks or providing companionship to customers. All of these tasks can be automated using large language models.\\n\\nModels, or applications powered by large language models, can help enterprises speed up many complex tasks and often execute them with a higher level of precision than human agents. For example, tech enterprises can use them to write code faster, while banks can use them to minimize the risk of human error when analyzing documents for fraud indications.\\n\\nAutomating complex, but often tedious tasks further allow employees to focus on more important tasks instead and make progress faster. We’ll see, for example, how healthcare enterprises can use LLMs to generate synthetic clinical data and use it to speed up medical research in Part 2.\\n\\nLLMs can benefit enterprises in many other ways, depending on how they are used. Some use cases, like LLM-based sentiment analysis, provide them with deeper insights about their audience, while customer churn prediction allows them to encourage customers to stay with their company just as they were about to leave it. Additionally, enterprises can use LLMs to offer new, conversation-based services, such as specialized Al companions.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n20\\n\\nChallenges of Large Language Models\\n\\nEnterprises that want to start using large language models, or applications powered by large language models, should be aware of a few common LLM-related pitfalls. Below are some general ones that are applicable regardless of whether a model is being customized, fine-tuned, or built from scratch.\\n\\n1. Large language models are vulnerable to adversarial examples. Adversarial examples are inputs specifically crafted to fool the models into making a mistake. This can raise security concerns, particularly for enterprises in sensitive industries like healthcare or finance.\\n\\n2. Large language models can lack interpretability. Interpretability refers to the ability to interpret and predict models’ decisions. Models with low interpretability can be difficult to troubleshoot and evaluate, as it may not be clear how they are making their decisions or how accurate or unbiased those decisions are. This can be especially problematic in the context of high-stake use cases, such as fraud detection, and in industries that require a high level of transparency, such as healthcare and finance.\\n\\n3. Large language models may provide un-customized, generic answers. As such, LLMs may not always respond well to human input or understand the intent behind it. This can be improved with techniques such as Reinforcement Learning from Human Feedback (RLHF), which help models improve their performance over time based on positive or negative human feedback. Even so, LLMs can sometimes reproduce the text data they’ve seen during training. This is problematic only from an ethical angle but may also expose enterprises to unwanted copyright and legal issues.\\n\\n4. Using large language models can raise ethical concerns. It’s questionable whether enterprises should use LLMs for important decision-making tasks, such as deciding which candidate is the most qualified based on collected resumes, especially without human supervision. Additionally, it should be assessed whether it’s ethical to use LLMs for tasks that would normally be performed by human, mainly white-collar workers.\\n\\n5. Large language models can generate inappropriate and harmful content. On that note, enterprises should keep in mind that LLMs are often trained on large corpora of Internet texts, which may make them prone to generating toxic, biased, and otherwise inappropriate and harmful content.\\n\\nEnterprises that want to build proprietary LLMs from scratch also need to address additional challenges, like whether they have sufficient computing power, storage, and datasets, expertise, and financial resources to develop, implement, and maintain the models. Ways to Build LLMs\\n\\nBuilding large language models from scratch doesn’t always make sense, especially for enterprises whose core business is not related to Al or NLP technologies. Since the process can be extremely time-consuming and resource-exhaustive, most enterprises are more likely to opt for customizing existing models to their needs.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n21\\n\\nCustomizing existing base models — also called pre-trained models or PLMs — can be typically split into three essential steps:\\n\\n1. Finding a well-suited foundation model (PLM). This requires considering ideal model size, training tasks and datasets, LLM providers, and more.\\n\\n2. Fine-tuning the model. Base models can be fine-tuned on a specific corpus and for a specific use case. For example, text classification base models may be fine-tuned for sentiment analysis or trained using legal records to become proficient in legal terminology.\\n\\n3. Optimizing the model. Models can be further optimized using techniques such as Reinforcement Learning from Human Feedback (RLHF), where the model is updated based on positive or negative human feedback on its predictions or classifications. RLHF seems particularly promising, partially due to being used in widely popular ChatGPT.\\n\\nAlternatively, enterprises may choose to only customize base models using parameter-efficient techniques like adapters and p-tuning. Customization can yield especially accurate models when the base model is trained on tasks similar to the selected downstream tasks. For example, a base text classification model may be a good candidate for customization for sentiment analysis, as the two tasks are very similar. Thanks to being trained on text classification, the model can draw upon the knowledge it gained during training to perform sentiment analysis tasks more easily.\\n\\nHow to Evaluate LLMs\\n\\nLarge language models (LLMs) use deep learning techniques to analyze and generate natural language. These models have become increasingly popular due to their ability to perform a wide range of language-related tasks such as language translation, text summarization, and question- answering. However, evaluating the performance of LLMs is not a straightforward task, and it requires a careful analysis of different factors such as training data, model size, and speed of inference.\\n\\nThe most crucial element in evaluating LLMs is the quality and quantity of the training data used. The training data should be diverse and representative of the target language and domain to ensure that the LLM can learn and generalize language patterns effectively. Moreover, the training data should be annotated with relevant labels or tags to enable supervised learning, which is the most used approach in LLMs.\\n\\nAnother important factor is the size of the model. Generally, larger models have better performance, but they also require more computational resources to train and run. Therefore, researchers often use a trade-off between model size and performance, depending on the specific task and resources available. It is also worth noting that larger models tend to be more prone to overfitting, which can lead to poor generalization performance on new data.\\n\\nSpeed of inference must also be used in evaluation, especially when deploying LLMs in real-world applications. Faster inference time is desirable as it enables the LLM to process large amounts of data in a timely and efficient manner. Several techniques, such as pruning, quantization, and distillation, have been proposed to reduce the size and improve the speed of LLMs.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n22\\n\\nTo evaluate the performance of LLMs, researchers often use benchmarks, which are standardized datasets and evaluation metrics for a particular language-related task. Benchmarks enable fair comparisons between different models and methods and help identify the strengths and weaknesses of LLMs. Common benchmarks include GLUE (General Language Understanding Evaluation), SuperGLUE, and CoQA (Conversational Question Answering). Notable Companies in the LLM Field\\n\\nThe release of BERT in 2018 and, more notably, the release of GPT-3 in 2020, prompted both large tech companies and smaller startups to enter the race with their own LLMs and innovative approaches to model development. The most notable companies developing their own LLMs at the time of publication are listed in Table 3.\\n\\nTable 3. Notable Companies Developing LLMs\\n\\nCompany LLM OpenAl GPT-3 Davinci (175B) Al21 Labs Jurassic-1-Jumbo (178B) EleutherAl GPT-NeoxX (20B) Anthropic Anthropic-LM (52B) Cohere Cohere xlarge v20220609 (52.4B) NVIDIA/Microsoft Megatron-Turing Natural Language Generation (MT-NLG 530B) Microsoft Turing Natural Language Generation (T-NLG 17B) Google Pathways Language Model (PaLM 540B) Meta Open Pretrained Transformer (OPT-175B)\\n\\nSome of these companies offer other organizations access to their models. For example, enterprises can customize pre-trained modes developed by OpenAl, Cohere, or NVIDIA for downstream tasks or integrate them into their products and internal systems via API.\\n\\nPopular Startup-developed LLM Apps\\n\\nOpenAl’s ChatGPT is by far the most popular LLM-powered app developed to date. It’s estimated that it attracted over 100 million users in just two months after its launch, making it the “fastest-growing consumer application in history.”\\n\\nHowever, many other startups entered the ring with their own, often more specialized and commercialized LLM-powered apps. One of the most popular such apps are LLM-powered content generators like Jasper and Copy.ai. For comparison, Jasper boasts catering to over 100,000 global teams, while Copy.ai claims it has attracted over 5,000,000 users since its launch.\\n\\nFigure 4 shows an example of a natural-language prompt that users can enter in to Copy.ai to generate a blog post outline.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n23\\n\\nFigure 4.\\n\\nExample of Results of a Natural Language Prompt\\n\\n2022-11-04 Untitled (2)\\n\\nBlog Outline\\n\\nCreate Saved (0)\\n\\nWhat is your blog title? tional\\n\\nBest GPT-3 Commercial Use Cases In 2022\\n\\nWhat is the blog about? @ Quick intro to OpenAl\\'s GPT-3 Analyzing GPT-3 powered writing apps like Copy.ai, Lex, Jasper, and Frase.io OpenAl\\'s GPT-3 Vs GPT-3 Powered Writing Apps (Differences between OpenAl\\'s GPT- 3 and product wrappers created around the tech) The future of GPT-3 powered writing apps 2772/1000 Choose atone Enter a custom a hitecers r @ Professional\\n\\nOther examples of popular LLM-powered apps include the beloved grammar-checking and writing tool, Grammarly, and GitHub Copilot, a Codex-powered coding assistant that can help developers write and learn code.\\n\\nPart 2 will cover more ways in which enterprises and startups can leverage LLMs to build specialized apps for content generation, anomaly detection, toxicity classification, and other advanced NLP use cases. It will also provide concrete examples of how they can be further customized to answer the needs of various industries, such as finance, healthcare, and telecommunications, in hopes of inspiring organizations to use LLMs to unlock new possibilities in their respective industries.\\n\\nA Beginner’s Guide to Large Language Models\\n\\n24 Notice\\n\\nThis document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\n\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\n\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\n\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.\\n\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\\n\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the products suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\n\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\n\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\n\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND. SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. Trademarks\\n\\nNVIDIA, the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.\\n\\nCopyright\\n\\n© 2023 NVIDIA Corporation. All rights reserved.\\n\\nNVIDIA Corporation | 2788 San Tomas Expressway, Santa Clara, CA 95051 com http://www.nvi\\n\\nNVIDIA. Transformers and Large Language Models\\n\\nIntroduction to Large Language Models\\n\\nLanguage models\\n\\nRemember the simple n-gram language model ° Assigns probabilities to sequences of words ° Generate text by sampling possible next words e Is trained on counts computed from lots of text Large language models are similar and different: ° Assigns probabilities to sequences of words ° Generate text by sampling possible next words ° Are trained by learning to guess the next word\\n\\ne\\n\\ne\\n\\nNeural Large Language Models (LLMs)\\n\\n° Self-supervised learners\\n\\ne Take a text, remove a word\\n\\ne Use your neural model to guess what the word was\\n\\ne Ifthe model is wrong, use stochastic gradient descent to make the model guess better next time\\n\\ne Advantages (?):\\n\\ne All we need is a lot of text (GPT3: 500 billion tokens) e (And a lot of compute)\\n\\nLLMs are built out of transformers\\n\\nTransformer: a specific kind of network architecture, like fancier feedforward network, but based on attention\\n\\nAttention Is All You Need\\n\\nAshish Vaswani* Google Brain avaswani@google.com\\n\\nNoam Shazeer* Google Brain noam@google.com\\n\\nNiki Parmar* Google Research nikip@google.com\\n\\nJakob Uszkoreit* Google Research usz@google.com\\n\\nLlion Jones* Google Research llion@google.com\\n\\nAidan N. Gomez* ‘ University of Toronto aidan@cs.toronto.edu\\n\\nLukasz Kaiser* Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin* # illia.polosukhin@gmail.com\\n\\na\\n\\nA very approximate timeline\\n\\n1990 Static Word Embeddings\\n\\n2003 Neural Language Model\\n\\n2008 Multi-Task Learning\\n\\n2015 Attention\\n\\n2017 Transformer\\n\\n2018 Contextual Word Embeddings and Pretraining 2019 Prompting\\n\\nA picture of a transformer language model\\n\\nOutput long and thanks for all Softmax over Vocabulary Linear Layer E E E Layers of Transformer Blocks I li SHI Mra nn ly I i = _ wT SEE 3 6B So long and thanks for \\\\ Input Embeddings\\n\\nTransformers and Large Language Models\\n\\nIntroduction to Large Language Models\\n\\nTransformers and Large Language Models\\n\\nAttention\\n\\nInstead of starting with the big picture\\n\\nLet\\'s consider the embeddings for an individual word from a particular layer\\n\\nOutput long and thanks for all “Vocabulary, Linear Layer Layers of Transformer Blocks I li SHI Mra Ni : i Ih li ZezZa —=we Input Embeddings So long and thanks for Problem with static embeddings (word2vec)\\n\\nThey are static! The embedding for a word doesn\\'t reflect how its meaning changes in context.\\n\\nThe chicken didn\\'t cross the street pecause()pas too tired\\n\\nWhat is the meaning represented in the static embedding for ‘it\"?\\n\\nContextual Embeddings\\n\\nIntuition: a representation of meaning of a word should be different in different contexts!\\n\\ne\\n\\nContextual Embedding: each word has a different vector that expresses different meanings depending on the surrounding words\\n\\n¢\\n\\n° How to compute contextual embeddings? ° Attention\\n\\nContextual Embeddings\\n\\nThe chicken didn\\'t cross the street because it\\n\\nWhat should be the properties of \"it\"?\\n\\nThe chicken didn\\'t cross the street because it was too tired The chicken didn\\'t cross the street because it was too trafficy\\n\\nAt\\n\\nthis point in the sentence, it\\'s probably referring to either the chicken or the street\\n\\nIntuition of attention\\n\\nBuild up the contextual embedding from a word by selectively integrating information from all the neighboring words\\n\\nWe say that a word \"attends to\" some neighboring words more than others\\n\\nintuition o* attention:\\n\\navers\\n\\nAttention definition\\n\\nA mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens (at the previous layer).\\n\\nMore formally: a method for doing a weighted sum of vectors.\\n\\nep)\\n\\nAttention is left-to-right\\n\\nCc Oo —_ Cc oO aa ft — oO _ ® > ca) —\\n\\nSimplified version of attention: a sum of prior words weighted by their similarity with the current word\\n\\nGiven a sequence of token embeddings:\\n\\nX,\\n\\nX)\\n\\nX;\\n\\nXy\\n\\nXs\\n\\nX,\\n\\nProduce: a; = a weighted sum of x, through x; Weighted by their similarity to x,\\n\\nscore(x;,X;) = Xj°X;\\n\\nQj; = softmax(score(x;,x;)) Vj <i\\n\\nay = Oj 7X j JS\\n\\nIntuition of attention:\\n\\nLayer 6 The animal didn’t self-attention distribution Layer 5 Cross the because because it Was Was too too tired tired\\n\\nXl x2 x3 x4 x5 N pe pa | Xl\\n\\nIntuition of attention:\\n\\nquery\\n\\nLayer 6 animal didn’t cross because The the self-attention distribution ® (dp) Layer 5 a ® O CC ® }— O ~~ Was Was too too tired tired\\n\\nxl x2 x3 x4 x5 x6 x7 xi\\n\\nValues Attention\\n\\nActually it\\'s slightly more complicated, but | won\\'t get into that High-level idea: instead of just having a query and a set of values, each embedding actually also has a key\\n\\nIntuition of attention:\\n\\nquery\\n\\nLayer 6 The animal didn’t cross the because self-attention distribution Layer 5 because it Was Was too too tired tired\\n\\nxl x2 x3 x4 x5 x6 x7 Xl\\n\\nSummary\\n\\nAttention is a method for enriching the representation of a token by incorporating contextual information\\n\\nThe result: the embedding for each word will be different in different contexts!\\n\\nContextual embeddings: a representation of word meaning in its context.\\n\\nTransformers and Large Language Models\\n\\nAttention\\n\\nTransformers and Large Language Models\\n\\nThe rest of the transformer applied to language modeling\\n\\nThe transformer\\n\\nAttention is just part of computing embeddings in a transformer.\\n\\nLet\\'s see more of the mechanism\\n\\nReminder: transformer language model\\n\\nOutput long and thanks for all Softmax over Vocabulary Linear Layer E E I li I li Layers of SS Transformer Blocks _— — Zt S _ SF ES oS 55s — Input Embeddings So long and thanks for\\n\\nThe residual stream: each token gets passed up and moditied\\n\\ni i+7 Feedforward (Layer Norm | ~ (Layer Norm}...\\n\\nWe\\'ll need nonlinearities, so a feedforward layer\\n\\nhi4 hj Ning Feedforward (Layer Norm | ~ (Layer Norm}...\\n\\nA transformer is a stack of these blocks\\n\\nBlock 2\\n\\nBlock 1\\n\\n-7 i+7 1 i+7 Ai4 hj Pind\\n\\nInputs\\n\\nTransformer Block X = Composite Embeddings (word + position) Word Embeddings Position Embeddings Janet will back the bill\\n\\nLanguage modeling head\\n\\nWord probabilities 1 x |V| Language Model Head Softmax over vocabulary V takes hy and outputs a Logits 1x |V| distribution over vocabulary V Unembedding layer dx |V| Layer L Transformer Block\\n\\nThe final transformer model\\n\\nSample token to generate at position N+1\\n\\nToken probabilities Language Model Head f Layer Norm 1 i] Feedforward Layer Norm i} q ! i , Layer L\\n\\nLayer Norm Layer 2 Layer Norm ( ) ( Feedforward ( | | ! Ts a\\n\\nAdd token + postion embeddings (EwiJEwa) = Ew) + (TPH PF PN)\\n\\nInput tokens\\n\\nwiw2... ~=wN\\n\\nTransformers and Large Language Models\\n\\nThe rest of the transformer applied to language modeling\\n\\nLarge Language Models\\n\\nPretraining (and how to train transformers for language modeling)\\n\\nPretraining\\n\\nThe big idea that underlies all the amazing performance of language models\\n\\nFirst pretrain a transformer model on enormous amounts of text\\n\\nThen apply it to new tasks.\\n\\nIntuition of language model training\\n\\nWe just train them to predict the next word! 1. Take a corpus of text\\n\\n2. At each time step t\\n\\ni. ask the model to predict the next word\\n\\nii. train the model using gradient descent to minimize the error in this prediction\\n\\nIntuition of language model training: loss\\n\\nSame loss function: cross-entropy loss\\n\\n¢\\n\\n¢ We want the model to assign a high probability to true word w\\n\\n° =want loss to be high if the model assigns too low a probability to w\\n\\nCE Loss: The negative log probability that the model assigns to the true next word w\\n\\n°\\n\\ne Ifthe model assigns too low a probability to w\\n\\n° We move the model weights in the direction that assigns higher probability to w\\n\\na\\n\\nTraining a transformer language model\\n\\nNext word long and thanks for all Loss vee Softmax over Vocabulary Linear Layer Transformer Block Input Embeddings So long and thanks for\\n\\n= pdilee t=1\\n\\nPretraining Data: mostly from the web (Common Crawl)\\n\\nThe Pile:\\n\\nBibliotik PG-19 BC2 Subtitles StackExchange OpenWebtText2 Wikipedia\\n\\nWhat does a model learn from pretraining ?\\n\\n¢ There are canines everywhere! One dog in the front room, and two dogs\\n\\n°\\n\\n|t wasn\\'t just big it was enormous\\n\\n¢ The author of \"A Room of One\\'s Own\" is Virginia Woolf\\n\\n¢ The doctor told me that he ¢ The square root of 4 is 2\\n\\nBig idea\\n\\nText contains enormous amounts of knowledge Pretraining on lots of text with all that knowledge is what gives language models their ability to do so much\\n\\nLarge Language Models\\n\\nPretraining (and how to train transformers for language modeling)\\n\\nLarge Language Models Large Language Models: Applying pretrained models to new tasks\\n\\nBig idea\\n\\nMany tasks can be turned into tasks of predicting words!\\n\\nThree architectures for large language models\\n\\nDecoders GPT, Claude, Llama 2 Mixtral\\n\\nEncoders BERT family,\\n\\nHuBERT\\n\\nEncoder-decoders Flan-T5, Whisper\\n\\nDecoders\\n\\nAlso called: ° Causal LLMs ° Autoregressive LLMs ° Left-to-right LLMs\\n\\n¢ Predict words left to right\\n\\nConditional Generation: Generating text conditioned on previous text!\\n\\nCompletion Text\\n\\na. Sample from Softmax linear layer Transformer Blocks Input y | | | l l l | l l | l l | | | | | Embeddings : l | |\\n\\nPrefix Text\\n\\nFraming lots of tasks as conditional generation\\n\\nSentiment analysis: “I like Jackie Chan” 1. We give the language model this string: The sentiment of the sentence like Jackie Chan\" is:\\n\\n2. And see what word it thinks comes next:\\n\\nP(positive|The sentiment of the sentence “I like Jackie Chan” P(negative|The sentiment of the sentence “TI like Jackie Chan”\\n\\n\"I\\n\\nis:) is:)\\n\\nFraming lots of tasks as conditional generation\\n\\nQA: “Who wrote The Origin of Species” 1. We give the language model this string:\\n\\nQ: Who wrote the book ‘‘The Origin of Species\"?\\n\\n2. And see what word it thinks comes next:\\n\\nP(w|Q: Who wrote the book “The Origin of Species”? A:)\\n\\nA:\\n\\nEncoders\\n\\nMany varieties! Popular: Masked Language Models (MLMs) BERT family\\n\\nTrained by predicting words from surrounding words on both sides\\n\\nAre usually fine-tuned (trained on supervised data) for classification tasks.\\n\\nEncoder-Decoders\\n\\nTrained to map from one sequence to another Very popular for:\\n\\n°\\n\\ne\\n\\n° machine translation (map from one language to another)\\n\\n° speech recognition (map from acoustics to words)\\n\\nMany more things | didn\\'t talk about\\n\\nInstruction Fine-tuning Preference Alignment Prompt Engineering\\n\\nWhere to learn these? CS224N!\\n\\nLarge Language Models\\n\\nLarge Language Models: Applying pretrained models to new tasks\\n\\nlarge Language Models\\n\\nHarms of Large Language Models\\n\\nHallucination\\n\\nChatbots May ‘Hallucinate’ More Often Than Many Realize What Can You Do When A.I. Lies About You?\\n\\nPeople have little protection or recourse when the technology creates and spreads falsehoods about them.\\n\\nAir Canada loses court case after its chatbot hallucinated fake policies to a customer\\n\\nThe airline argued that the chatbot itself was liable. The court disagreed.\\n\\nCurrent research direction to address hallucination\\n\\nRetrieval-Augmented Generation (RAG)\\n\\nUse information retrieval to retrieve some passages from a high-quality source\\n\\nThen use a language model to generate an answer from those passages\\n\\nCopyright\\n\\nAuthors Sue OpenAlI Claiming Mass Copyright Infringement of Hundreds of Thousands of Novels\\n\\nThe Times Sues OpenAl and Microsoft Over A.I. Use of Copyrighted Work\\n\\nto train = — ~ — = —-— = == =: ———j — — SS SSS:\\n\\nMillions of articles from The New York Times were used to train chatbots that now compete with it, the lawsuit said.\\n\\nPrivacy\\n\\nToxicity and Abuse\\n\\nThe New Al-Powered Bing Is Threatening Users.\\n\\nCleaning Up ChatGPT Takes Heavy Toll on Human Workers\\n\\nContractors in Kenya say they were traumatized by effort to screen out descriptions of violence and sexual abuse during run-up to OpenAl’s hit chatbot\\n\\nMisinformation\\n\\nChatbots are generating false and misleading information about U.S. elections\\n\\nVast growth in interest in Ethics of LLMs and Al\\n\\nNumber of Accepted FAccT Conference Submissions by Affiliation, 2018-22\\n\\nSource: FAccT, 2022 | Chart: 2023 Al Index Report\\n\\nEducation Industry Government Nonprofit Other 600 a fo} fo} 400 Ww fo} fo} 200 100 2018 2019 2020 2021 2022\\n\\nNumber of Papers\\n\\n2022 HAI: AI Index 2023\\n\\nlarge Language Models\\n\\nHarms of Large Language Models\\n\\nLarge Language Models\\n\\nOur last class together!\\n\\nLearning goals\\n\\nWrite regular expressions for text tasks Apply the edit distance algorithm Build a supervised classifier Build a search engine Work with neural word embeddings Train a neural network Build a recommendation engine Build a chatbot Prompt a large language model\\n\\nWhat\\'s next? Spring 2024 NLP adjacent courses CS224N: Natural Language Processing with Deep Learning (Chris Manning)\\n\\nAlgorithmic internals: transformers, GPT, parsing, machine translation and other applications. More of the gory details! More math, more machine learning\\n\\nCS224C: NLP for Computational Social Science (Diyi Yang)\\n\\n..machine learning and theories from social science to study human behaviors and important societal questions at scale. NLP, social networks, causal inference, application to social topics like hate speech, misinformation, and social movements.\\n\\nCS 224S: Spoken Language Processing: (Andrew Maas)\\n\\nIntroduction to spoken language technology with an emphasis on dialogue and conversational systems.\\n\\nCS 336: Language Modeling from Scratch (Tatsu Hashimoto and Percy Liang)\\n\\nevery aspect of language model creation, including data collection and cleansing for pre- training, transformer model construction, model training, and evaluation before deployment. Application required.\\n\\nCS 246: Mining Massive Data Sets (Jure Leskovec)\\n\\nNext year courses!\\n\\nCS 224V: Conversational Virtual Assistants with Deep Learning (Monica Lam)\\n\\nTopics include: (1) growing LLMs\\' knowledge, (2) stopping LLMs from hallucination (3) experimentation and evaluation of conversational assistants based on LLMs, (5) controlling LLMs to achieve tasks, (6) persuasive LLMs, (7) multilingual assistants, and (8) combining voice and graphical interfaces.\\n\\nCS329X Human Centered NLP) (Diyi Yang)\\n\\nhuman-centered design thinking in NLP, human-in-the-loop algorithms, fairness, and accessibility.\\n\\nCS329R Race and NLP (Dan Jurafsky and Jennifer Eberhardt)\\n\\nIntegrate methods from natural language processing with social psychological perspectives on race to build practical systems that address significant societal issues\\n\\nFun courses outside of CS\\n\\nSpring:\\n\\nLinguist 173: Invented Languages Linguist 134A: The Structure of Discourse Linguist 156: Language, Gender, and Sexuality COMM 154: The Politics of Algorithms\\n\\nNext year:\\n\\nLinguistics 150: Language and Society\\n\\nLinguistics 130a: Introduction to semantics & pragmatics'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50704056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272834"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a008309",
   "metadata": {},
   "source": [
    "#### Split the entire text_string into 1-sentence chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3abb2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2071 senteneces were found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Splitting the essay on '.', '?', and '!'\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', text_string)\n",
    "print (f\"{len(single_sentences_list)} senteneces were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ba93450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x2c7145ff000>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(single_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab334d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2071"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08813bdd",
   "metadata": {},
   "source": [
    "#### convert this list of sentences to list of doctionary  (List[dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c0d0e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS.',\n",
       "  'index': 0},\n",
       " {'sentence': '624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.',\n",
       "  'index': 1},\n",
       " {'sentence': 'The user has requested enhancement of the downloaded file.',\n",
       "  'index': 2}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47fb2b",
   "metadata": {},
   "source": [
    "##### Combine the sentence before and after so that we reduce noise and capture more of the relationships between sequential sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef5e2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences = combine_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS.',\n",
       "  'index': 0,\n",
       "  'combined_sentence': 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.'},\n",
       " {'sentence': '624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.',\n",
       "  'index': 1,\n",
       "  'combined_sentence': 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025. The user has requested enhancement of the downloaded file.'},\n",
       " {'sentence': 'The user has requested enhancement of the downloaded file.',\n",
       "  'index': 2,\n",
       "  'combined_sentence': '624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025. The user has requested enhancement of the downloaded file. ResearchGate\\n\\nInternational Journal of Computer Engineering and Technology (IJCET)\\n\\nInternational Journal of Computer Engineering and Technology (IJCET) Volume 16, Issue 1, Jan-Feb 2025, pp.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60397a",
   "metadata": {},
   "source": [
    "Now I want to get embeddings for the combined sentences, so we can get the distances between the groups of 3 and find breakpoints. I'll use OpenAI's embeddings for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908babc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#oaiembeds = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad801057",
   "metadata": {},
   "source": [
    "Get the embeddings and do it in batches to make it quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4712df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = hf_embeddings.embed_documents([x['combined_sentence'] for x in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1a16d",
   "metadata": {},
   "source": [
    "We have list of embeddings that we want to add to our list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d240d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749b325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'CS329R Race and NLP (Dan Jurafsky and Jennifer Eberhardt)\\n\\nIntegrate methods from natural language processing with social psychological perspectives on race to build practical systems that address significant societal issues\\n\\nFun courses outside of CS\\n\\nSpring:\\n\\nLinguist 173: Invented Languages Linguist 134A: The Structure of Discourse Linguist 156: Language, Gender, and Sexuality COMM 154: The Politics of Algorithms\\n\\nNext year:\\n\\nLinguistics 150: Language and Society\\n\\nLinguistics 130a: Introduction to semantics & pragmatics',\n",
       " 'index': 2070,\n",
       " 'combined_sentence': 'CS329X Human Centered NLP) (Diyi Yang)\\n\\nhuman-centered design thinking in NLP, human-in-the-loop algorithms, fairness, and accessibility. CS329R Race and NLP (Dan Jurafsky and Jennifer Eberhardt)\\n\\nIntegrate methods from natural language processing with social psychological perspectives on race to build practical systems that address significant societal issues\\n\\nFun courses outside of CS\\n\\nSpring:\\n\\nLinguist 173: Invented Languages Linguist 134A: The Structure of Discourse Linguist 156: Language, Gender, and Sexuality COMM 154: The Politics of Algorithms\\n\\nNext year:\\n\\nLinguistics 150: Language and Society\\n\\nLinguistics 130a: Introduction to semantics & pragmatics',\n",
       " 'combined_sentence_embedding': [0.062472179532051086,\n",
       "  0.08914627879858017,\n",
       "  -0.044796839356422424,\n",
       "  0.02988601289689541,\n",
       "  -0.013650881126523018,\n",
       "  0.006889938376843929,\n",
       "  -0.0113496920093894,\n",
       "  0.002437766408547759,\n",
       "  0.019794994965195656,\n",
       "  -0.07791659981012344,\n",
       "  0.06696818768978119,\n",
       "  0.0015604905784130096,\n",
       "  0.016885165125131607,\n",
       "  -0.02132447250187397,\n",
       "  0.020643290132284164,\n",
       "  -0.07973562180995941,\n",
       "  -0.0011901838006451726,\n",
       "  0.0419352650642395,\n",
       "  -0.022071590647101402,\n",
       "  -0.004541512578725815,\n",
       "  0.025224676355719566,\n",
       "  0.0436846949160099,\n",
       "  -0.005487874150276184,\n",
       "  0.07117579877376556,\n",
       "  -0.01530170626938343,\n",
       "  -0.02045835182070732,\n",
       "  0.019938083365559578,\n",
       "  -0.006746360566467047,\n",
       "  -0.022394616156816483,\n",
       "  -0.008186870254576206,\n",
       "  0.01871061511337757,\n",
       "  0.05573758855462074,\n",
       "  -0.02711225114762783,\n",
       "  0.03300127014517784,\n",
       "  2.1496773570106598e-06,\n",
       "  -0.07181990146636963,\n",
       "  0.008034893311560154,\n",
       "  0.0049249399453401566,\n",
       "  -0.030256658792495728,\n",
       "  -0.01744738593697548,\n",
       "  0.0635601282119751,\n",
       "  0.016859306022524834,\n",
       "  -0.022089853882789612,\n",
       "  0.006765121594071388,\n",
       "  -0.058355413377285004,\n",
       "  0.03966662660241127,\n",
       "  0.048988278955221176,\n",
       "  0.010365800000727177,\n",
       "  0.02267874963581562,\n",
       "  0.009215665981173515,\n",
       "  -0.00540827726945281,\n",
       "  0.0029597687534987926,\n",
       "  -0.0315994992852211,\n",
       "  0.014167682267725468,\n",
       "  -0.007471438497304916,\n",
       "  -0.028725244104862213,\n",
       "  0.01948774978518486,\n",
       "  -0.04462621361017227,\n",
       "  0.06846997886896133,\n",
       "  -0.032077010720968246,\n",
       "  -0.03438667953014374,\n",
       "  0.022722318768501282,\n",
       "  0.005707226227968931,\n",
       "  -0.017991481348872185,\n",
       "  0.04356084764003754,\n",
       "  0.0025152384769171476,\n",
       "  -0.03947421908378601,\n",
       "  -0.03881961479783058,\n",
       "  -0.0096881790086627,\n",
       "  0.030038589611649513,\n",
       "  0.07885684072971344,\n",
       "  0.018800152465701103,\n",
       "  0.013144819997251034,\n",
       "  0.013098787516355515,\n",
       "  -0.01773133873939514,\n",
       "  -0.018367180600762367,\n",
       "  0.0024567744694650173,\n",
       "  0.038062043488025665,\n",
       "  -0.03653731942176819,\n",
       "  -0.0097302021458745,\n",
       "  -0.008300662972033024,\n",
       "  -0.009873555041849613,\n",
       "  -0.00286621181294322,\n",
       "  0.06919368356466293,\n",
       "  0.04245491325855255,\n",
       "  0.07223168015480042,\n",
       "  0.0013750898651778698,\n",
       "  -0.005553413648158312,\n",
       "  0.027614358812570572,\n",
       "  0.030552593991160393,\n",
       "  0.08051200211048126,\n",
       "  -0.04280121996998787,\n",
       "  0.019331717863678932,\n",
       "  0.017996741458773613,\n",
       "  -0.03419288992881775,\n",
       "  0.003258022479712963,\n",
       "  0.04263308271765709,\n",
       "  -0.009827124886214733,\n",
       "  0.031211508437991142,\n",
       "  -0.06922458857297897,\n",
       "  0.01692947745323181,\n",
       "  0.02565470151603222,\n",
       "  0.03427692502737045,\n",
       "  -0.00164584640879184,\n",
       "  0.03697158768773079,\n",
       "  0.020286694169044495,\n",
       "  0.016057796776294708,\n",
       "  -0.01909528113901615,\n",
       "  0.0008943117572925985,\n",
       "  0.012628448195755482,\n",
       "  -0.02837449498474598,\n",
       "  -0.03241394832730293,\n",
       "  -0.029265759512782097,\n",
       "  0.019588002935051918,\n",
       "  -0.02726675383746624,\n",
       "  0.06324838101863861,\n",
       "  -0.05597197264432907,\n",
       "  0.013516332022845745,\n",
       "  -0.053813833743333817,\n",
       "  -0.0005154761020094156,\n",
       "  -0.043647658079862595,\n",
       "  0.026664594188332558,\n",
       "  -0.01447847206145525,\n",
       "  0.0061402712017297745,\n",
       "  -0.03169046714901924,\n",
       "  -0.034007035195827484,\n",
       "  -0.04334765300154686,\n",
       "  -0.005476603750139475,\n",
       "  -0.03693369776010513,\n",
       "  -0.0774591863155365,\n",
       "  0.027892829850316048,\n",
       "  -0.014000192284584045,\n",
       "  -0.04780742898583412,\n",
       "  -0.03619890287518501,\n",
       "  -0.005173319019377232,\n",
       "  0.0403708778321743,\n",
       "  0.020054500550031662,\n",
       "  -0.02129654586315155,\n",
       "  -0.03968868777155876,\n",
       "  -0.05349859595298767,\n",
       "  -0.013286209665238857,\n",
       "  0.00710385711863637,\n",
       "  -0.03219776228070259,\n",
       "  -0.06900674104690552,\n",
       "  -0.014876212924718857,\n",
       "  0.0040768724866211414,\n",
       "  0.018580352887511253,\n",
       "  -0.08130861818790436,\n",
       "  -0.02275441214442253,\n",
       "  0.02752169966697693,\n",
       "  -0.03879302740097046,\n",
       "  0.06765499711036682,\n",
       "  -0.014541076496243477,\n",
       "  -0.016554657369852066,\n",
       "  0.053129713982343674,\n",
       "  0.008360130712389946,\n",
       "  0.06539960950613022,\n",
       "  0.07132919132709503,\n",
       "  0.05067144334316254,\n",
       "  -0.013351665809750557,\n",
       "  0.08812767267227173,\n",
       "  -0.015486899763345718,\n",
       "  -0.013147406280040741,\n",
       "  0.011233033612370491,\n",
       "  -0.03594904765486717,\n",
       "  -0.02140141651034355,\n",
       "  -0.008421984501183033,\n",
       "  0.008795623667538166,\n",
       "  -0.004428061656653881,\n",
       "  0.007942968048155308,\n",
       "  0.0033491163048893213,\n",
       "  0.05458112061023712,\n",
       "  0.004641519859433174,\n",
       "  -0.006010452285408974,\n",
       "  0.006962096318602562,\n",
       "  0.09689097106456757,\n",
       "  0.003651323728263378,\n",
       "  0.05398283153772354,\n",
       "  0.007942560128867626,\n",
       "  0.03838764503598213,\n",
       "  -0.04749973490834236,\n",
       "  0.0020526479929685593,\n",
       "  -0.028421925380825996,\n",
       "  0.000410491949878633,\n",
       "  -0.04114967957139015,\n",
       "  -0.034025005996227264,\n",
       "  -0.0075201126746833324,\n",
       "  0.016833316534757614,\n",
       "  3.288519656052813e-05,\n",
       "  -0.046605490148067474,\n",
       "  -0.03463234379887581,\n",
       "  0.01737126149237156,\n",
       "  0.05379794165492058,\n",
       "  0.00939155649393797,\n",
       "  -0.027205513790249825,\n",
       "  0.023077690973877907,\n",
       "  0.023890309035778046,\n",
       "  -0.0081467991694808,\n",
       "  0.02773278020322323,\n",
       "  -0.04979448392987251,\n",
       "  -0.018460167571902275,\n",
       "  -0.04347716644406319,\n",
       "  -0.03767495974898338,\n",
       "  0.07382688671350479,\n",
       "  -0.021456070244312286,\n",
       "  -0.01042868010699749,\n",
       "  0.036300718784332275,\n",
       "  -0.02581118419766426,\n",
       "  -0.015550442039966583,\n",
       "  0.06715086102485657,\n",
       "  0.031971026211977005,\n",
       "  -0.006241141818463802,\n",
       "  -0.014444560743868351,\n",
       "  -0.0004240394337102771,\n",
       "  0.026964807882905006,\n",
       "  0.04649188742041588,\n",
       "  0.0013142428360879421,\n",
       "  -0.0024150917306542397,\n",
       "  -0.007820473983883858,\n",
       "  0.05592086911201477,\n",
       "  -0.006900546606630087,\n",
       "  0.0491582415997982,\n",
       "  -0.0731046050786972,\n",
       "  0.03140096738934517,\n",
       "  -0.0003402084403205663,\n",
       "  -0.01801927387714386,\n",
       "  -0.012701481580734253,\n",
       "  0.005896954797208309,\n",
       "  0.015327977947890759,\n",
       "  0.0503842793405056,\n",
       "  -0.028869809582829475,\n",
       "  0.021903404965996742,\n",
       "  -0.030542459338903427,\n",
       "  -0.007552148774266243,\n",
       "  -0.021949224174022675,\n",
       "  -0.0007775446283631027,\n",
       "  -0.01687050051987171,\n",
       "  0.03426404297351837,\n",
       "  0.059965480118989944,\n",
       "  -0.09846951812505722,\n",
       "  -0.03812012821435928,\n",
       "  0.07310769706964493,\n",
       "  0.04338117688894272,\n",
       "  0.005413954611867666,\n",
       "  0.08528625220060349,\n",
       "  -0.060121845453977585,\n",
       "  0.02912333607673645,\n",
       "  -0.029941337183117867,\n",
       "  0.0011039376258850098,\n",
       "  0.010204648599028587,\n",
       "  0.03273223340511322,\n",
       "  -0.06100480630993843,\n",
       "  -0.0010106327245011926,\n",
       "  0.015088285319507122,\n",
       "  0.03744442015886307,\n",
       "  -0.026286054402589798,\n",
       "  -0.0476224347949028,\n",
       "  0.025523534044623375,\n",
       "  -0.029295295476913452,\n",
       "  -0.0456446036696434,\n",
       "  0.059620004147291183,\n",
       "  -0.0017697858856990933,\n",
       "  0.031230295076966286,\n",
       "  0.07684873044490814,\n",
       "  0.03548469394445419,\n",
       "  -0.0561685748398304,\n",
       "  0.02929246984422207,\n",
       "  -0.06645721942186356,\n",
       "  -0.01289617083966732,\n",
       "  0.005594151560217142,\n",
       "  -0.013893979601562023,\n",
       "  -0.03193432837724686,\n",
       "  0.01621805690228939,\n",
       "  -0.01916317082941532,\n",
       "  -0.015304404310882092,\n",
       "  0.03996972367167473,\n",
       "  -0.04017919674515724,\n",
       "  -0.021600035950541496,\n",
       "  -0.009293579496443272,\n",
       "  0.03204188868403435,\n",
       "  -0.028313564136624336,\n",
       "  -0.012700282037258148,\n",
       "  -0.005693071521818638,\n",
       "  -0.03108719177544117,\n",
       "  -0.029873793944716454,\n",
       "  0.041283007711172104,\n",
       "  0.049019668251276016,\n",
       "  -0.05701592564582825,\n",
       "  -0.004767740145325661,\n",
       "  -0.004494823049753904,\n",
       "  -0.015356667339801788,\n",
       "  -0.00848529301583767,\n",
       "  0.02182307280600071,\n",
       "  -0.0045108795166015625,\n",
       "  0.024022452533245087,\n",
       "  0.04848158359527588,\n",
       "  0.004025783855468035,\n",
       "  0.05803298205137253,\n",
       "  0.0013805769849568605,\n",
       "  0.048158999532461166,\n",
       "  0.006179837975651026,\n",
       "  -0.02991432510316372,\n",
       "  -0.020266933366656303,\n",
       "  -0.0015683048404753208,\n",
       "  0.01589253917336464,\n",
       "  0.017865367233753204,\n",
       "  0.01677638292312622,\n",
       "  -0.024742750450968742,\n",
       "  0.013276055455207825,\n",
       "  0.04585236683487892,\n",
       "  -0.003845356637611985,\n",
       "  -0.03200472518801689,\n",
       "  -0.0019720871932804585,\n",
       "  0.04531174525618553,\n",
       "  -0.10182914137840271,\n",
       "  -0.037437114864587784,\n",
       "  -0.039949338883161545,\n",
       "  -0.062437474727630615,\n",
       "  -0.04722439497709274,\n",
       "  0.03328734263777733,\n",
       "  -0.0074668582528829575,\n",
       "  -0.0022673332132399082,\n",
       "  0.06192444637417793,\n",
       "  0.012844021432101727,\n",
       "  -0.0172030758112669,\n",
       "  -0.028471020981669426,\n",
       "  -0.025423221290111542,\n",
       "  -0.02643202617764473,\n",
       "  0.05327752232551575,\n",
       "  -0.0596633143723011,\n",
       "  0.028012484312057495,\n",
       "  0.057927511632442474,\n",
       "  -0.03740880265831947,\n",
       "  -0.02745836041867733,\n",
       "  -0.030583664774894714,\n",
       "  0.018556857481598854,\n",
       "  -0.0570087656378746,\n",
       "  0.024389756843447685,\n",
       "  -0.03452061489224434,\n",
       "  0.025663696229457855,\n",
       "  -0.033612560480833054,\n",
       "  -0.07110635191202164,\n",
       "  -0.03580272197723389,\n",
       "  0.007240832317620516,\n",
       "  -0.062405575066804886,\n",
       "  0.01907617785036564,\n",
       "  0.036759842187166214,\n",
       "  -0.02060851640999317,\n",
       "  -0.015671202912926674,\n",
       "  -0.023274777457118034,\n",
       "  0.026382893323898315,\n",
       "  0.014394926838576794,\n",
       "  0.0006174109876155853,\n",
       "  -0.015934953466057777,\n",
       "  0.007913360372185707,\n",
       "  -0.01149820163846016,\n",
       "  0.0002885976282414049,\n",
       "  0.026396118104457855,\n",
       "  0.018040675669908524,\n",
       "  0.008387192152440548,\n",
       "  0.0024259809870272875,\n",
       "  0.04834140092134476,\n",
       "  -0.014968384988605976,\n",
       "  0.005845041014254093,\n",
       "  0.0232089851051569,\n",
       "  0.029460148885846138,\n",
       "  0.07829077541828156,\n",
       "  -0.05221370980143547,\n",
       "  -0.01586398482322693,\n",
       "  0.026518015190958977,\n",
       "  0.031375784426927567,\n",
       "  -0.02216721512377262,\n",
       "  0.03107682801783085,\n",
       "  0.07180584222078323,\n",
       "  0.03984278440475464,\n",
       "  -0.05157756060361862,\n",
       "  0.022530462592840195,\n",
       "  -0.018277106806635857,\n",
       "  0.0010794076370075345,\n",
       "  0.021797966212034225,\n",
       "  0.04117530584335327,\n",
       "  -0.06257620453834534,\n",
       "  0.028083596378564835,\n",
       "  0.00774365896359086,\n",
       "  0.018944811075925827,\n",
       "  -0.07786238938570023,\n",
       "  -0.07034932076931,\n",
       "  -0.021942496299743652,\n",
       "  -0.06813258677721024,\n",
       "  -0.0012744362466037273,\n",
       "  -0.012729326263070107,\n",
       "  -0.03192209452390671,\n",
       "  -0.002234791871160269,\n",
       "  -0.02322230488061905,\n",
       "  -0.014156168326735497,\n",
       "  0.029746130108833313,\n",
       "  0.009187482297420502,\n",
       "  -0.0024448330514132977,\n",
       "  -0.013179455883800983,\n",
       "  0.033849816769361496,\n",
       "  0.05005897209048271,\n",
       "  -0.10178717970848083,\n",
       "  -0.003851943649351597,\n",
       "  -0.002187347039580345,\n",
       "  -0.04052304849028587,\n",
       "  0.028262875974178314,\n",
       "  0.04286568984389305,\n",
       "  0.07073713093996048,\n",
       "  -0.016803085803985596,\n",
       "  -0.03573126718401909,\n",
       "  0.03509965166449547,\n",
       "  -0.01350227277725935,\n",
       "  0.002769340295344591,\n",
       "  -0.038179632276296616,\n",
       "  0.0043476056307554245,\n",
       "  -0.04694242402911186,\n",
       "  0.002529452322050929,\n",
       "  -0.08783568441867828,\n",
       "  0.011741732247173786,\n",
       "  -0.04522126540541649,\n",
       "  -0.010242302902042866,\n",
       "  0.04049154371023178,\n",
       "  0.02458748035132885,\n",
       "  -0.006476290058344603,\n",
       "  0.010407401248812675,\n",
       "  -0.06519383192062378,\n",
       "  0.0092244241386652,\n",
       "  -0.019317708909511566,\n",
       "  0.03167249262332916,\n",
       "  0.020389365032315254,\n",
       "  -0.048711370676755905,\n",
       "  -0.012329582124948502,\n",
       "  -0.10093383491039276,\n",
       "  0.017588956281542778,\n",
       "  -0.056282009929418564,\n",
       "  -0.1264357715845108,\n",
       "  0.011070703156292439,\n",
       "  -0.0013553652679547668,\n",
       "  -0.07212904840707779,\n",
       "  -0.029155924916267395,\n",
       "  -0.08680799603462219,\n",
       "  0.0482497438788414,\n",
       "  0.00018639385234564543,\n",
       "  0.017883405089378357,\n",
       "  -0.028523443266749382,\n",
       "  0.024745486676692963,\n",
       "  0.013315708376467228,\n",
       "  -0.10059158504009247,\n",
       "  0.026312410831451416,\n",
       "  -0.019569842144846916,\n",
       "  0.05529317632317543,\n",
       "  -0.01313617080450058,\n",
       "  -0.027833882719278336,\n",
       "  -0.013141780160367489,\n",
       "  0.0007928732084110379,\n",
       "  -0.008708770386874676,\n",
       "  0.07120408862829208,\n",
       "  0.011081275530159473,\n",
       "  -0.049891162663698196,\n",
       "  0.05019448325037956,\n",
       "  -0.0029578248504549265,\n",
       "  0.025672495365142822,\n",
       "  0.004244809504598379,\n",
       "  0.0008873050683178008,\n",
       "  -0.09393830597400665,\n",
       "  -0.056507501751184464,\n",
       "  0.06113426014780998,\n",
       "  0.023306945338845253,\n",
       "  0.03052130714058876,\n",
       "  0.004250369034707546,\n",
       "  -0.008843811228871346,\n",
       "  0.021846702322363853,\n",
       "  0.011072512716054916,\n",
       "  0.02646935172379017,\n",
       "  0.021993767470121384,\n",
       "  0.04642690345644951,\n",
       "  0.0021120605524629354,\n",
       "  -0.0027645279187709093,\n",
       "  -0.007890371605753899,\n",
       "  0.03746318444609642,\n",
       "  0.026524513959884644,\n",
       "  -0.008471922017633915,\n",
       "  -0.06560259312391281,\n",
       "  -0.006235644221305847,\n",
       "  -0.003964378032833338,\n",
       "  -0.05994727835059166,\n",
       "  0.012864540331065655,\n",
       "  -0.04708537831902504,\n",
       "  -0.013361361809074879,\n",
       "  -0.02594292350113392,\n",
       "  -0.014902226626873016,\n",
       "  0.0425640232861042,\n",
       "  -0.04032513499259949,\n",
       "  0.017651010304689407,\n",
       "  -0.04038956016302109,\n",
       "  0.042448535561561584,\n",
       "  0.005690949968993664,\n",
       "  0.08318273723125458,\n",
       "  0.022162586450576782,\n",
       "  0.04220176860690117,\n",
       "  -0.01791139505803585,\n",
       "  0.010800372809171677,\n",
       "  0.03012853115797043,\n",
       "  0.04042234644293785,\n",
       "  -0.02672094665467739,\n",
       "  0.05136463791131973,\n",
       "  0.007255207747220993,\n",
       "  0.01467174757272005,\n",
       "  0.035519592463970184,\n",
       "  0.002477456582710147,\n",
       "  0.01704128086566925,\n",
       "  0.00976076815277338,\n",
       "  -0.015808679163455963,\n",
       "  -0.036993999034166336,\n",
       "  0.00981659721583128,\n",
       "  -0.011080078780651093,\n",
       "  0.017012927681207657,\n",
       "  -0.01392852421849966,\n",
       "  0.007674048654735088,\n",
       "  0.026774095371365547,\n",
       "  0.01643877476453781,\n",
       "  0.02465910278260708,\n",
       "  -0.0030570037197321653,\n",
       "  -0.013531643897294998,\n",
       "  -0.023152578622102737,\n",
       "  -3.0604609492002055e-05,\n",
       "  0.01878444105386734,\n",
       "  -0.009322025813162327,\n",
       "  -0.05412966385483742,\n",
       "  0.010139789432287216,\n",
       "  -0.016455495730042458,\n",
       "  -0.016981907188892365,\n",
       "  0.011854679323732853,\n",
       "  -0.03588022291660309,\n",
       "  -0.01615181379020214,\n",
       "  -0.028443869203329086,\n",
       "  -0.021109309047460556,\n",
       "  0.011905350722372532,\n",
       "  0.020410781726241112,\n",
       "  0.03325871750712395,\n",
       "  -0.14209915697574615,\n",
       "  -0.065917007625103,\n",
       "  0.014866544865071774,\n",
       "  -0.00480924267321825,\n",
       "  0.041994232684373856,\n",
       "  0.026321008801460266,\n",
       "  0.046420611441135406,\n",
       "  0.020260024815797806,\n",
       "  0.013533523306250572,\n",
       "  -0.06020582839846611,\n",
       "  -0.04251575842499733,\n",
       "  0.013468592427670956,\n",
       "  -0.009276201017200947,\n",
       "  -0.023519165813922882,\n",
       "  0.0172747615724802,\n",
       "  -6.573689874796616e-33,\n",
       "  -0.016628289595246315,\n",
       "  -0.0802026316523552,\n",
       "  0.011459854431450367,\n",
       "  -0.030051305890083313,\n",
       "  -0.04205242916941643,\n",
       "  -0.024344030767679214,\n",
       "  -0.0002776725450530648,\n",
       "  0.0450020469725132,\n",
       "  -0.014701765961945057,\n",
       "  0.027026552706956863,\n",
       "  -0.03865036740899086,\n",
       "  0.03524220362305641,\n",
       "  0.0018946825293824077,\n",
       "  0.011677849106490612,\n",
       "  0.033552344888448715,\n",
       "  0.0390118770301342,\n",
       "  0.017547501251101494,\n",
       "  0.016511108726263046,\n",
       "  -0.0007867504027672112,\n",
       "  -0.04415404424071312,\n",
       "  -0.01479809358716011,\n",
       "  0.022514304146170616,\n",
       "  0.07390308380126953,\n",
       "  -0.0603073313832283,\n",
       "  0.07788202166557312,\n",
       "  0.031670715659856796,\n",
       "  0.006788518745452166,\n",
       "  0.0029495598282665014,\n",
       "  0.05434989929199219,\n",
       "  -0.019808897748589516,\n",
       "  -0.015760071575641632,\n",
       "  0.07910412549972534,\n",
       "  0.040856022387742996,\n",
       "  -0.026901859790086746,\n",
       "  0.0024269926361739635,\n",
       "  0.04143882170319557,\n",
       "  -0.04214375093579292,\n",
       "  -0.04820837825536728,\n",
       "  0.03037901408970356,\n",
       "  0.0308810044080019,\n",
       "  0.013411492109298706,\n",
       "  -0.06949122995138168,\n",
       "  0.02675650827586651,\n",
       "  -0.0012883684830740094,\n",
       "  -0.08534575998783112,\n",
       "  0.000488957273773849,\n",
       "  0.06640107929706573,\n",
       "  0.011323000304400921,\n",
       "  0.049715880304574966,\n",
       "  0.03383887931704521,\n",
       "  -0.06068003550171852,\n",
       "  0.029513029381632805,\n",
       "  -0.055343467742204666,\n",
       "  0.0335044302046299,\n",
       "  0.009065561927855015,\n",
       "  -0.030624890699982643,\n",
       "  -0.0049605052918195724,\n",
       "  -0.008210212923586369,\n",
       "  -0.05221083015203476,\n",
       "  0.02165828086435795,\n",
       "  -0.032929226756095886,\n",
       "  0.0019050132250413299,\n",
       "  0.011246616952121258,\n",
       "  0.023388037458062172,\n",
       "  -0.004738816991448402,\n",
       "  0.02544490434229374,\n",
       "  0.043161239475011826,\n",
       "  0.044563110917806625,\n",
       "  -0.019730590283870697,\n",
       "  -0.016070064157247543,\n",
       "  0.04473726078867912,\n",
       "  -0.002315250691026449,\n",
       "  0.016124406829476357,\n",
       "  0.06041809171438217,\n",
       "  -0.01403700653463602,\n",
       "  -0.05640052258968353,\n",
       "  -0.0018608568934723735,\n",
       "  0.0023863238748162985,\n",
       "  0.0334312841296196,\n",
       "  0.003900909563526511,\n",
       "  -0.008033606223762035,\n",
       "  -0.020419258624315262,\n",
       "  -0.07736421376466751,\n",
       "  -0.03986333683133125,\n",
       "  -0.01147711556404829,\n",
       "  -0.025088069960474968,\n",
       "  -0.002893904224038124,\n",
       "  0.004845485091209412,\n",
       "  0.0006497750291600823,\n",
       "  -0.0430888868868351,\n",
       "  0.0003068647056352347,\n",
       "  0.029992645606398582,\n",
       "  -0.036692943423986435,\n",
       "  -0.06228591501712799,\n",
       "  -0.03977986425161362,\n",
       "  -0.06102922558784485,\n",
       "  0.018340999260544777,\n",
       "  -0.04095320776104927,\n",
       "  0.02410443313419819,\n",
       "  -0.007969729602336884,\n",
       "  -0.01082812249660492,\n",
       "  0.02628212235867977,\n",
       "  -0.028273796662688255,\n",
       "  0.07174341380596161,\n",
       "  0.029824109748005867,\n",
       "  -0.03733355551958084,\n",
       "  -0.054844655096530914,\n",
       "  0.0017694473499432206,\n",
       "  -0.04756453260779381,\n",
       "  -0.018860023468732834,\n",
       "  -0.014403031207621098,\n",
       "  0.029879992827773094,\n",
       "  0.03981063887476921,\n",
       "  0.03376990556716919,\n",
       "  0.025176579132676125,\n",
       "  0.05417380854487419,\n",
       "  -0.012910667806863785,\n",
       "  0.02651979960501194,\n",
       "  0.04044697806239128,\n",
       "  -0.011629958637058735,\n",
       "  -0.01909469999372959,\n",
       "  0.009183702059090137,\n",
       "  -0.017572907730937004,\n",
       "  0.03036276251077652,\n",
       "  -0.04253963753581047,\n",
       "  -0.03463684394955635,\n",
       "  0.012175352312624454,\n",
       "  0.03153393045067787,\n",
       "  0.08741489797830582,\n",
       "  -0.04521230608224869,\n",
       "  -0.024298233911395073,\n",
       "  -0.05939093977212906,\n",
       "  2.792266116102837e-07,\n",
       "  0.020135775208473206,\n",
       "  0.05520932748913765,\n",
       "  0.019273504614830017,\n",
       "  0.009830000810325146,\n",
       "  -0.0006426562322303653,\n",
       "  -0.007171069271862507,\n",
       "  0.020424749702215195,\n",
       "  0.04701193794608116,\n",
       "  -0.006993208080530167,\n",
       "  0.009413565509021282,\n",
       "  -0.027424657717347145,\n",
       "  0.005080644506961107,\n",
       "  0.023416170850396156,\n",
       "  0.010999415069818497,\n",
       "  -0.08671864867210388,\n",
       "  -0.01793992705643177,\n",
       "  -0.034639254212379456,\n",
       "  -0.0037827936466783285,\n",
       "  -0.03565279766917229,\n",
       "  0.05444735288619995,\n",
       "  0.02455078810453415,\n",
       "  0.035176049917936325,\n",
       "  0.032912708818912506,\n",
       "  -0.013506320305168629,\n",
       "  -0.07231388986110687,\n",
       "  -0.015902383252978325,\n",
       "  -0.047959908843040466,\n",
       "  -0.030196547508239746,\n",
       "  0.04395144060254097,\n",
       "  0.013403709046542645,\n",
       "  0.0296351108700037,\n",
       "  -0.011605695821344852,\n",
       "  0.02580353617668152,\n",
       "  0.026251355186104774,\n",
       "  0.022197047248482704,\n",
       "  0.0006498990696854889,\n",
       "  0.0425887331366539,\n",
       "  0.021183842793107033,\n",
       "  -0.031700558960437775,\n",
       "  -0.0008182550081983209,\n",
       "  -0.018869780004024506,\n",
       "  -0.030210323631763458,\n",
       "  -0.018106231465935707,\n",
       "  -0.04140085726976395,\n",
       "  0.030712120234966278,\n",
       "  0.05618470907211304,\n",
       "  -0.0512048676609993,\n",
       "  -0.02465846948325634,\n",
       "  -0.04419507458806038,\n",
       "  -0.008253300562500954,\n",
       "  0.01772509142756462,\n",
       "  0.00516666192561388,\n",
       "  0.0030067504849284887,\n",
       "  0.041809841990470886,\n",
       "  0.02069348469376564,\n",
       "  -0.030965175479650497,\n",
       "  0.019061695784330368,\n",
       "  -0.04591232165694237,\n",
       "  0.013860980048775673,\n",
       "  -0.02009756863117218,\n",
       "  0.01727413572371006,\n",
       "  -0.04215065762400627,\n",
       "  0.009000149555504322,\n",
       "  0.03984234854578972,\n",
       "  0.0129809295758605,\n",
       "  -0.0014195252442732453,\n",
       "  0.001469018985517323,\n",
       "  2.332431043758449e-34,\n",
       "  0.0020807974506169558,\n",
       "  0.01184876635670662,\n",
       "  0.0381302684545517,\n",
       "  0.01134506519883871,\n",
       "  0.02245291694998741,\n",
       "  0.04446176439523697,\n",
       "  -0.00016754459647927433,\n",
       "  0.0012617130996659398,\n",
       "  0.041863854974508286,\n",
       "  0.023478513583540916,\n",
       "  -0.012328780256211758]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cbb58",
   "metadata": {},
   "source": [
    "Get cosine distance between sequential embeddings pairs where the breakpoints are? Add \"distance_to_next\" as another key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a9b5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, sentences = calculate_cosine_distances(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.004361404047567019),\n",
       " np.float64(0.6475389528243547),\n",
       " np.float64(0.4735403441926822)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2a9c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2070)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(distances), len(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d05069",
   "metadata": {},
   "source": [
    "#### Plot the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "072ec7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c248ae",
   "metadata": {},
   "source": [
    "There are many ways to chunk up the text based off these distances, but here we are going to consider any distance above the 85th percentile of distances as a break point. This is the only parameter we'll need to config.\n",
    "\n",
    "I initially used 95 percentile but chunks size was too big be handled by open source qwen model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJGCAYAAACZc8uEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsnQe41ETXx8/efi+9X3rvHekqoKAo9orYELEhIKJYUBEL9t59xa7Y5cWC+imoiK9gQUQURUQRRBAUAeltv+c/epbZ3GQ32U12k93ze57cu0kmk8m0nJw5cyYUDofDJAiCIAiCICRNTvJRCIIgCIIgCEAEK0EQBEEQBJcQwUoQBEEQBMElRLASBEEQBEFwCRGsBEEQBEEQXEIEK0EQBEEQBJcQwUoQBEEQBMElRLASBEEQBEFwCRGsBEEQBEEQXEIEK0EQBEEQBJcQwUrIClK9cpPT+5mFl9WmBEEQkiMd/Wheyu8oCA7YsWMHTZs2jebMmUPff/89ffvtt1S/fn2qWbMmValShZ544gnKycmhPXv2UCgUosLCQiooKKBmzZpRaWkpffHFF7RmzZqoOBFOb2z5+flUUlJCGzZsiBxDnLVq1aJq1arRN998Ezmel5en7ov7/Prrr1HxVqpUiWrUqEErV66krVu3Rt2vqKhIHdPvzceLi4tp3bp1ZeJCenC/Xbt2qftVqFBBHdu5cyfl5uaqe61du5Z2796trilfvrx6bjz/559/HskXPB+O4Xkuu+wyde2KFSvohhtuUGFAuXLlVL6edNJJKt9Gjx6t7rl582YVBulo27Yt7b///ur6Bx98kLp27aryZuPGjeo+yMMzzzyTWrduTRdddJGKB8eQLqT7xx9/VOFatWpFtWvXpkWLFtHy5cuj8hZ5gmf/448/ovICaf/ll19o+/btkfLB8yNPfvvtN9qyZUuZuoO48OzIP9zXWP64H+KpXLmyqmfr16+P1AfcD+XL+YNnwHPqaUWe6vdFfuHaTZs2RY4hr7h8AMq7adOm6l6oJwzyqW7duqqu6vXQCsSL+x9//PH01ltvqTqE+oX8wP1xrlOnTnTwwQdTr169qHv37pE2smzZMvU8b7/9tirvV199VT0r2tktt9yi4kGePfrooyoepHf48OHq+efNm0f9+vVTcUydOpU++OADFR5l3b59e9U2ULY9e/ZUbZTvc+KJJ9Jjjz2m0jdo0CCqXr26igf3QJ39+eef6Y477lD5ifrdsGFDdc/GjRvTggULTO8LcJ8OHTqo46tXr1bHkJbevXvTUUcdpdIP8Hwoh59++kml76WXXqLzzz+fHnnkEVUvL7nkEho4cCC9/PLLNHv2bFUW8+fPV2lHWbVo0ULd68svv1R9CtKI+telSxfVdpE2pBnpnz59utqQ16g/KFs8B9rYnXfeSUuXLqXx48erNDNIJ8KjHnJfwfUF7Q7P9H//93+q3qIcEb5OnTrUrl07lT9oR1yfuX6g3AcPHhxJD84jD5Ae1If99tuPHnroIdUewddff63qHupwy5YtVft+//33I+lE/UY6cC9uQ6h3hx56qLrmxRdfjLSv3NxcteE8non7V6Qdx9HX4LnOPfdcuuqqq1S+4pm4L0Y6kQe455FHHqna6F9//aX6u7///lvVz88++yzSl+K5ES/i19ubEYRr0KCB6vvQdrh+mPH777/Tf/7zH7r66qvJMViEWRD8yJIlS8JNmjQJFxYWhvPz89GCZJNNNhtbKBRS/4uLi9X/6tWrhwsKCsI5OTmRffxv3LixOm68jre8vLxwtWrV1P/c3NxwlSpVVBwcT6x7Y6tbt67637BhQ/Uf90Jbvuuuu9T+0KFDw7169Yq6pqioKNysWbNwSUlJ+MYbb7S8b/ny5SO/69evHz7xxBPV1rdv30gc//vf/8LdunWLih/9Cf5XrVpVPRs/J/4feeSRZZ4n1rPKFtytSZMm6h1jxVdffaXKPhFC+ONcHBME78EXIr548NX2zjvvpDs5ghAIoGWFFqhNmzb01FNPUceOHem7775T7ahevXpRGlgzDa4ONJTQgEF7A20CNCQICw0TNCZ//vmn0mxAmwBNDjQAFStWVFochIe2UdfMxboXOOuss+jpp59WWg5ohKDlGDdunNLyYFuyZAmNGTNGxQ9NW9WqVZWWCNq0CRMmqHPQvCDNV155pdJwQ8uCe0LrAY3H4sWL1b2gTYOGDdpbaEkQD2v+oP1gbTE0INC04TyeD+lC+pAG/Be8oXLlyqp8oWV0C5QjNtQDaFShvXr44YdNw2KEZMiQITE1YJYkrk8QBG+Rr0XZZEt807U0/NuokXrllVfCq1evjuzXrl07XKtWLfW1jrD4qgfQAEFzxNodnIPmxxifcZ+3rl27qnMA/3VN1nPPPafuieO///57+LXXXgs3aNAgfPLJJ0e0XLjnf/7zH6Xt+vrrr8ObNm2KpB+aq2eeeSbcsmXLSPzoN8zSoh979NFHI8/+3nvvqf9IF8JAo4dnnjdvnro3a7SwjR8/Pu1lKxsltKFsUY+NWlrjxvUnUY2VGK8LvgVft6NGjVJfk/jaxj7Al6QOvjwFQfjHhoTBtwm3GWhvwAsvvEAffvghHXPMMWofNndsJ4awsG2DTRXsdgDs1wDaHzQ12K644goVN9u7sS0btFNTpkxRx2Bvc9NNN0XSA5seXPPmm2+qfWgMcB1symAjhXsy0LDh/LHHHquuR9oRFvY4sG2ClozjhSYO7R82VWwLBC3H5MmT1QZgj4U+BLZT0LS9/vrr6vh5551H99xzT0TLBxA3gGYOzwwbQz7HwE4KQKuVKLC3EqJBfqIeMdAOogyMGPt/J7C2lDWNqLOoE8YNWjKuq4neSBB8Cb6W8dWIr+iaNWtafg3Llv7NSrMoZZaeDTZHvKFs0H4qVaoUnj17tmpb0EghHL7e2eaIbbLuu+8+pbFB2cHO6eqrr1aaIsQFzRPaY//+/ZV9EsKUK1dOXbfvvvuG586dq+K84YYbwo0aNQofd9xxKt527dqpsG3atInch+vHqaeequ6J/bFjx4YnTZqkzleuXDncuXPncI8ePdR9a9SooeLs3r17+PTTT1fPA01Shw4dlMYK56GBgs3WoYceqmyo8NzQarVq1So8c+bMqGcfMGBA5NkRv15fL7vsMvXMFSpUiNhh4Rz6I74mqNr0oKR73LhxEa2lmxvs9VCP8PuAAw6wfP+w1jYRRLASfMvUqVPDgwcPVh13uhu5bLIFceOhDXyknHXWWeHJkyertgUBBMNwvXv3jpoYAkGCr+FjGB67/vrrwwcddJAStCD08HCg8YXNcUG4GjFiRPiXX35RQ2cQjnB+y5Yt6h78cm/durVKm3HYEkM1d999t0qn8b58LcJB6OGPLh66wTkIXbfcckv40ksvDR988MHha665Jvz888+rZ1+3bl34ySefDO/cuTN8xBFHqPA81Id74b+xzzHmSaKbPqQoG9muw8bJCm4IV3369FECuRVcTxJBjNcF3wMDUhih8pAFjFExfZmnXWNa9rZt25QqGdO4YaCLoQM2Nv3f//6nDGsxNRqGqZiWvXDhQqXub9KkiVIHwygWQwT77LOPmjYO9T/+Y2jj008/VdOWcR0MKjFsMXfuXBUnhhMwbIHwiAvGr4gbBrKIH1OD0cQwpRtTxjEFHAazgKc1YwgD94CxPoYI8Ay4F6YgDxgwQKmtESfiggEvzmMoBNPskW64OPjkk0/U/ZFOxIfp1YgP90XakDcwKsZUd0wfnzRpksrT/v37q/hh7Ithnj59+ihj3ieffFKFx7W4L45369ZNhYHx83PPPaeGTWDAjOnayGs8J9wsIF2vvfaaMgxFOaEMkJcYxkHcmJbO09kxZNSoUSP1TBiugRoeU+NhGIzhqh9++EHFg2sw/Rn5ijxDHUAcyFPkAVT3yDsYMWM6/apVq1T+I99QXjCuxn1QbogbwwlwNYBJEXh+5CeGpXAN8g9lg2Eg5BfiQL5jKjeGo1Af4f4DeYNrkIcYMkN43Bfxw+gbw2m4J/IHcWC4CuVw+OGH0wMPPKDKCPmC50FcyMOZM2eqOGEQjmdDnsDYFs//8ccfqzxAuaAskE/4j3yEiwtcB/cKSAvqOcoGeYR6gfLF9HYzUL733Xefqitjx45VeYv8RP3DsBzSrYN0ok1hKBBuKWDci+fG9Wh7uB7tCOnU74H4+vbtq8p41qxZKn/gdgBGxGhDiBO/kb9IM+Kyui/ugWEi5C3KEnHq7hZQ77jvwHV6Wox9C4zrkZeoG2hLaJ+oy8hPlDlPoEHdxb1RpnD9gTaLc0g76g0b8XPbgrsL1EW4DEBdbt68ubon2gGGKVGHMNyJvgvpRR6hT0O54RkwLIu0oM/R2zryGs/M7lHQB6I94hjSgP+oZ4gb5Y7h3q+++krVBdQB5HHnzp3VfVEH8axoO+gzUXdxf57wgGdE/uBZ2A0I7n/CCSeo8+hTR4wYoeKH6wrkc+vWrVVcyDuEQbox3Iu04JkwLAwXDcgP3BsuLWbMmKHaGvoK9MfIR8D9sF5W6A/gJgLtA5Md2I0D8hn9Be532mmnqXqJNgn3EEgnu0vR64dXiGDlA9ApoAGi8ep2ETpWxxMJZzcuJ2HxDGjoAA2br4t1vfEaq/vZyR+3wT3RkNGZoqPBywqNF509ZlyZjfOjo+LjuJZnFeF/LLuAfzXH6jd3SHjRp+I57WLWTegzvLhc/JTmRNHLg/eTsadJJ8b2ou8b26hZmzVrb7HiEDIDvV3zf7/1SX5GBKsUAEeI999/v/rqwAsaL1t2nIbfRiCh40sVkjb+wxgTDh91UMEhbMAJHL6O8ZWELwK83NnpGRvoISykdHyVIE6zKcL40sDXEAxToYm47rrrlEM6fMHgqwBfAfjixD3sVhl89ePrzAjSg+c3c+oIkE581ePr1OiEMxZo+Pi6hlM3nmp+wQUXqC8cfOkgb4yOO5FGPT9YGNIxHos3ZdwqbexQ0+q5kwHx4+vu9NNPV1+o0Eqdeuqp6guOHafy9HU8D75I8QWLr0/WBPCUcv4CRJyoNyhDfJXzfdhwGNPaEQ++DNkBJ9KAL04WJJG32HAM4VB/oAkwq/eAhTOkBWHwxZkoiAdtCV+n7IwUx4z3tkqLMa54Zc6OEZEXukPSZNKPfEM7h4E2NFNIK+os7oNy0aeCQ7uBZ4XmBvkGp5a4BuUELQJA3wBNEfb5N8rQ7L8ehq+PFQc0MOinYPSLdoZ+Cx9OqGcIh3ThWeDwEZo5aLj0fZQR6iM0KtBYGcNh6js+ruDcE2GhFcJxaF9wDv0rps4jPO4LNw3IAxjroy9AvUO9gtYJ7QPlibxEf4P6ijzDdbg3u3Zgh5MsWKMu4zf+o3xwHfoyaGSgdb3wwgtVfsB5JTQw0NDcddddSgvKbcqNuoE0o++/99571XNBU42049nQ1+EZUFfwvkGbg2YRG65D/iAMzuMZkG5osnAt0o56xU6Ija4GjAJ4PHJzc1Ufgr4X90GaoSXj/sSqzzXeE3Ub5cMTJuyCPg/vEmjE8C5BO4IGG+WLegPtGeou6ipGB3SHobgfnNaKg1Af8tZbbyl7gXr16qV9rNruhvTCbiHd6Uh0g52FF0aPQdlgiwIbgnSnQzZ/bLBpQntmg3EYX+u/8Z9dKejOM7kPgB0UfvPG5/maFi1aRMVhZSgNI3PYPrG9DAzB2d5Kt2mCHVLFihUjDkFhwI7/qNPoR3ENT5fHcRi54xjiu+CCC5RN2LHHHqvugWsQFsb4Bx54YMTGCdd16tQpss9OSHEv7MNQH/9hEM/pdZLnMJbHM5nlidub2KBSUhvqGtoDHMp+8803kXc37PsSdbcggpXHoDMZOHBg1Mwbv83oMDZMdAY8a8KvG9KIzhQdIzqxbJt9hplZ7D3br1sqXiqyWee73lc4LQs9PNqZ8Zj+W+8/OGyXLl3C++23nxJ2sA8j+SFDhqjJKNg/7LDDVJs96qijlLDFggxmYmG2HvxYsRAG31YICyEHdR7e1REf4sZMxOOPP17dFx7WAV6Sy5YtC0+ZMkVdf9NNN6ljbdu2DT/yyCORuAD3czCUx3GE4ZmSMMTX94cPH67CYsahk7x0w/Ad/RwLw5mwtfl3dqjZc/Jv1F/jqhsskLuZlt27d4c3btyoJltgMsSXX36p6oYIVj4GlSCVL32v7+WWgJjs5oclbjJZmLPTeZk9P09N5y/BdD9HNm4QSPACgtNN7GNGHZauwaxA7MMFAvZRftAA4eVx4YUXqnOPPfaYerl9++23SiDhY/itx4FjXP4fffRRpOzfffdd9fvFF19Uy8mwcPfmm28q4YlnANapU0f9x8xELG8D4YUdhOI67POyNyxY4XkgECEO3Af3QzoQL7RNOA9w/Isvvgj/+OOP6no4IMUxhOEp9Og/OCzCTJs2Tf1HGBYOoQnT99944w31HzPJrOq/bMltdvp1N99BRncKqOco888++ywpwUochHoMZpDwLDCMr2M2hT52jLFdHTPjQB7fZ1uZWPD1uqNAq7h5H/HjN8ajGdgpmYFlI9zAqRGkMTw7BUXajXnI6M7mvCDdhpyx7p9s2qxmUemY2VjoNhlWtmTpzrdMB7PLYCMDu0sA+xbMbv3oo4/UPmazwhYR5YfZXrD5gc0R+ozrr79elSGuv/zyy1X7wjHYCulxwAZTn3ABcB1sngDP2mR7IhyHXQ3SojvixD7sIbneIC5ch30c1+NHXLD/wzWIDzPL3n33XRUvnocX7sZxzBqDLRb6gFtvvVUdw+y6V155RdVt7lN51h7yCuexwa4V12Giir7PjkF5CRSz+m/W72YrNbT3iV3M+gbjMav+3g1Q52FnjNm1mGmdMC4oZYQYvPTSS1GqYHw9Jipd63YFdq+xExbqdGgXmjZtmlFDOKxW9ouWLZUbD6Mkuk2fPj2jfO5kk3YB2kb0Fbz0CoaQYJLw6aefqnyALQkvNgwHifp/+HpCGNhNLVy4UGkQ8PUOh5zGOIz9Eo5Dq4XfGMpr3rx5JN+hZUJc0KThmG7rhGFt7iPbt28fue66665TYXCM40ffhDjg9HPlypXh0tJSdX8MD+I8hh9hX8nL7RxyyCHqP64xakOM/RzajNkwqt36hQ0OThEPhg31ZYC83JK9B54V+YM8S+b9FC9/KIHr9KVnvEqXGfCBhvxIVGOVuG94wRbQUGEGCWZqwQeNvgyE1awKfCHx1x8v0wB49hp/BSIcZlPgCw8znswWi7QzAwUzwnjmIHwfHXLIIcqPEd8bWjfMIjGb4ecF+CLBbCjMEOIvXLvgCxOz7+DvCLOQMPMDPlF4Nua+++6rZsfgSxd+TTAjBV+ZnLeYNYPnjLW4KtKGmYaYXaPPMrQC8WPWDmbEwNcLXDkgv3GfWLNceHqzk0VAocmDDyGUF54FG+oA3xPPi/RjJhGDmVbGfIbPJMwUgh8aJwvNYlYgtApIM2bYQDPAs4vszIbkWYE8uy6hBVBNgNYYWhlengTlhlk/dmd32pnNZTaLyg1wb2hZUHbIG8z8g9bJmB6UI/oEzA7D895+++3qOLQ16Icwy46XhsE5/MaMQf0/lpDC/7PPPlvNksLzoA5hVh2Ww9HjuOaaa1QecjrglwizrOBzC9oe+MJC28P16FuQtuOPP175LmOfcwD9IsoF2jO4MkHbhBYJM9TgH+q4445T5zGTC3kxbNgwVc/hBgXxYeYgNpzDbDy4SDniiCPU/XhhaNQ9bk9c5ng2o2sYpBXPZKccER9fi7RgH3658FyYIXnVVVepvHnwwQcjPrbcBHmIfEb7RF+CPEN6sA+tJc9gjDfrFX0m6gz6TWj93njjDcu6jrx00h+EQiE1MxMz8lA/0R+jjNinVDz0me2JODBA+WLmJPp9+JJDfmHWKfq/m2++WS1zZMall16q7me1QHM8xN1CmsFUWKyiDbUpKi0apXEIC5USDRUvZlZhW4WD0zRUIlRidD5ocKwixzXoxBAXHD5CgNHPGeNDB8VOKtHh4OWLRmpcmw/PgE4P01jRoM3uBWHG7Bo4+Is1vGk3nJvgnmiEEBAgkKEhQnBFI8X0fQgu+rTcVIGXA6aYswNNgBcuygOdF4Q9L4Y/cT+43tCFQLzo0EmiY8JLA3UF9RMvOq/WQUP9Y5ciPPUaZYK6hjqCFzHSirKBC5IePXqol6sTcP3zzz+vHB6yc0l0kVhTTl/Pzkma0cbYnxnSjXqMMkNbRnuBEIw8RJkiHNouyhPTwJ2m36w/4Par70PgwW+4TIDwbPyvh+HrjXEa49DDCfHrBfISggP6F71vRfmjjqDPgVsJ/ghCW8c5CKn46EmmbsQCbRntHXUS5Yp+EB8i+Dg85ZRTPC3j9evXqw1tGnmh5wuO453C7YffcX5EBCuH8MvF6iUPnyno0OHbhMOjAUH7hE4UnSxeSmgcqQZfo+z52CioQPjhNNsFXx14XlR8fAnw1wwaJF7y6HDN8gm+nHgRVlQ/fF3D5gLejPH1BHsJCIQQOJFnEAAheKIDSmTcXhD8DtqR7kfIqFGGEIsXKX6jPUCzowvREPjR1tCOYR8JP0YHHXSQ8pyt90d6/8S/OYzxvx5G7xtixcHH8CxIS6y4rPZjpdmYBmNYq/B20x0rPXbyifMefRfKkctF72ONZY0w8O2EY4cddpgSoHCO++hE+uZsY/v27UoIxAcWr67BGkm8mzCqgw8XvHfZl5XZu4nf17gmqQ/5pAyIsgTMQMEUW54dgg3j5zh2zjnnKHsBzG7B7BSMd8O+ANNJY40PY9owFiqF/QCu51lYGPPl+8AmAGPe8BmDMPiPacyYGWO0AWCfL8ZjuB73YtsHDoP/vO4Wzw7CtGmkGTYZsHuAPxg8F+wYMEMGdliIy4ntjdtj5Lj3Mccco9Ygwywd7C9atEj510HeI48QDs+L4/wfz4KNz/OGY8YFno1hES/Hj//IM+NvDsvx4Zj+25g+vo7vxWGN8eE3T2fnNLLvH1zD9hX8H2WHPMdsJ8SB36hbiIPdU2CDTQVs6mArg/+8zhqHxX/UP/YnhFlcsBtBfeJZVbw2HGbRYKo73xO2CbgH7GqQxpYtW6pn43vjN+4BWxTUZU47Px9PLTfajuBe2Nj2wUvbi6BtnLeo67yP38gv/Gf0ff5t9d8Y3k4cfMzs/rHSYhVXvDTES5/TdMdKj5184rw3lgvnh54/ehj+ra93qK+JmEzd4BmXZufQltCnow27YYcY615ksqGdsw0sruP06PZwOI53at++fdVi2ejTk7X/RJxYK/DDDz9UZbJ169bwmWeeqdLP78Zzzz03vG3btnAiiGAVByzCiExGobIQwsKCLjTE8ynE03rhbO+SSy7x3Cgv2UqX7jTEa7zoCOB7Bvu8Wr2+oaxwnP9b+ZKxOpZoHhgXk7UT3q0FXmWTzWpzWr/0OsmG4PqxROIzxqXfxxgv/uOlaxaO/TkZw2JjB5/sBNUsHrM8SfS5ZAvWFvq3fPnD8JRTTlHv+XHjximfZlOnTg1/9913yv0GJnDgXZ0IIljFAZoazJiBBoe1VOzzBRtrE+BczFiAaNxB8mDOM8nw36jZkU022YK16Ro/Y3tmDblxxixry9nPFH5DG4nfiIOP4Txfo3tv12ej6hp+XgmA4+J9hMcLDZpx3fcZ4u/Xr1+UwIP7Iyw+cnVhjMNimzhxojoGDQdfD62sWbr133p6zPInU2ZKc16mOw1kY4NgA2/5bsbJnvpZG4g6AHCvt99+O+rdP2vWLFWPEkGcbsSBDeZgQAj7IRhswrAVNg+A1zI7//zzI9foC5SycXgy8LprdnykxFrsNxbsIwv/YTuFWYx83Cniy0WIhdf+xbKZESNGRNogZqqh/T799NPKjvGMM85Q544++mhlcwJgt8hhANaXhFH9e++9F5kxhdlis2bNUnEMHTpUHTvqqKMiPq3YFgX3GzNmTKT9c1zPPPOMsgvDfR5//HEVL/sigj0R1rfEDEKExYxVhIPx9AcffBBJA+6H+yPs+++/r8JymnEOswIRHjMVcQ59L2yTkG78Nks3fuvp4/Qgn/T84WfT8wmTI4C+OLedeu2Hxbz9alYdMrxrsO4iZqu6CWY7wgYL/3E/ngSE9zjPUmVgz8v+1pwib8A4oNOAuwE0PCzgiY4BDRiz5dDYuEE+9thjUdfBQB0Fh4aJzsIMnLczu4wbo51GiTQiXkwldQqnBQ3PmC4nAhsM0L1G78QwvTtWGD90Zm4S9JlXqXLbkY1gFhnz8ssvq/9wrQDHmOxSAx+HaKPoJ9gRLMJAIIKLFRj9YkYaZhej7eDYJZdcEhUH7oNjuAYuFrivRL/Iiw1zXHhhYfYgwKwyGBBjn++PazBhRU8zwsEQmdMAo2KkmcOij+rSpUskfTiG8ADxIh4sQB4r3cb0cXrwMaznj1k+8Ww0DgesFpxncJ0X7jiCTEjLH6PAh/xKZhF2M1joxrsBk0FQZ6E8Oe2005TTW8w6BpgNDCEd9S8RRLCKw3777af8qkDKRYHAC/Ett9xCF110kWqQ7Mfo7rvvjlyDcDiHzgCzEzBd1einB0B4sRK6zGYinnfeeZZhIMihosD/DKbiokJCGLLrFgBpxmwIrtxjx46NHAfxfKHo8FeAl7DPHsAemo1wJ4awmUSmCozZBj6C3Gbu3LnqP4Sbt99+W/UBJ554ovJldcABB6g6A5cd6CPQtjE7Cv/x8YhrICQhDF408LmGlxuECfitMsbRsmVLdQ0LP+jroA1C/2aMC76rOC3oU3l/yJAhytcU4kJ71dOMcHoaRo4cGRUWntX19HF43B95C7cZsdJtlj5OjzF/rPIJ94KbEStNPa7BcWzorxIdUchUwjG0Z9C+2vET6ATUG4w84X0GYRmuczCLE4I1tLRwCwRXJ1CIoC3dd999Cd1HSjkOzz77rHIEeccddyiniZjujELAVxca7GuvvabcBKCRT548WX21wNEd1OcQmnAtXAagQeELCw0M0jAKGC4HDjzwQKWaR4d011130X//+1+lFcNUXpxH4cN1AeI988wzlTocqmuoKBEPT8NF3HCmh/jRmSCtmMaLadlwr4DOIhboyPDV9+abb6o44b8J8UMljt/oRPAM7NsJldPKiRx3HnCWB40evh75mXENviydOP7EM6Mjg2NEPDcEpf79+6sGgbzBVwd8rwB8paKDx/0gVCJ/8QWK4xD44AgP/oH0sHBsCGEFeYz/GPbFPfXfKEM0clyL+BAWLjT4N1+PcIgTgjWu4yU+AMLq8fG99bjRoPE8EKbxH47qMD0b8eN6hMdz4CUCB6p4USAv2YElrkE60InjP39FsxaBnU2ijFGW6GBQLiyEIhzKukOHDrRkyZKIHxm8aPA1h7oO+EUBcE+UA8Kjo2RVO8oe98Ezs8NQwH6PcAzn2Vkgu9+ANgDlZ1W/dGeB+I36wPmP9Bo7a4SBlk/XFvAz8/PiRax/HeMcrmMhlsPrX9i61pTzmcsB5/jFi/Tw1G/kJc5jmAEb0ov26dQRrh1effVVdb9jjjlGtW9ogXBvDGdNnDhRhcEQnP4f1+hh+IMKTjlR58zi4GvQB8KEAD7grOLS02Lct0qzWRr0sMZz+kegnXRbpc8sf6zyieuDmcNfHnYy5o9d0Jeg/qJNQNvI9Zs/rFC38BttiR1qsnYM9VLX0KANoq5DI4nnZWe8qJs4j7aL6/AcuI7rMJ5Bb4+s+cF9cC2/D7iPycnJUe1STwN+4zibnOAdincc7ol+kp0IIy70kRjKRV8AxQY++nEf9F3o9wGeBX0h3PfYGd5kcxe8JzE0jLqBfH3nnXeUU1R2Soy04GPh5JNPTnh0QPxYuQgKBS9JVh/7CVRW+FZBZcYLRE8jKjvbNgiCkFp4DTy8WHTtNsDLDzae6OwRDsI0Xox6e2VhGS8GvAjgcBTncQzCMLd1vX9Ct4/feHEhjPG/HkbvK2LFwcfwDKw1sorL2Ffyvh6XWTg9DcawxvTp/ZmddMdKj518gmDCKx7gY4HLBfFxH8v5w2XNHygQHNiTOuC06HZnQnAQjZUDPv/8c2W0Dk0DGh4aARoGGgUvaYAGgQ4FDQzHoN0xfnmjc2TJGF8v+LKHZghf/QjPcSFe7kyhokQHi/84h3tzWGjQ8PXLqmj93FtvvaWGyvBlgGvxZYzGyl/nCIvr+bk4zfp/Y1r0hVX5nrGeCfHjBYFz+jPwdfo+px35x/HhHMJBbYs04hyuwX2gSYF6Xk8zpw9O9pAOCJRwjIovHMTD5QcNEWvXOCzSzGnEf06D/pvvi2v19BvLifMB5/S8ipVfetx6OeL//Pnz1TkeyuEy5XublYsOlzV/0SMc1zlcizzg/EQ4HIfWUx++4DRxXUBeGsOYheXf/AxWaYd2C1/oep7pZWOsbwiP/9D0oZyhxjdLC8Jhw5AQtJ56m9TvaTwGLaHVuXiTNDhtqGf8vMY6pR/TgS0QPws0gbgeH0X44sZHEvY5f7hc0Qfw0jd4GfPL3Kx/MrZxva1zGL4exIqDj0FQQFrQt1jFpcej7+txmYXT02AMa0yfbl5hJ91m6bPKH7NrOe9R/7hsWZsJzQv3nxC29HaPMkZbRP3A9TgHYQ3h0UdjtENv03rd0fsbq76U4fajtymzdwS3G71tcT3Hb7Rz1LtY/Yx+bft/wyNN6Gf4Ptxfcx/C/Tq/y/hdgWuM71e8g/m8VV8A0B/ofTk/s1Wb1e/hCsk7JMgeME31k08+UVNz4Ydq3rx54eeee065XIDvC8C/Eebee+9V/+FUEdN48Z99sOhTevl37969VbxwHLpp06aIE0Xcl8PC/wbiwEKjiO+2226LnEdYvh7X4hxPGeYFTHmBVvb7wmGuuOIKldahQ4eq6aicdvw3poXTjLQgLt0nl9kz8b7u+JQdWXI43ue063FwOPgU4XNwcIn/8DGGa/AfU631/DZOszUuwoprkCf6VGo9n62eia/lsmSfOVbXY8M9OL9i1QE9buQdlzFPB+fn1x2D6nEYy4XLWS9rPRwWr9XThGfRw7HTWL4PzuGY7hsI1+hh9PtxWLQJTrv+DMa0m+WZnj/IG2PeOtnSsbA03zNePXO6cTmjDDlO5DPKg/sQ9E+oO3Coa9U/mbV17s9wPY7FikPvHxCG+x6zuPR49H09LrNwehqMYY3p42PATrrN0meVP2bXcv/L7YH7LO7PHn/88TL9p17v2aGu3pcY/Rwa24FZ32nc53Zo1qZQZ7hv4fsifbHah/GcWT9jVe8n/usGA/dBnrFTbOTHNddco/IH7zTdCTLeRXDaiXRyGXHcOG9895j1H8Z35kMPPaTKg+sHY1V/EkUEqzjAvwU8Tg8cOFAV0vnnnx8REOC7il8SWEn++OOPV7/hP4U90CbiIVr382InrLEiOdl0L7+67xZURHaiZjctTtKa7Kbnq1seimVzt8z0jlgXdKWcvNumTJkS6QtuueUW1T+hHd9xxx1R/RN7tucXMb8kZ8yYocLMnj1b9Xs4j2M4p8eB++AY2h4+eLjfgCdr/NbjOuyww8JPPPGESgv8VJ100klqn1+uuAYfc9jnNCPc8uXLI2mA/0Ckk8MiDIQDTh+OITzAsyCexYsXW6bbLH2cnkGDBkXlj1k+YaUK/QPPi/aTii1V6Qtp9zF+9NoR4lDeWMUh2XRASEOZLV26VNUtlDvqiV5/8H7H+WQQwSoOBx98sFp2w6yQdM2K7oZfltuQTTbZ0rHpDjWxygP/rlWrlq3+ibUOhxxyiBJcIFRAyDHGob/49Pj0jzuOq3nz5uGOHTtGwrZt2zayjw1xssNPTjPCHXfccZE0sKNODotjEIg4fRweghPCwIkoXpax0m2Wvnj54ySMbOR6vU5242Xifvrpp/CNN96oBHZ2GMr1B+91nE8GcbcQBziqw9gtz/rhqbVAn0XEho4g04wNxaGjIAQDnouE/7D5YWA/AjcD8fontr+DY85PPvlE2azAtgt9QM+ePZWtou4vD+iL1eqLBfMMNdjT6I4WYevIM+Vg0wKbO549ybNO8Rsz8DB7FL/hqw7XYSY27I9g/A1/goiH3Rgg7U8++aQ6B5udwYMHq/QMHz7cNN2Mnj6kBzNi49na8Iw8WRw5NYTDYVfyGu9xzDzEAtdwKgtXGyh71Au0D9QfzFJEPYO7B7jsSAQRrOKAqbqTJk2K+HU6/PDDI8aR/fr1izRATOFkwzkYI+ozdoIIp1/3uyUIiRLUdhA0dN9mPMOM/S+xWwt90gaEIv6Na+GviQUxTGtH24fPPggrMKaGKxkwbty4SJmeddZZ6j/CsodzCGKDBg1SvyEQYdIAT8HHPk/xRzhMbedp+1jBAscQF67BByvCsx8pfOTiAxbC1IABA1Q8cNKJ6/GCxNR8HDviiCOUbz+8JPHCNKYb8cO9jZ4+TjfnIdLDrgGw6fnEv8XZberY5qJLEtQlGMrfeuutqo6hfOGmCPUHdevKK69UbQCujxK9gRCD9u3blzHKMxsKhB2SqIdlk002vwwFYl093VTBTv/EQ2VDhgwJH3XUUeoaNhDW47CKSz/OcWGoDRufb9OmTWSfj/EC95xm7J9zzjmRNOj3xYa4BwwYEEkfh4fdFIZ72rVrFzfdZumTLbO3goKCyFDfyJEjw126dClTf2CDJUOBHoO1iqA6xLRNAAeNZkOBUHuzJ2VefytTsOu9XRAE/wwFwgkkgIYGU9eN6wOawVooOE3EUCDAdHhobnr06KFcQABMw+c49CEaM7MBuKzAsiGs+YHWCv0lO6fEtPx/7X2VRoqBU2Osywpwb6ThoIMOUvsY9kP6cA07+IUblgsuuEBp16DFgnYKPqXggNks3ZxWPX1Ijz7iEK9PtLNyhpA87AA1WVDv8N6GQ238h+NvOBmFFhQjTqg/eN9jTcprr71WOWdOBBGsbMKd0VNPPRVZCw+qaO6IoG7mxnjOOedE1NlcGRIVtJxc56Ywx3FB5c3PGyRhMUhpzQZ0OxwZFkwN8I0EIHzA2z3bgBr7J7RxHurito4yOu6449Tv448/XsWxePFiZfNkjAOmEgC/EZYFj27dukXFxWujIi0YboHQg32YUeAcjmHJMMTLacYxbJwGrNoAX10cFmmGryROH3y84Rw8qOOFiaHACRMmxEy3MX16eoz5Y8ynTFqiJlV9Zk4C7Z+9wbtl9wufa9OmTVNDz6iHSBPqiV5/YKeH8wnh2phZFgD18o8//qhcLiDrXnvtNaU2hEoZPq0A1ND4jTBQOZ566qnq/AknnKD+Y+MZMPoMGl1Fjam9mO6JuHBdq1atItcZfRhhhgNcQvA9+Hrssz8nhGvatGnUfXCMwyBeDHcirTgH9TunHf+NaeH06yrWeM/E8fJvDsfXsFqe087DAvosHt3fC4YB9Pzi83p+c/x21cTG9PO1Zq4d9GfgoQn+He+e7IojVn4Zy5jLBrNYjPHp1xnvzXmrl7VxmIjDYXYV34+P6/fDfXCO4+M6ZXwG/X78G22CZ2YhTrO8tZNvxvqWriGFZDcnz+2k7iKf9T4E/RPqzp9//mnZP5m1de7PeGo6wlvFofcPCMMzr8zi0tOi7+txmYXT02AMa0wfHwN20m2WPqv8McsnvBf09mDsyxGe+yTuP63avd6XcB031o9Y4fW+VP9tFg+3dX5HcJ+ZSJ3ke1m1y8J/Z3UivVxH8Rt5xv0EjqFfwvXGdwWXEa5BmFNOOcVRX8D5jfccl5+OVf1JlMwRt1MAr/H0wAMPqP+YJfOf//xHSdI84wazCvAbYWCUedRRR6kZBvgPlbRxLSks9AgDSHzdYRV2GMxhzT54i0VcuA4S9XPPPaeuRXgYYkJFjv9Qq2NtK6xZiLD4SsUXFy88rJ/D/Q877DD19cbX6iAsvgax9tWMGTMiaY+VFjbeRPo5PqtngnoVa9/hKxLn9Gfg6/R9TvusWbMi8eEcwuFrFWnEOVyD+8BgFYtQm+U31nxEOjC8gDWikE+IBzOn8DWEL2JW63NYpJnTiP+cBv033xfX6uk3lhPnA7609LyKlV963LyQLp4LefLYY4+pc1jjCv/1MrUqF2NZc1lwOJQP4kaeIQ84PxEO63HhK04f+uDy4bqAvDSGYXjGDcLydfwMVmmHdgJDWHqe6WVjrG8Ij7LEmpIoZ8RpNnyAcPgaxWwx/NYNkPV7Go9haM3qXLwvaE4b6hk/r7FO6cd05syZE3kW/Ia2BGuA6vfk/EE+4ysbfQDaLfonbrfx+ie9rXN/hutxjPs8sziM/QOXs1lcelr0fT0us3B6GoxhjenjYxw2XrrN0meVP2b5hPcC+kScQ1tB3utlq/dnZv0nt3sc437I2Bca27QxfrPwZn08tx+9TenvCKxFiHcEt2G9bXE9x2+0c9TjWP3MXO1aDo80Ic8w2oP7oH/R+xDu1zkv+V3BZYR1crmMkEY+b9UXAPQHel+OsFw/GKv6kyiyVqAgCIIgCIJLiLGDIAiCIAiCS4hgJQiCIAiC4BIiWAmCIAiCILiECFaCIAiCIAguIbMCbTJ69Gi67777ImsHDRkyRP3H/p133kmff/65Wu4GvlfgfAznsX/KKaeocFOmTFG/4UIfy+T8/PPPaqZVlSpVlOM+XPfqq6+qZRiwVM4jjzyifmPmA2ZH4Dr8B19++aWaYQGnpXCUB58cbdq0UefefPNNdV/8xwy/Pn36KMd5uAfuz+k477zzlE8X+OXCMhSID2ng/3gG/bn0tH/88ceRtCA+xI8NfnLgbA/LT/AzIX7QvXt3ev/999W6Y0gr4hszZoy6BvuLFi1SvmRwf44P1wD8nj9/vkrHZ599FnHmhtk3yAMsuYE8Qf4hLqxzhnzFjLMXXniB7r333kg6MesFa4NhuQ/sw+EgygFO4TAzBTNKDjzwQHUOYL0y+GDC8yCfcV/2w4KywTH+jbBYBgRhP/roo8jyIFw+eEb8Rz7gGfg5uA4gHRw38gJ1AI4NMUMFSy0cfPDBKsxNN90UqQ94LsSHWTPw44O0830AfgPUJV4HDnD5IL9RFljOAc4TORz8tvGsM+Tn/vvvr+oUng9Lh+A/8hJlALhu8T2Rn127dlX7fB+kHeExSwf1H+A36hPSjnviOqQNG+JBfcIsHpQN0sZ5DeBzBueQz7/++mukTMGmTZvUf66TKMPatWtHjjNwDIkygJNKvhZLZyBenGN/RXp4HEM87BeK0Y/x8hvIUz7O99brGeoznpXvjd+YXYm1zHAMy22gHHjWE3wzVa1aVcWH86g7SD/2kT8oG/Y/ddttt6lywj5+c7+APov7J95H+9HDXHLJJWof/7k/039zHHyM+yX85rg4Xk4Lxw2McfE13KfyPqfDmGYdY5oZPR6+3phu/m3MH34WPX36s3HYzp07R/pn9I2YfcZ9LPpP9OXol/R6izLm2XTcxrhf4/4L53kmHPoP/Vq0C65T3K9gn/tLvf1z38ZtSm+PmDmLc6ijHJ7bI9oi0oE0cDtHGrmeox8HqG/oGzgNoKSkRIXn9w7uh5l2uA/3CwiPZ0K/jv94F6HN4N64Bv06v3uQz5j5h3UkcU5/9yC9gN8zaBvcn3KeIo08O7h+/fqqTOD4E/8Bl7MZxjplm6ScNWQRyKprr71W+fyAH4y33npLHWefRBdeeGFkaRuEgR8OHGcfJvit+2Ry4teGfXCwjyf2CWLmy4bv26RJkzLndV9Y+j05XRwvnkF/LrO0c1rs+uoxptPsnNVzYevZs6dlXLwkQSwfP058svh9032RJRMP8tuqLMzy0KxO8blYdSFWfU13XmbihvwuLS0Nd+7cOdKeuR2j39L7J+yj/ehhcA7XYx//OTz/1uPgYyh/xMFxAfzmtOj31+PV78/o6UE4YEwzY0yzjh6vVbr1ZzXmhzF9ZvlkbIfcbxr7Vj9tqW6POTH6F14uDvlqTAvns7Hf0X0dJpIeLhP4z9LroRG9/jmWFxK6KguZOHGiKowRI0aoAoETMRQ4v1RQ2CigChUqhHv16qWOX3zxxVGOIxOpvGaOFGNVYITH2lh272VcQwuCoN4pVKtWLdyvXz9PG168xh3rvvo6YrLZK2t947qib40bN066zIwvFbMXjJSb+xu/cOrXr6/20R/BkSXa8UMPPaT6KL1/grCgh0Fbx7mDDjooXKdOnfB3330XPvzww9WxZ599Nvzmm29GxYFjOM/3RlzYjj322IjgznHh/rgf7oN4ERffH9vs2bPDo0aNCletWjWSHoQDnAakGffnsIiX04y0MBwPx2uVbrP0cXpuueWWqPwxyyduK5lQl9P1DPn5+WozOk1FPqNc+Jhxzd5YH+JWm66kqFmzZqQecp3S6w/X/0QQwcoB8M6KxgThil8m++23n2p82H/11VfD/fv3j0jbKHgIYLqHbrsVgDsB7rBihTWrYHxPO19MnC5UNv6Nio6049nw8rWbFje2WJqYeHmIvEhWk+PHTV/wO92dp5M6oHt+xoLm6c7HbNggUHAfgsWUK1eurH5/8cUXpv0T2rixrXN/hr4OL5jbb79daRXM4tD7B8TTvXt39RvhjXHpadH3J02aFInLmGZjGoxhjenj8IsXLw5PmTIlbrrN0sf3MOaPWT4dcsghtgQHp20nVVs60lRoeGeZ9dn6BxuXUdu2bV25N4Qq1A9Gryd6/UkUEawcMnjw4EhB80sK6sJrrrlGFVi9evUiv9FI0Qjx38nLnhsvxxWr4mMZEoQ1SvP6OW7UsTZeFgbPhfB62u2mxUlDirch7VaaEKslDKBJTCS//b5xOeK53IiPy9pY54x5Fqve8PI3dtPPYfE73fmZqZternofsmTJkvADDzwQs38ya+scHteDWHEY+weus2Zx6fHo+3pcZuH0NBjDGtPHx+ymO1Z67OSTWf+rb/yO8Jtg5eQd4caWpz0/+mvjMlVIhzEva9SoEVVGgwYNSioNKAu9fpjVE7PzThDP6zHAqu4AhrEwImXgRh8LVupZh9W34UIfxsc4x+70BUEQ0gkb7sJoHosGo69CH6YvhgsDeCxAjIkLFSpUoB07dqjfMETmBbTRB/IxnOc4eEKH3u9xXADXG+PChALcx7gP42Y9HAyn9X2AMHgGPSwmLoAffvhBGV5jggH6bYbjQbo5H9B/I82IS88bgGfDMyC8/oz4LwSbnH/rKiZ7oN707ds3cu7ll19Wxuw88QhgEoJTRLCKAWYXAMx20AUlyTJBEIICXiJGuA/TPxA5HH8kspDB6MfwO1Y/yHHpwotVXGb7sdBnWOrHjBjD8H0Yfk4zON3S12cueXl5qt5g/UArUAcw69opIlgJgiAIgiC4hDgIFQRBEARBcAlxEGqTNWvWKEd7cGi2efPmmKpDOH2DgzdRBgqCkG7QJ+n2U7BN4v7JbLiLj2GoBLBzXNgd1alTR9kr6XHEGlJj2KYJ90c8CM/xclxIIw8xss0X9uHUEfsctly5cuo/31OPFzYzbAfF18BB5erVq03TjTD8rBwPh4nXj8tQYbDJy8tTdWvhwoV0880308yZM9V73mhHB8fPjuN2MZ0ZDTy/whgSRpFsvA4bLIzjw8ARxm4oFBhBwuP5xo0bVcGhkf47+9L2vXT7BKfgWm7widwThn3c0aAj5ueC4WoqDDels8qcPGFjX6Qfv2VCh/dwO4ahNvohCBMAHsHh3R7tGH0Y909o4/gPr9fwYs19GF44bdu2VfEtW7ZMxbPvvvuq//BkzXEgHPq4VatWqbAwlNfjwkcofiMuePrGNYgHaYMnbcTHcSGNOI5wMGbnfXjIhmdtDov4OQ7cEx66OV6stsB5wNfAM/h7771nmm4c42fl9HEYvR9nb/zGfEL+6SsaWJVHENtvKgmZ5BN/DOAYhFz8TvYdBGEagjm2nj17KsP1s846i2bNmkWnnXaaWp3BzCbRMUnNKcwikFUPPvig8nGCqcwff/yxOo5942/9P8LqG0/51KeU4rd+HcfF4Tks/vO9Oax+3OwcuznQw+hh9ecypt0sLXqa7TyT8ZzxOrO0G++hp1GPC89mld96Wtknl36MfdSYpd/4HMbffK0xjWbXm+W9nbiNz6WXpbFMY93LWNZmdY7j4Pw0yyM9Tfo9jWHMwpo9g1nazZ4jVn1zuqVqSrmxnlmVdbwp+rE2vS8wlie3Wzv9k1lb16+PF4feP8SLy2o/XrhYYa3C2013rPTYySezstXbgFn/qYc360vM2rRZu4jVl1rFY/WOiNe2jO5y4t2rUKv3+n2MfYj+7jH2TcYyctIXmL0zrbCqP8kgGiuHakNoqaAexppEAPvG3/p/XhOKgbZLV13jSx6/9es4LlyLMDz9Gf8huethkS4+DozxYNYDDwFwGIav5+cypt0sLfxFoccX65kQP74yjM/A15mlHVpAjg/nEI7TiHN8HzybVX4jHGtM+AuFhyA4X3j9NQ6rpxH/OQ36b74vrtXTbywnfTq7Me+t8kuP2/hcGNLAOXwlG8vUqlzMypq/+PQ6hzzjtcc4HOLX80gvH64LyEtjGLOwxmewSru+DiOf18vGWN/4WfTp8GZwONQBaGz0r2Kz6/Sp9Vbn4sH31J/XWKf0Y1b30J9RR+8LuA8wtls7/ZNZW9evjxcHH9PL2Souq/144WKFtQpvN92x0mMnnzjv9bLV+zM9f4ztnuuHsS8BxjZtjN8svFnb5/ZjrDPGdwS3Yb1t6XWQ63Gsfka/trxW7/X7GPsQ/d2jvyvMykg/b9UXAGNfzs9shVX9SQaZFWiTESNG0BtvvEFXXnklPfHEE2os/9lnn6WpU6fSa6+9ptTO/PuQQw6hd955R/3H4pN6oUP1DDUyChsVBYu6osJhseSjjjpKXcdxYfHgtWvXqjC84C/U0I0aNaLDDjtMhcVxhMUxVKgGDRpEzuHaTz75RKk3odLGtXoHjbCoUAiL58K0Uj3tZmlB2lFloKbFAq+IL9YzwacMFjHFOajY+Rn4OoTlfU77DTfcoBbhxDW82C8ayVVXXaUWxMQ+VLp4JiyifOyxx5bJbx5mALxgM16qWIAZjRrXQ/2P9HJYpJnTiHzF83K6+TfC475YtBThOP24Xi8nxI995JOeV7HqgB43wnM54rnGjx+vwvHwhF6muLdZuehwWaMsOBzXOeQZFkVGupGfiPuZZ55R4ZAWtrXh8kF45Afy0hhGvx+HRVvBb34Gq7Szql/PsxUrVkTKxljfUI64hoe/IejhBWFU5SMMwuI47HqwkDF3e/o99fC6cGV2Tj9mBg8RoR7ALgnPa6xTej3Tu2HUdX4WpJv94+m+nDh/0O5RrmijWARY70MwHIg+Klb/ZNbWOTyuB7Hi0PsHLmeruPR49H09LrNwehqMYY3p42N20x0rPXbyiftftEsuW66naAP8jtD7T273qIfcD+l9iV7Hua3qfRD3icbw+j7DfZvepvT3B78juA3rbUv3bQbQbjFEa9XP6NcCbucYisV/3If7a+5DuF/Huwd5ye8K9E0vvPBCVBldfPHF9PTTT0flsbEvAGjnEKj09w2eGWWh1w/Gqv4kgwhWNkGG81eGIAhCkGDbSbx07Nqp6EIqNKWAnWbqcdixAURciIPjQzz4jWO6Vg7HcQzx8T6Mz7///vtIWH6Zcxywe+J4ISCwNoOvQd+Nl26sdOvpEyeg2UFxcTG1atVK1YGlS5eq/xDAjJq4L7/80nHcMhRokx49eqj/MOTElwGDRg1KS0vVcUjJDIdDGJam4znWMzrqY/QZMMZw+m99CIO/5uPNauGOiv/r2gd+LlYrI26vOh43DD31PMmUbwYup2SM2N02orWTFmO9MqvDgruwM060YTa2RhvGCwN9F/8H3F9x++bf3IchrBUcB6P3iXpc/Fu/v1VcfM64D+N1aDP4nDEO/RjCMnwNjn344Yem6WbM8ifes+E/ZhsCHqozm2Gp/+fzfmsD6TKyD2kzUPV3C9djq6F3ns3pNL0srKNtwDGoXl/cRDRWgiAIgiAILiEOQgVBEARBEFxChgJt4opvC0EQBMERThY/Ni4s7fR6IXttD61IxP+eCFY2wYwmXj0ds6Fgv4T/K1eupOrVq0dcBGDMFrPPvv76a2revLkyeIdhHE/1xrgxxuPtFjpmm+A6zAhCAbMRqRFUDBhpwqbC7B48hd7qWv0/7gkbgho1aqj016pVSz0nxqXZYR46qliO8WLh1FYoVsdodQ75ZZVXQcbtl4RZfGZ1JVaZWaXJWK94lhHXTXnheQNPN8eMKJRlx44d1axP2Im2b9+ePv7440j/BANeGOditiRm4qKt43/Lli3VNSeeeKKaCYZZW0OGDFF9A/ohjgNOOdE/cRzoJ2BsjnLGMaQFM3IRF5wsI10cF/pEeLXG7DTMHEPaeJ/vj3DoU1988UV1HZ4BtjFIA8LiGGZ9cVjMOsMMMayQ8fDDD0eOffrpp6bpxm++F6eP04P+DnmhP5sxnzBbFTPT0Peiv2FDerO2wse5n/Zb3U91nxnS8gn2cHi38D573dffZahLeNeib8L7iW0IE8lHdpOBWeaIU4frD2YJXnvttYk9m9hYJcfIkSPVlNA77rhDbZhyjKmbzz33nOoM9N+DBw+O/EdYHUy9RUNFhcKsBOyj4OEZlq/juHAtzmMaLXxw4D88pOPeHBa/p0yZoo4D/Rw8DsOYE8f4Wh0cx8bPZUy7WVqQdoD0c3yxngnxw20FzunPwNfp+5x2uITg+HAO4dCwkAacwzW4DxrdoEGDTPN7+fLl6jqe/dO5c2e1j06Tp8d36NAhKizSzGnEf06D/pvvi2v19BvLifMBnZieV7HyS49bL0f8HzNmTJQRsl6mVuViVtYoCw7HdQ55hjzg/ES4yZMnq86P80gvH64LyEtjGLOw/JufwSrtKBd0snqe6WVjrG/s6kB3wwABwojuhqFhw4aR+xnvaTzGS7iYndOPxXPDwM9rrFP6MR12w4Bn4d9mbhi4L+A+wNiHgHj9k1lb16+PF4feP+h11iwuq/1YaTamwRjWKrzddMdKj5184rzXy1bvz8z6T273ENS4HzL2hcY2bYzfLLxZH8/tR29TZu8IbsN62+J6zn6tcF2sfmaddi2HR5pQL/k+3F9zfnC/znnJ7wpcYywjvIP5vFVfoLthML4zjfUjXv2xhavuRrOQDz74QHl4Xbp0qfLyWq5cOXUc+8bf+n+E1Tfds21BQUHkt34dx6WH1z3c6mE5XRy/fm7KlCllrtU3/XqztFulRfd6G++ZOH6zdBj3+b56fHo4/Rzfxyq/9XQY4zGmXw9rzG/jbw5rTL/xOrP7x8svPW7jc/E5/m92b7N7mZW1sc7peWAMZ4zDrC5Y3U9vK/ozxEq78Tn0/DHLP6dbMtcme0+zOpVMeozlamy3dvons7auXx8vDr1/iBeX1X68cLHCWoW3m+5Y6bGTT5z3Zu0hVv9prNdWfaNZvY0V3tgOzfohs3eEnbYVr58ptEizfh+zPsSYlxzOWEZm/ZKd9OplYYZV/bGDDAUmAaTisWPHKrXinXfeqaR4OMTE8BkcwkFtzL/hLI3/I6zuYBBSPCR3Rld98nUcF65lz7uA12LDvaHSRFioqaEF4MVX8Z/PYXjvggsuiLpWTwv24XCQn8uYdrO06Gnn+GI9E9I3evToyDk9HbiO12LU037rrbdGxrp5oWt81SCNOMfqYN05nTG/EcboiwzxGBfVNgvL8evj7fpvvi+XDadRLyc9H/S8ilUH9Lj1MsZz3XjjjZFz/J/z0qpcdLis9bLgOoc84zXSAMoMX4Z6HnEcSBPXBT0vze7HYflZ9GewSrsxDqSL89WqvjklHesYmtUps2NO4L4A5Yo+wNiHgHj9k1lb16+PF4feP+h11iwuq/1YaTamwRjWKrzddMdKj5184v5XL0e9P+N3hFn/qbd7vS+xquPGPtEYXt/X65jetxnfH/o7wk7b0uuq8V5hk2vZhYJ+H+5z9H6d85L7J1xjLCO8g/m83b5Af2eiLMywqj92kaFAm7BRpGSXIAhBxal9I+ygWFiGPRH/TrYf5LiMfvmM8fKKBLovKSufSwiLFzOOw36Kl1zioXQ30i1kDsXFxcq7+9ChQ8v4P0T9YW/8Rx55pOO4RbCyydlnnx0xXodTOAha6HTQ6DFmCxf7KAwY1GH8GpIw/mO8HAaBkKAx1oswPOaLL3XOfoypI05cj/WT8PWAjgHO6hCOOwrASyLA4A9fOGwvhPuwkXu1atXUGDPiZ4/F+r2wYbwZXwfsxh/xIW0wUEX8sNthI00Y5yMtMDC0Mhhkp6T6fRj81p3j6bMxcI6/NnAc6ee8wcZL0CBNfA2eCWG5QaCR4HqkHcdh54PrjGs1xoId0hmfST9mnEGC8yx0sz2PMR/4HI5znnA8eh3Qw6KMEZaX2eHlLfjFhvtwnIzRmR7nGeJCXUUdRH3htHGZ4D+OsfM8lC/SpT8bwqFuIh6ER1ycFj1/cD/UQaSbQfpxPdux6RoqPe2xuiLjs9q5xq/ojg+NdcsJXE8431H3cQx5j7YMo3XkNfoS2JPAIBfn9f4JZcrrRHJbx3++Bn3EV199pX6j7GH8rcfB/R3HgX4KaWI7MNiwcFyIh23iENf//vc/tY+lTVBn2AYGxvJ8DcIhnzgNeAZ+doSFAf6SJUsiYQFrQ2Dczses0q0/K6eP04N49fwxyycYtqO94Bpet47bNttAcZuBXQ/iQBvANWbtHmlH/qFeGDWZHK+xjbPNH+/rk0bY27yxjfL6eSgvrnvcNjm83fao983cdwP9PcD9DPKE+0Bey4+fGXnD/Q7C8LsM2qzu3bvTd999p8oUeYfjnF5eKzCWI2zUR7QJlBkmKWBCw4ABA1T5wEhdh+sPJkrgfCKIYCUIgiAIguASYmPlACxoiZkC+JLB4rDQIrAGgF0rYOFNfBHxgpv4jS/1VatWqTigkoZ0janQPMNC/zKBtAzpHxI7xnexxALCQVKHdA6NEWupEBaSOMLiP3+VQJuALwCEx1cs9qHhwX14DJ9ndrCbBoB4+GuTp7I2a9Ys8lzsZuLHH39U2gpsrH3jryn+WjKOu7OanzVz/HXDWhFe1oBtaDg9vAgtf1EBzh/OP30ZH17fDF/KeFZoF/H8TmxWjEMN+r4+XKEvNcO/+WuTwxjvqy9XpM9as0ofnhVfefiPuoR6g3JBmejLI+nLExm1avoSJ5xvujaPtYT6Fyg2vgenA+DZ+GuXbRm4bunPwNo21q5iH/fHNXgGfClzPrEGzY77BT2vk13mJ924tfwSa265nJHvqDP4j3jx5Y26g74DrgrQl+j9E/7jHPoKbus4D805rsfizmjruA5aMJSdHgfCoU6if8B9Ub4cF/dBHBdm4eJa3A8uDzhexAUtAU+5R53iNCMc4LDok9AH6dPzOV6kT69DuAbaKKTTKt36s3I8nB4Oy/ljlk/QyEKbgjSjv+EhTrQHrvvQzEAzhrRBM4Pr8d/KvQFrj7ld8aiEsU/i9s5tlsPoy5IZ25l+Pdok9wus/eH2bFfbb+w3cnJyIrNWrbTxPJuV74c8ZPsqnsnHz49wSAu0Tai/qF+LFi1S//l6qyWFjOlEXNBcYcNCz6jbp59+unovf/bZZ6qsjX0QzjtFNFY2mTBhAk2aNCndyRAEQUgZ/OHFw774ncwkAat4Y4XDSxAvVrOwRsEaYVlo4Gt4KMiNdAuZQygUUgIdhgJPOeUUJejycLoeBgbyTpElbWxyyy23KGkZtlQojPPPP181WmiqMANE/40vKv6PsCwhY9NXZmfw+5xzzolcx3HhWv5y4f+IA/fmsJDydSlcPwdYI8TX6hvC4np+LmPazdKip9nOMyF+3X7KeJ2+z2nX4+Nw8KeCNPI1/GxnnnmmaX7rsD2Gnn52vqqH1dOop8H4fHytnn5jOekY894qv/S49XLEfz5nVqZW5WJW1txp6HWOF93l/GS7HT2P9PLR64IxjFlY/h0v7WZ5Fqu+OUXXqKYK/XmNdUo/5hS9L+A+wNiHQOCI1z+ZtXX9eny916lTJ3IMWnSr/kGvs2Zx6ft6vLHSjHCwTbIKi/N6eGgeEF6/Bi9Mq3Sb3dcqf8yu5bw39llcNmb9p97uzfoSszZt1ifG6kt5s6ozxncEt2Gr+qjbWxqf0yzNOVq9R7009tecH9yvc17yuwLXGMtIPx+vLzB7Z3I584a6A6Hp4osvVulCPdHrD7ZEhCpFQk4ashBkFfxllJSUKP8X8HEBsG/8rf83+vRgXxpFRUVRv/XrOC4Oz2Hxn+/NYfXjZufYZ4ceRg+rP5cx7WZp0dNs55mM54zXmaXdeA89jXpceDar/NbTCp8txmO41ir9xucw/uZrjWk0u94s7+3EbXwuvSyNZRrrXsayNqtzHAfnp1ke6WnS72kMYxbW7BnM0m72HLHqm9PN6LsnFZv+vLHqmdNN7wuM5cnt1k7/ZNbW9evjxaH3D/HistqPFy5WWKvwdtMdKz128smsbPU2YNZ/6uHN+hKzNm3WLmL1pVbxWL0j4rUto1+oePcq1Oq9fh9jH6K/e4x9k7GMnPQFZu9MK6zqTzKIjZVNMDMBHlgHDhxIv/76K33xxRdK0sa+8bf+H2F1YJ8FFXVpaamalcK+OfTrOC5ci/CY4YfZhfgP2xY9LNIFlTqOA/0cvqTwZYAxaL5Wh6/n5zKm3SwtPO6O9HN8sZ4J8cMuAef0Z+Dr9H2+7wcffBCJD+cQjtOIc7gG98GzWeU3ZvggHXh2DAO0aNFCxQM7C7RRXAsbCT0s0sxpxH9Og/6b74tr9fQby4nzAbYOel7Fyi89buNzvfnmm+ocvqCMZWpVLmZljbLgcHqeIQ84PxEOtgZ6Hunlw3UBeWkMYxbW+AxWaWf/NXqe6WVjrG88swpfsoiTZyQa4VlEqAdoK7rtiH5P4zHYeVid04+ZwWnTn9dYp/RjOviy5mfhoSy2s2M4f/Q+wNhuQbz+yayt69fHi0PvH+LFZbUfL1yssFbh7aY7Vnrs5BPnvV62en9m1n9yu0edNOtL9DrOZW2M3yy8WR/P7UdvU2bvCG7Detvieo7fXI9j9TP6tZW1eq/fx9iH6O8e/V1hVkb6eau+AKA/0PtyfmZj/YhXf5JBbKxigGVS2MXC4sWL6Ztvvom8dACM6dD5ogBR2Oj8UGFQmDxlF0aKTtcysvLTYhXWzEDQbrGywTAaEBo6r3nFa41hqjAbB/qVIBswZzKx1qcUvIENfTFNHP0QXkRox1hPD0bKMLJmIR7neTkk9GEw8obwCcNwhMGaf+gf0IfNnj1bTQjBC/iTTz6JuGvAf/R7mMyDNgiDbz0u9Jn4jbjw8oPbA3YDgfsjXqzVh3Bdu3ZV1+vhsK+ngdf1Q3x8DrZTn3/+OfXv318tTcTT+RcsWKCuQTzoxzjdmEyESUEIO2vWLPWb7bhgFM35g2flfNIFYfSNqNfIWzYqt0L6JvsYJ69g8gXeO9h4aDGZ/gT1AuXMTkGxhiU+uCFQXXfddTRs2DC1XiWbQjDix8plUm2PIQiCIAiCP97zPGvbKTIUGAOROQVBEARBcILMChQEQRAEQXALV03hM5Dbb79dzSZ64YUXwg0aNAi3bNkyvGHDhvDMmTPDzZo1U+fOPvts9b9mzZrhhg0bhp9//vlwXl5euEaNGuE6deqo3xUrVlSzFHJycsKhUCgyewHnqlWrFjlXoUKFqNkNfF316tXV/Tg+PQ5ch2NXX321Ske/fv2i4sB5XI8wVapUiZq1gfvhOJ/HM9StWzfyXHra+Z4Ii3QgXrMZGfo1xg3X6Oc4ThzTnwm/ef+yyy4rc6/c3Fx1DOmtXbt2VFz6hue1SmeqNrN0xdrKly8f9xzqjPG5+D44HuuZubyRh/osTLP85fguvfRS07LkuqWH57TwffRjCIOwevmaxZvO8gryhvxu1KiR6guwj/Zx4403qvaM/61bt1b9E++jP8PGYV5++eVwmzZtwtOnT1d9Gbbly5dHfr/++usqDj6G+NEv1atXL1y/fn0V11133aX+4zjSwnG/9dZb4fXr10fFxfefNWuW6lN5H/dHOhCXnmbEwbz22mtRaUYcQI+Hrzemm58L1+n5g2vwLHr69GfjsKeddprKX71fRz+EOo++MT8/37IP1NuYsc+LFd6qvetty6r98DuC+3njuVjtMVa6zM6F/j2G+xj7Fz38iBEjIjP3+DjyBNfxO/XJJ5+Meh793WOV5lj9iv6eq1WrlipnvM+NoJ7qdcopYmMVBxil9+zZUxnVwSPv8OHDaezYscqg7YADDlBGltOnT6dDDz2UZs6cqQwiYSgKY0rM6IEBJAweYczOBr28HhOM6WBIB/8ZgL3V6ms7IQx7Rr/22mvp8ccfV/FhrSqOA0UIQ3r8R1pff/31yNgwe2OGgSkvqAqjQHiC57WYYASqr4eFZ4BRH54LRrCcdsTPa2/huWDkymv26UaHbOBvNjbNXqKRJr4WcbKRv3G9O5zDBp8inG98D8yQw3MhvTDKZd8kDOcd8tXKwzGnyWiAqq+/ZXXcKowelvPNaqye49DXPWQv2sY0cxiuD2bPxfUBBphs6Gn0iM51jq/ndOrPhTrBec71C3WH1xbj9b0A1y2Ex28OgzLCfbDPZYprUKd4AohZvqBexiovgUzLHHmJckB7YM/hWC0C9QX9wty5c+nSSy9VM2txDe/j90033aTCAPRrCIP2DRYuXKiMegH6NpxHH4hjiJ/7JV5TFXFxvI8++qhKC8f93//+t0xcCIf7oR7o+/jNcXGaOQ6APpiPcZpxjvtmfi6+Xk83P9e9994beXbOD34WTp/x2RD27bffVnWdnY6iP4N/JvTN8LCO/hP9MHtg53Li3zCAR1lxOzBbO9LY13CfyejhuT2yd3Vjf4Y2hdlx3M/zSiB8LWO2VqrZ8Vjk/NtPoZ/h+xj7Su5jEA5h+D3Aa4yib8c7FSCv+bmRb/zuMVtzVF+TFseNK2VwvrNPK7QV1AMzUDe4TjkmcV1OdgAJ+o033lDaKnyRLFq0SB3HPn7jywrZ+OabbyqJuLS0VJ2Dtoq/YvCbJWbjVz0kaByrXLlyRNI2StxVq1ZVYXA/xGfUArAkj7QiHbpmg79ksHGaJk6cqM6xdozTwP/xDPxcetr1tCAdfMyYXj19Vl8RlSpVivzGs1uFxTnWhujHOa2cFrOvFORBPO1NvK+uWMfjfWHqX1Tx4jN+fRk1l3pYLgNjXdL/G/PLrM7x9UYNGcJwXdG/JHFfPQzHodctvSz5PsZ0cZ3S65XxyzJWvspmvXH7RXk99NBDEQ0h+gX8Rx+C/knf/+677yJhoJnBPv6jXSEuwL+53+Njer+E8xwXx8tp4bjN4uL7Gff1uDjNHAf3wcY0632z8Xo93fxc+rNzWGP6jM/GfSy3B70fQt3W+0+r/gLXcjuw29fEaxdWbZ7P6f18vHoUr9+yc331f+9jFQfykt9FnCe4hq/jstWf2/jucZpuPR+4rVih1ymniPF6HNj/DTRErAkA2GevuwBTmdmTK0/BxYYvGF2C1uF103j9KA5nVCLyumq4nx6fHhbxIK1IB4AGiM9zGF6nkLVO/LXFaeD/eAZ+Llyjp53TgnTwMWN6EY8RYxh9eQrW2JmFxTkzTQ+nFRjzhLHr6sLsWitFrlne24k3XnzG82bLb3AYLoNY8Vg9t17nGK4rehiuKwzKgDVRHEZPM4fXy1K/j54urlNW6XTqnkTYC7dfdp3CeYl+AcfQh6Dv0vcB9x3QLkEzgv96OaCN6Wu98TG9XwLstoXDcFo4bqu4cE7vXzkdxjRzHEA/xmnm4/pz8fV6uhm+h54fxvQZn437WG4Pxn4I13H/adUHGNuSGcZr4/U3sdqN3r+b9c9W9050QCv8bz8TKw7kJb+LjHkCuGz15zK+e5ymW88HEMuzul6nnCLG63FA5s6YMUP5NsEQBvytAOzj9//93/+p/XfffVepM+ETBudQYbCPa7jy8MLJDA/rsBrXatonu+fH/RAfDwVxHADHkFakA/B9+J6sasX/V155RZ1jNTqngf/jGfi59LTraUE6rJYT0NNnBd8bGP2GGJ8dz2W8F+7Bz4a0mOUdP3ciy4ZYlYVxHal4cejDe07i0/PHCJeB2XPpi7Oa3VOvc/qyNMYwXFcYlIGxnPT85fB6GL6P8Rq9M7UqNyExuP2ivDCEwc4h0S/gP/oQ9E/6/tdffx0Jg+Fb7OM/2hUvd8S/ud/jY3q/hPMcF8fLaeG4zeLi+xn39bg4zRwH0I/xNXxcfy6+Xk83P5f+7BzWmD7js3Efy0vT6P0Q6rbef1q1bx4+j4Xx2njtwqrN8zm9n/eakGayYAXykt9FnCe4BooMwGVrXCon1rsnXp+M6xA/91tcD8zQ65RjEtJzZRHt27dXht4wpoOqsm3btuGtW7eGR40apYwhYWCH4wiDDUbFCAs1I/bZeNpKdYnj+lIbxnC8j7hwPwy1mMUFtSqM7TgdxvM4ZrwXx49rcZ7/I838XGZp5zCxVNKx1NZOh3/w3FbhkRbjUGK6lzFJdoulfufnTua5uG7auU+sMuC0ct2yuo+TspYtuQ31An0Q+gIum3bt2kX+Y0P/xPvozxCWw+AcfsOwGO0fca1bty7y+7zzzlPn+RgMj1E30B9gH3F16NBBxduqVSt1DceNOLds2RIVF99/9OjRqk/lfYTFdYhLTzOOM7heTzPiAHo8fL0x3fxcnCd6fuBZ9PTpz8Zhuf/T6zL3zfgP43U7Q3eprBvcHlN135w492nRooXpNdj4nTps2DBH7x477YPzAXUX5Yz3uRHUU71OOUWM1+OALxZ47tUNzyFZQ9KFehngK50N6XCODX6DjhgRC0Lw0I2aWfuIoXd4ncawLyYtoH/CUAvaN/+Gx3PWcMGIGEM12HANzqOfw3nExf0grtX7CNwP3tGxUgXOsRYCwy8wHNY1uIibh45POOEEleapU6eq6xAWWttrrrmGJkyYEDGVgLbi8MMPV78xuQYTJlgLc99996l+GROHbr75ZhXv6NGj6emnn1a/9XQD5AOeldOnGzWz1kRfKgVwPgneEvLYYz3qDDzpo46hXo8aNUrVW/D999/TAw88oOrnl19+qSbuOCZxXU728PHHH0cZ3Momm2yyZfrGEyp4YoKb8Tq5P6fBajKGPtGCjbzhcqZv376iHZUtbLahbvz000/hZcuWhQ899NAy9QfHcD5RRGPlAKxR9dFHH0UWn8XUVqyfhbWw/vjjj8haWfiNLyCWuvG1hfWp2CAZ/7HPX1s4j68+rFnUp08fNa6LLzbEBRcPkJqxFhe+HNkwElNOMb23YsWKas0qTD/F1F18aWK6b506ddS53377TX3BYUorvsgQL87z+Df+cxxIK54HcXDaeYqqnhYsfAltHc7hOK4BVs+0zz770OrVq1UacQ2++vAMSIdu9Il9Tju+HD/99FN1HfIaXxf4osV5fHkjLqQBZYHn2n///VV+6PmNPMPXB/IAX5n42m3Tpo3KfzwDjsGdBjSSuA5hcS3ugTzBMXzVIjy+YJF2/Mbz8dcunh1fNuyKAue5nBA/no2nOOO5EB55blUH+L6IG9PCsfGxpUuXKvceqF+sWcAz8RRiGGIay0W3c+CyRjl+++236hnq169PPXr0UOnD2mo4ji/6Ll26qPqEYygrrM2G6dqIA8+BeyAPodHFc+B6DqPfj8PCYB3lhjjxDKhvZmlHvvDEBpzH9Ti/bNkylUewe+HFUrmMkcesRWAXI6gvrHUA0FQgHOJGHqD9oP3iODa+J56DNdM4hjDs/sJ4Tg9vBsKhH0AcSA/Sjq9jvU6hLXA9Qx6x5hvn2bAW+Yh6zvUD9ZNdqKCcMbUf9RhtBeGwz31I8+bNVRxoL1b9k1lb5/4M16McUE+t4tD7B5QHyhnpNItLT4u+r8dlFk5PgzGsMX18DNhJt1n6rPLHLJ+wPiyuwbqCqNcoO/RZaH8oK2hC8I7Q+09u9/iPcudw3Jdw34g6zm0aecp9EPeJ+K2H1/tSbvs4hr4N9+J+CHWE3x/8jkB9Q3+AOsltiw3Q0QfwxCeUL+5r1s9wv7ZixQr1G+HhdgHhUCfRjtF2unfvrjRByAsYiLOLCqQdtk3ITzwj1n9E/8xlhDoPm2f0S0in/u7hvoDbJtKJdo5n5Hdm586d1fuI6wdjVX8SRQQrQRAEQRAEl5DpN4IgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAlCIIgCILgEiJYCYIgCIIguIQIVoIgCIIgCC4hgpUgCIIgCIJLiGAl+I5QKESjRo1Ky72vueYadf8//vgjLff3Kx9++KHKF/zPVpYtW6by4Pbbb/f8Xk8++aS6F+4Zj0aNGtEZZ5wR2ZeyKgvnySuvvJLye/fr109tQvYggpWQMpYuXUrnnnsuNWnShIqKiqhixYq077770j333ENbt26lbAAvQHTwvOXl5VH9+vXppJNOokWLFlGm8O2339Kpp55KdevWpcLCQqpTpw6dcsop6rjXeapvqGdC6mCBUN9q1qxJBxxwAL399tvpTl7geO655+juu+9OdzIEh+Q5vUAQEmH69Ol0wgknqJfs6aefTu3ataMdO3bQxx9/TJdccol64T7yyCOUDSAPHn30UfV7165dSuB8+OGH6Z133lHCFYSQIDN16lQaMmQIVa1alYYPH06NGzdWmpfHHntMaQxeeOEFOuaYYzzLU53c3FzKNvr06aM+VAoKCtKWhuuuu06Vezgcpt9//10JXIMGDaI33niDDj/8cMom3n333aQEq2+++YYuvPBCV9MkeIsIVoLn/Pzzz0oj07BhQ3r//fepdu3akXMjR46kH3/8UQle2QK0VNDm6PTs2VO9cJAPZ599NgUVCImnnXaa0kp+9NFHVKNGjci5MWPG0P7776/Of/311yqMl3mareTk5KRdU3fooYdS165dI/sQsGvVqkXPP/98TMEKHxp79uxJq1DoNpn0LII9ZChQ8Jxbb72VNm3apDQWulDFNGvWTL10jUybNk1ptqCNaNu2rdLoGIeAYF9iZSdlZrcVL04zfvnlF5VGXIevb7BkyRI67rjjqLS0VL3E6tWrp4THDRs2UCIgHhYQmHXr1tG4ceOoffv2VL58eTV0ihfWggULylx/3333qecpKSmhKlWqqJcavnZ1Vq5cSWeeeaZ6wfHzP/7442Xi+vXXX+noo4+mcuXKqWGcsWPH0vbt2209x2233UZbtmxR2kddqALVq1en//znP7R582ZVJ4zlBQEbZVq5cmWqVKkSDRs2TMXl9jAVtKQXXHCBSh/uheFpaE/Xr1+vtKnIP2yXXnqp0riYcdddd6kPheLiYurbt6/SKhj5/vvv6fjjj1eaO9QRlMnrr79eJhy0tQceeKCKC/Vo0qRJSrgwgrTgHMKgnDG8Zja0amZjBRsf1F9oRHEdrscwrV4Oen0/8sgjo8r///7v/5Ky20I+4/n0+q3brGG4q2nTpqpe8pC4nfxz0kaMoE5DyENd++STT6LqIu594oknqviqVaum+qdt27aVEQKvv/76SLrRF11xxRVl2orRxorL56WXXqIbbrhBlSeer3///qoN6NfhQwvlwcOqen9np80L6UE0VoLnQP0P7UTv3r1tX4OXH4aUzj//fKpQoQLde++9SpBZvny56ugSIZE4oYHBSw+d+3vvvaeEA7yEBw4cqDrQ0aNHK6EIQsubb76pXs7oqOPBxvG7d++mn376iS677DKVBv1rHschCGIIFcMqEOogmOBFrg8ZTp48WQkKeAnxCwAaoU8//ZROPvlkFQbXQivGAiaECti8QJOwcePGyFADhpDQwSNPECfu8cwzzyhNo92yRucPzZTVMBXOm2ko8SLDc95000305ZdfqqE9vNhvueUWW/c2m3AAbQFejjpcZtdeey3NnTtXCYF48ePl2qBBA7rxxhvprbfeUkIihBEIWzpPP/00/f3330rbiryGjSDqyMKFC5XQCiDwwH4Qwsvll1+uhBS8SCGwvvrqq5Gh0NWrVytBBy9pDof0QAgxcvXVVyvBCkNq2JBHBx98sKqPdvjrr7/okEMOoWOPPVblNYZlUe8glEAYARB68SyrVq1SdQn5hJf1Bx98QE7ABwbKA8LgmjVrlBCAjyszreITTzyh8vGcc85RAgramt38s9tGjKCeH3XUUfTFF1/QjBkzqFu3blHnkT+op6iLqCPoK5B/KHvmrLPOoqeeekq1u4svvli1N4T/7rvv6L///W/cPLr55puVdhGCIfILQi7sEBEPuPLKK9VxfOhAkAcQHu22eSGNhAXBQzZs2IBP/vBRRx1l+xqELygoCP/444+RYwsWLFDH77vvvsixoUOHhhs2bFjm+okTJ6qwicTJ165duzb83XffhevUqRPu1q1beN26dZEw8+fPV2Fefvll28+kpxnXGre6deuG582bFxV227Zt4d27d0cd+/nnn8OFhYXh6667LnIMedu2bduY9x0+fHi4du3a4T/++CPq+EknnRSuVKlSeMuWLWr/7rvvVul56aWXImE2b94cbtasmTr+wQcfWN5j/fr1tsr6yCOPVOE2btwYlednnnlmVLhjjjkmXK1atXCieYpt4MCBkXBPPPFE5NiePXsix3v16hUOhULh8847L3Js165d4Xr16oX79u0blfe4vri4OPzrr79Gjn/66afq+NixYyPH+vfvH27fvr0qQwb37N27d7h58+aRYxdeeKG6FnEwa9asUWWC47gnH0P9Peyww6LSfsUVV6hwyAMGZWQsKzwHjj399NORY9u3bw+XlpaGjzvuuMixO+64Q4WbNm1a5NjWrVvDrVq1ilv+eh4bN9TZJ598Mios52fFihXV8+nYzT+7bYTzBG3277//VvlRvXp11ZZ1uC6ijuqcf/756jj6DPDVV1+p/bPOOisq3Lhx49Tx999/P3IM99LrEaeldevWqgyYe+65Rx1fuHBh5BjK26yPs9PmhfQhQ4GCp0AbAqAhcsKAAQOUip3p0KGD0jzgCzVRnMSJoR189eKrFV+0ULUzrJHC8EgiQ1VQ+0P7hQ1x4AsbX6LQQvzwww+RcPh6xxcta7b+/PNPFa5ly5ZKW8FA24Kv2s8//9z0fpAr8ZV/xBFHqN/QJPAGzRu+ijk+aGowXIsvYQZDDdAmxANaHDtlzee5bjDnnXde1D60XnhmY7h4eapv0AoYgZZOHyru0aOHyhcc143eMbRiVjegNYEmhenevbuKA3nHw1PQ8EHrgTzhvMazIL8xjAwNJ8A10CQiDgbaRGgudFAHoZmCtk1PuxOjZtQdXWMEbR7uqz8jhsbxbBgK1PPWqd3fAw88ECmDZ599VmnloOGBxtgItMb6sLGT/LPbRhjUdWj5MNSHIblOnTqZph/aSB3kO+Ay5v8XXXRRVDhoroAdm1EMdev2V6zltdPHxWvzQnqRoUDBU3gYhl+6dsGQjBEIN1DHJ4qTOCGEYFgHgg+r3xkMOaBDvfPOO2nKlCmqQ8SLCC8tO8OAeGlDyNOBUNW8eXMaP368EoIA7GwwzPTggw+qCQB4cTD60CWGc/DixUsStmB4cWA4AEMpYO3atWqIEkNMVjMvMVyj25MZbdTwoooHC0zxytpKADOWDwuzKB/jcJ6dPLXCeB8uM7i9MB43qxsoJyMtWrRQQ1UAdjIQ1CZMmKA2q/yGAIP8hlBmxJjfCGd2bwgkutAfC9jyGMsV12IISb8PPj6M4VAnnIC6qBuvY5Zo586d1TA0hrt1gQLtScdJ/tltI7ogimGz+fPnK/skK4z5jDyBAMd+xZBP2DfmC4ZOIfRwecUiVn2PR7w2L6QX0VgJnoIXIuwczIx7Y2E1TV43JjZ2/ozeuTqNU/+Khn0VBCcz7rjjDvVCgrEq7DVg74COGl+RiYCXHl6mmEnHwNYHAhzskvDVDyEPGgDcRzdubt26NS1evFi5Mdhvv/2UYIb/EydOVOc5LAQ/M60ONjc6ZAgi0HbpL2ozcB4vRaOw5KR8ksHqPmbHE7k35zdsZ6zy26mg4gapyl8zIIRAawXbLWicdIz2ZE7yz24bYWBXheeFJtPsvBVWfY3Vca/LI16bF9KLaKwEz8EXKjQlc+bMoV69erkWL77woIkxYudrMR4wXMYMJjZ0NzMIhdEvtquuukoZPkM4gT8qGBgnAgyYYeDLwLgYLyPMptTBM8OIXgfGvYMHD1YbhoxgoIwZR9CAQauBZ4DAGU+rg5luEILRuesvDXTidssahrWYKICO3sjs2bPVVz9m4gUVo2AAMITLM7bYjUR+fr6t/DaLz5jfCMf31t1UQBuZjBbXLD0w+jaWvz5bLVFQv4Fex81wkn9O2ggP40K7g9mnaBMPPfSQabzIZ12ThueHIMZljHzCPsJByGFgPI97c3klSyzBLVabT7e7jWxHNFaC52DaOjoB2FiwuwIdaIagzncK1POwmdA1JPgitjMjx06HBmEQtkZDhw6NmuYNmx9+STAQsPBVbtctgdmLGS/Tjh07Rn3RGr9eX3755Yh9CQO7Eh0Ms7Rp00Zdu3PnThUPNHD4qjXTHOLlrA9J/vbbb1FLf7D7BDvA2Ss0EBCcjOmC7QzsqGCzhXBBBbPQ9DL47LPP1GwsnlmHmYyYKg/bOdTHePmNWWeIQz9v1JRCwICggdl1ep1w2ys3bJjwbHp9x9AZhOVkQD2Eo0zUTV0QMcNJ/tltIzqY5YlZfvgIwpCalY2YDvIdcBmj3MzyH+YB4LDDDiM3QL9p5sIlXpsX0otorATPgQCEKdv4skKnqnteh6YHHaG+1pld4DcKHSOmXmMoDgIAvkBh72JmuOoUCEoYXsBXLgxpYbCKqegwrIWtCKZ4414QsuCSgAWYeCA84gX46oUGB508fuuqfGh/4MEaRq5wVYHp/HjhGh1r4gscth3QmMEuDNO977//ftW5sx0Thj4wZR72PDBERicMQQf5BFsN/AY4h2tRRvPmzVNDe3g2CEN2gG0KpqDD+BrCptHzOoyQ4SRSn0TgBnqeGkH9wAvKLTAMBW3ciBEjlCCNlyvsefABob+YEQZ5gDxFmeGjAlpbDBeznyVcg/yFGwRMm2d3C9B46B8M0DpiaAzT+VEv8GKHnRBcZphpZhIFAjHKHzZRSA/KH3WONSB2h76QLhiIsz0U2j+0O3CdEM9ezkn+2W0jRtB+8YEElwYYwsaQvg7stWA3iXLBPVG3oLXmDx/8xwcXygoaKkx0gXCMuo/+Alo0N9hnn33oxRdfVMOdcAkBe0/Yf9pp80IaSeOMRCHL+OGHH8Jnn312uFGjRmrqeIUKFcL77ruvcnegT6tGtRw5cmSZ6zHtWJ9WDt59991wu3btVHwtW7YMP/vss5buFuzEqbtbYOCKANOly5cvH547d274p59+Uq4BmjZtGi4qKgpXrVo1fMABB4RnzJiRkGsATDfH9HLj9ciTiy++WLlJwBR/5NWcOXPKTN/+z3/+E+7Tp49yTYBp5kjXJZdcolxd6Pz+++8qD+rXrx/Oz89XU+1x30ceeSQq3C+//KKmm5eUlKgp6WPGjAm/8847tqbbM19//XV4yJAhKu18L+zrU8lj5bk+dZ9dDjjJU33j6zm+zz//3Nb9EW+5cuXKuAe47bbblFsC5CPye//9949Mw9dZunRp+PTTT1fPjjyAS43DDz88/Morr5TJK5Qn6hLCXH/99eHHHnuszLPDrcC1114bqQ/9+vULf/PNN2XqsJW7BbPp+WYuS1C/Mc0f96hRo4aqg6+++qqKE/XfqbsFPFenTp3CDz30UJSrCD0/zbCTf3bbiO5uQefSSy9Vx++///6ourBo0aLw8ccfr/qoKlWqhEeNGqXcTujs3LlTlUfjxo1V+lAfxo8fH9WXcd7bSQvnB/KQ2bRpU/jkk08OV65cWZ3jsrLb5oX0EMKfdAp2giAIgr+BVg4e2KEt0l1NZBrwvA7HsRhudFMTKGQXYmMlCIIgRMAsVx3YWMHeCcO8mSxUCYJbiI2VIAiCEAGzy+BjCc4zYTgN+yLYS1m5HhEEIRoRrARBEISomYFYpxGCFFx0YKID/CVh8okgCAEZCsQMEPgHwcwTzFrSpx4bwbRfeLrmFegxDdkYHmZjWLAUM1ow9RthjL5iMAsKM5cwQwWecjF7KZ5/FUEQhEwH3snhlgP9IYYFMTs0W4Qq2Fjh/SH2VUKgBSueSopp5pj6jWms+GLiJTaMYH0nTAXG1HFMg8UyFJh6qvstwSrh7KcE/mUwhRlxwlaAgVCFFdThpffNN99UHq/trIcmCIIgCIJgRdpnBUJDBf8c8MEB4MsHwhIWvYTPk3hAVQ3NFfveweNgCRUshgm/LwB2AvD18eSTTyrfR/D5AfU2FrDk9ayw+Ch8w2DWC64XBEEQBEEIlI0VHERCzQwX/LpTRgzdQRtlBziFhKfZqlWrRhy7rV69OmopBDiAgwCHOCFY4T+G//RFQhEe94aGCw4FjcARoO5VGwIghhPhGDCZ9aIEQRAEQUgdUMBgMXgoUfDezyjBCl6YoXGCNkkH++y1Nx7wvI3MYUEKQhXHYYyTz+E/lk3QwbpwEM44jBF4PIZ/E0EQBEEQgs+KFSuoXr16rscb6FmBWKYDs1Vgd+X1opPQqsEWjMHwIqYk33Dbs/TwyoLI8dkDuqv/Z8xZSEs3R/uDcUqTkhD9tCX5kVqk6c7vl9F/fzW3WzOjd/XKdEunFrT/jL0TA05skU8v/SDrUAnJcUyz5vTfH8suPCwkT62iEP2+bW+fUb0oRJd1L6JLPkq8L6peLkR/bP4nzgf6F1POvxr6ETO2OIqnSjHR6+eW0P53/3Pd/nVz6ds/99C6f9P70ICBtGP3bhrzwQy1f1TtXLqhbWFUHBt2hmnfWebPMvtC82WX/ty8h46evNe+1i1OatWKXvhXAdCpYQ599cseCjIn9m1IL81KfgF7L5l4aAENaJkXqUOJsmf7Flr50D8LcXtBWgUrzLzA+mrGhXmxj3WQYnH77bcrwQrrnHXo0CFynK9DHJgVqMcJvywcxmgcj7XGMLRndd/CwkK1GSkqLqGcwr2CVcVy/6yDlVdUjnJ2JTdEmFcUopzdyQtWSFNhcXnKKbQ/67GgqJy6Lqdwb2dVWFxAOYU7kk6PkN0UllSIqleCe+QVhihHM5vNLQpRSbliyilMvC9CHDm7/omzXLmSiGCVU+g0HqKKFctFrssvzqPcwt2R9JYrV5Hyd++O1I3C4jyqWC76JuGdYctnQdxmbM/ZQzmF7g/3FBbvrcf5xbmUU7ibgkxhufK+b5cl5QqpYsU8x3XPCq/MeNI6KxArcmORyZkzZ0bZLmG/V69eltdh1t/111+vDM51OymABV8hHOlxYrFN2E5xnPiPhTNh38VgYV3cG7ZYfkHWGhIyEbFIDBbSDwlCwIYCMbyGVcIhIHXv3l2tSbV582a1WjnATD8sowAbJ3DLLbcoH1VYLR2+r9gmCqt+Y4MECj8skyZNUkswQNCaMGGCssPCquOgdevWatVyrJoOlwwwfsdq5zBsz9QZgbIkpCAI6cSsBwryvJ+f1q9PdxKyjjAFg7QLVnA8hwUvISxBSMJwHTRRbHy+fPnyKKv9hx56SM0mPP7446PigR8sOHcDl156qRLO4JcKmqn99ttPxanbYcGrMISp/v37q/iPO+445fsq2yqAIAiZTbJ9kZeyT5C/9/73217fiYLgK8EKQMDBZgYM03WWLVsWNz5ora677jq1WYEZgNB6CYIgZAoBVgB59ixBFt6EYNbvtHteFwRBEPxL2MOXYpCHAgXBChGsPCIT+wv58BPcIRNbh+AY6VCEDEUEqyzpd6QPE4TMJ+xjjVA4g/ssGW5MDWEKBiJYCYIgCEIS+EmAFdKPCFYeSdBuSNZutdWbF/1Er6yIdsKaCNJ3CG4g9SjFeRuUz3xByBBEsPIxbvWHLy43X/9QENKBfN0Ldvo8qSaCkaDUCRGsBEFIKWKPEtyXmdtl5/RFKXVHCAIiWAmCIGQobnzheyXLiIwkZGqdEcEqy1WWVny09i/avCvYi4oK/kSGAgNGUN5mQlKEpaBdQwQrwZK7vo/v5V4QnCOSVSqR16UgpBYRrDwiEzqz+es3ZtwzCUIm44nIKnJwXMT2KzWEKBiIYOVn0txY90hnIQhCmiSUoLxEQSZ0lct/35zuJGQMIlgJluyWzzBBCBR+brFOu5NwwJ4v6CwLgGAVpmAggpVgW7B6afGOtKVFEIT0E8RltlLxfRgk7VomP4NfEMFKsC1Y7QrK54Lga6QDz87yktmgQrYggpVgiQwFCkKwCbkwjV56AUFwhghWSZLJsocYrwteIIqL1OFKE5Z+IDuQhpk+weqpp56i6dOnR/YvvfRSqly5MvXu3Zt++eUX91ImpL0/E42VIAhuYac7kbUChawUrG688UYqLi5Wv+fMmUMPPPAA3XrrrVS9enUaO3asF2nMWtIt1ohgJXiCvDGDldVSXoLgiDxnwYlWrFhBzZo1U7+nTZtGxx13HJ1zzjm07777Ur9+/ZxGJ/gYEawEL9i4XWaXConh1x7Jr+lyQkgk6PRprMqXL09//vmn+v3uu+/SQQcdpH4XFRXR1q1b3UuZkPbGKjZWghe88dOP6U5CxmL6aszydpzljy8EQWMFQeqss86izp070w8//ECDBg1Sx7/99ltq1KiRF2nM2gUt060wEo2VIAQcN5QQYe/cLYgLBiETcayxgk1Vr169aO3atfTqq69StWrV1PF58+bRkCFDvEhj1pJusUYEK0EIFn5usdKd+BsRctOoscIMwPvvv7/M8WuvvdatNGUEboxXp7sfSvf9BUHILqTPEbLWj9Xs2bPp1FNPVS4WVq5cqY4988wz9PHHH7udvqxGvvAEQUg30g0JgseCFYb/Bg4cqFwufPnll7R9+3Z1fMOGDcoVgyAIgiDEQz4chUzFsWA1adIkevjhh2ny5MmUn58fOQ53CxC0BEEQhPQQMl3Sxr040y0Lpfv+guCJYLV48WLq06dPmeOVKlWi9evXO40uY3FjVuAeV1IiCEK24oogEgq2sbQIY4LvBavS0lL68ceyfmhgX9WkSRO30iUIgiD4gXB6hgJlllpqkfxOo2B19tln05gxY+jTTz+lUChEv/32G02ZMoXGjRtHI0aMcDFpgtggCIKQIW6sfHk3t5C+WkjK3cLll19Oe/bsof79+9OWLVvUsGBhYaESrEaPHu00uowlE9wtCIIg6Nz2+TYa371IfVQLguCSYIUGdeWVV9Ill1yihgQ3bdpEbdq0UUvdCO4igpUgCMkQcjmOH/7aQ39sDVONEncEKxHP/IOURRoFK7hV2L17N1WtWlUJVMy6desoLy+PKlas6GLysnxJG1dSIgiCkBnDazLkJmSkjdVJJ51EL7zwQpnjL730kjonuId0IoIgCPZZ8/ceOnbyFnrui50pva+MjApJCVYwWj/ggAPKHO/Xr586J7iJSFaCIGQniQgr9320k35eF6bbZu6IHJMPVJuIdJg+wQqe1nft2lXm+M6dO2nr1q2UyKLOjRo1oqKiIurRowd99tlnlmG//fZbOu6441R42HrdfffdZcLwOeM2cuTIKCHQeP68884jvyH9gSAI2USyQtDWHdJrJoqIVWkUrLp3706PPPJImePwxr7PPvs4iuvFF1+kiy66iCZOnKi8tnfs2FEtl7NmzRrT8JiFCF9ZN998s/KnZcbnn39Oq1atimzvvfeeOn7CCSeUcRuhh7v11lvJb8iXliAIgn12S58pBNF4HUvaDBgwgBYsWKBcLoCZM2cqgebdd991FNedd96pBJxhw4ZFhLPp06fT448/rtw6GOnWrZvagNl5UKNGjah9CGFNmzalvn37Rh0vKSmxFM7cQNwtCIKQdkLJ9yNB6of2pGm5CvkIFpLSWGFNwDlz5lD9+vWVwfobb7xBzZo1o6+//pr2339/2/Hs2LGD5s2bp4S0SGJyctQ+4ncD3OPZZ5+lM888s4zfFTg1rV69OrVr147Gjx+vtGHxhkA3btwYtcVCZgUKgiBkh8ZK+mohKY0V6NSpkxJMkuGPP/5Qbhtq1aoVdRz733//PbnBtGnT1PqFZ5xxRtTxk08+mRo2bEh16tRRAuFll12m1kCcOnWqZVw33XQTXXvttZRKpLEKgpBu/GR7E69P3L1Hek0hoIIVPK/DOShsofBbx2yB5nTx2GOP0aGHHqoEKJ1zzjkn8rt9+/ZUu3ZtNay5dOlSNWxoBrRasAdjoLGC1k6asSAImYxZH7dmS/JjbmEPBLrdWrJun7mdvv99D008tDCBmAQhhYLV3Llzlcbnl19+obBhYBnDbdBC2QHDcLm5ufT7779HHce+G7ZPSN+MGTNiaqEYzEYEEBatBCss24Mtlci4vSAIfmPe77vphcV73Rm4SdjFocApX/wze/2Tn+29kwQhbTZWcEvQtWtX+uabb5S39b/++iuyYd8uBQUFahYhDN8ZaL+w36tXL0qWJ554gmrWrEmHHXZY3LBfffWV+g/NlZ8QuUoQhGQIefCB9u4vqXG+GXbJeF3XYmXDcKkQQI3VkiVL6JVXXlEG68mCobWhQ4cqQQ1uHOCXavPmzZFZgqeffjrVrVtX2TexMfqiRYsiv1euXKmEIqxTqKcHAhoEK8SNZXZ0MNz33HPP0aBBg6hatWrKxmrs2LFqCLNDhw5JP5MgCEImESShIRVCVKby06pN6U5C9gpWGDbDkJkbgtXgwYNp7dq1dPXVV9Pq1auVUfw777wTMWhfvny5minI/Pbbb9S5c+fI/u233642uFL48MMPI8cxBIhrMRvQTFOG8yzEwU4KTkevuuoq8hsyFCgIQroJUje0S9YXFIIoWI0ePZouvvhiJQjB8Ds/Pz/qvFOtz6hRo9Rmhi4ssVd1o12XGQcffLBlOAhSs2bNoiAg/YEgCIJ9Nzbp8mMlCEkJVtDuAF0bBKN1CDJOjNeF+IhgJQiC33BLA4R4Qi4PQe4xSVwqlsCTvlpISrD6+eefnV4iJIg0VkEQ/Iaf1+rdlS6NlXTWQjKCFRxrCqlBbAMEQUgGH8tAnmDmH1T6UcH37hbAM888o5a2geNN+IsCMAZ/7bXX3E5fYHGjLUt/IAiCkNysQOlHBd8LVg899JBykwB3BVguhm2qKleurIQrwT2kQxAEId3YmTDkZ8Eq27R2QgAFq/vuu48mT55MV155pfKczsAX1cKFC91OX3YTnP5MEAQhJumSz6QbzSy+XLE78wQrGK/rvqQYLPcCv1DZhlWjdeMrSToEQRDSDWZ7exd5hnSKohZLGVMXpMbzfzI4FqwaN24cWQJGB449W7du7Va6BB/3IYIgZE8/ksqhQC/uJTKP4PtZgbCvGjlyJG3btk01gs8++4yef/55tezMo48+6k0qsxQRrARByCTWbcnQXi1DH0tIkWB11llnUXFxsVoCZsuWLXTyySer2YH33HMPnXTSSQkmI/OQdiYIgrCXDduI+t+3JSP7YunvhaQEK3DKKaeoDYLVpk2bqGbNmolEI8QhQJNxBEHINvuoFHPf0h307u/ODZelHxV8b2N14IEHKjcLoKSkJCJUbdy4UZ3LNl5d6d0MBVn2ShAEv9ksrduWHknljiU7aeFG571iBsmWQqYKVlgYeceOHWWOw+Zq9uzZlG2sSlMnIwiCEGStF3pOt9cKFIRADQV+/fXXkd+LFi2i1atXR/bhJBSzAuvWret+CgOKdAqCIKSasIlQlKzSystZgW7GbJVOGQoUfCtYderUSTVSbGZDfjBoh/NQQRAEQUjHx6fIUEKgBCs4BsUXQZMmTZSLhRo1akTOFRQUKFsr3RN7tiMNXBAEIbk+8o1Vu2jnnjAdWzc/4fuIjVXy7F+3Hs1e+Wu6k5F5glXDhg3V/z17xKRaEATBjwTtgy6WzLN9D9HoBdvV7wNqJDSBPWVDgSK7CToJ1dYlS5bQBx98QGvWrCkjaF199dWU7ezcs4d+2rQ13ckQBEEILDv27JWINu+OLx1ZhRA/VoLvBSsswDxixAiqXr06lZaWRs0YwW8RrIieXfZbupMgCILga8IOxBHfa4REskoJ4XCGClaTJk2iG264gS677DJvUpQBfLFuY7qTIAiC4Ar/fDyn5o3mxV18L5QJGYdjP1Z//fUXnXDCCd6kRhAEQfAVXrlbCMURedy6a0CUHEIGaa0cC1YQqt59911vUiMIgiD4SlsT9sFQoK34EF0AXroCZfwMT8dDgc2aNaMJEybQ3LlzqX379pSfHz0N9oILLnAzfYIgCEIaV4Lw8l0mnteFTBSuHAtWjzzyCJUvX55mzZqlNuNYvAhWgiAImUM4TXHr52y/SwPw0hWS00oGYSjQsWAFR6GCIAhCMAhnQMJsaynCQcoAIVNxbGPFYCHmxYsX065du9xNUQYQBIlaEATBb0OBbttdCZlHKJSBgtWWLVto+PDhVFJSQm3btqXly5er46NHj6abb77ZizQKgiAIaXoJaX463cXlF6SIZNlRn8PhDBSsxo8fTwsWLKAPP/yQioqKIscHDBhAL774otvpEwRBENLIeq8WkYjzgtRfoH5XUgTgXS/42cZq2rRpSoDq2bNnlNd1aK+WLl3qdvoEQRAEQRAIwnZGDgWuXbuWatasWeb45s2bowStbEbsBARBEATBfTJyKLBr1640ffr0yD4LU48++ij16tXL3dQJgiAIWc+Ha3fbeuGu/rvsW1c+dAXfDwXeeOONdOihh9KiRYvUjMB77rlH/f7kk0/K+LUSBEEQBAfTAk25/NsdcaN6fO5Od9Ik+JpQKAM1Vvvttx999dVXSqiC53Usb4OhwTlz5tA+++zjTSoFQRCErGL+hj2Owj/8sQhWmU4oILMCHWusQNOmTWny5Mnup0YQBEHICsJpWJ5HCDbhgFQJxxqrL7/8khYuXBjZf+211+joo4+mK664QjkNzXam/7Y23UkQBEEQhIwklIlDgeeeey798MMP6vdPP/1EgwcPVs5CX375Zbr00ksdJ+CBBx6gRo0aKZ9YPXr0oM8++8wy7LfffkvHHXecCg+j+bvvvrtMmGuuuUad07dWrVpFhdm2bRuNHDmSqlWrptY9RJy///47ucFVXy8RU0lBEHxDUL7yBSFT6rNjwQpCVadOndRvCFN9+/al5557jp588kl69dVXHcUFf1gXXXQRTZw4UWnCOnbsSAMHDqQ1a9ZYen1v0qSJ8vBeWlpqGS98aq1atSqyffzxx1Hnx44dS2+88YZKPwzuf/vtNzr22GMdpV0QBEEQgBvv+sG9ClyIRQikYBUOh2nPnn+MCmfMmEGDBg1Sv+vXr09//PGHo7juvPNOOvvss2nYsGHUpk0bevjhh5X26/HHHzcN361bN7rtttvopJNOosLCQst48/LylODFW/Xq1SPnNmzYQI899pi694EHHqgM7p944gk1q3Hu3LmO0i8IguBnQpkxKTApAqDgUJRW8ntpCZ76sZo0aRI988wzSttz2GGHqeM///wz1apVy3Y8sMeaN2+eWgonkpicHLWPGYbJsGTJEqpTp47Sbp1yyimR9QwB7rlz586o+2KosEGDBknfN2gNWRAEIZ1IXylkIo4FK9g1Ydhu1KhRdOWVV1KzZs3U8VdeeYV69+5tOx5ot3bv3l1GGMP+6tWrKVFgp4VhyXfeeYceeughJfDtv//+9Pfff6vziLugoIAqV67s6L7bt2+njRs3Rm2CIAh+597529KdBEHIKhy7W+jQoUPUrEAGQ3S5ubmUbuC8VE8rBK2GDRvSSy+9RMOHD0843ptuuomuvfZal1IpCIKQGlZu8q9eSAa/hEzEscbKCszqy8/Ptx0edk8QxIyz8bAfyzDdKdBMtWjRgn788Ue1j7gxDLl+/XpH9x0/fryyz+JtxYoVrqVREARByN5ZZEJm4Zpg5RQMx8FwfObMmZFjMIrHvptrDm7atImWLl1KtWvXVvu4JwRA/b6LFy9Wdlix7gtj+YoVK0ZtgiAIgiAISXtedwu4Whg6dKgyiO/evbuy39q8ebOaJQhOP/10qlu3rhqGA9A0YV1C/r1y5Uq1vA58UbGt17hx4+iII45Qw39wowBXDtCMDRkyRJ2vVKmSGhLEvatWraoEpNGjRyuhqmfPnmnLC0EQBLcJmrLGiwWTg+BQMkjpFHwuWMG56Nq1a+nqq69WhuPwjwWjczZohxYJMwUZCEqdO3eO7N9+++1qgy+tDz/8UB379ddflRD1559/Uo0aNdTahnCjgN/MXXfdpeKFY1AYpcN31oMPPpi9vZkgCEKGkuqhwHJFebR5267U3lTwFWkVrABmF2Izg4UlBh7X4UcrFi+88IItezB4fMcmCIIgZK7AE07xTUTzJNgSrDBsZhc43hQEQRDSj98Nt1Mhg/y2IbWZIHKVYEuwmj9/vq3IsC6fIAiC4A98LlelhFe/kmG5TCFMGSRYffDBB96nRBAEQcgqgvKiTAWilsgc0uZuIZPxYmaLIAiCU+LZpPqOgCXXFJGQKNuzNiHj9S+++EJ5MsesPbg90Jk6dapbaRMEQRAymKC8KJ0QysinEjzVWGHWHdYE/O677+i///2vWtD422+/pffff1/5iBIEQRCEbFBOuTliISbKWSxY3XjjjcoP1BtvvKG8p99zzz30/fff04knnkgNGjTwJpWCIAiCYzJVeBGykzBlqGCF5WEOO+ww9RuCFTylYzbg2LFj6ZFHHvEijYEjKIUvCEJm4+e+KJShWqpMHAqUGf8eC1ZVqlShv//+W/3GcjPffPON+o1Fjbds2eI0OkEQBCELJSsfJ00I+iSIoBmv9+nTh9577z1q3749nXDCCTRmzBhlX4Vj/fv39yaVAeOLdRvTnQRBEARfs2F7mH7dFP3Clte3EItQpgpW999/P23btk39vvLKKyk/P58++eQTte7eVVdd5UUaBUEQhATws6Cy+K896U6CIPhDsKpatWrkNxYyvvzyy91OkyAIgpDhglXGkqBaRcyYMqc+2xKsNm7cSBUrVoz8jgWHEwRBENJMUN5Egq8R43UPBCsYrK9atYpq1qxJlStXNs1kGLfh+O7dux0mQRAEQfACkasEwaeCFYzTeQhQ1g0UBEEQBHfxs07IL7MCQ0leP6BGLs1Yu9sfglXfvn0jvxs3bkz169cvo7VCxq9YscL9FAqCIAiCICTJQ50Lqfm7W/znxwqC1dq1a8scX7dunTonCIIg+AN/6BmyAMnoQGRzTorUgo4FK7alMrJp0yYqKipyK12CIAhCkvhkBCfjCWf4YKAYr3vkbuGiiy6KZPCECROopKQkcg4G659++il16tTJ4e0FQRAEQRCyULCaP39+RGO1cOFCtU4gg98dO3akcePGeZNKQRAEwTFBUli9vPh7GtCwEQWRRSv3OjtNVLfjZ6WQX4zXM06w4tmAw4YNo3vuuUf8VQmCIAiuMf3npYEVrAQhKc/rTzzxhNNLBEEQhDQQNEXDum1b050EQUi9YLV582a6+eabaebMmbRmzRrasyd6vaeffvop+VQJgiAIWceU7xZRtuLjkcAo4/VqRSH6c1vAJHa/C1ZnnXUWzZo1i0477TSqXbu2zBYQBEEQXGHTzh3pToIQh/4N8uilH3amOxmZJVi9/fbbNH36dNp33329SZEgCILgCkHTK8hnegCQQnLfjxXWDeTlbQRBEARByGyBRWYFeixYXX/99XT11VfTli3eu4UXBEEQEido78OQn6ULm4h1jOB4KPCOO+6gpUuXUq1atahRo0aUn58fdf7LL790M32CIHjEoMb59NbPYiuRyQRMrhJ8im5LHWThN+RXweroo4/2JiWCIKSUPMf6akEQvCIo4kpYxHX3BauJEyc6vUQQBB8SlI5cyCKkUgoZQELfrOvXr6dHH32Uxo8fT+vWrYsMAa5cudLt9AmCIAgJsidgyoXMkKsy4ymEFGqsvv76axowYABVqlSJli1bRmeffbaaJTh16lRavnw5Pf3000kkRxAEQchWgmy/s5eASbMBK6NwgtcN6hRtD+4rjdVFF11EZ5xxBi1ZsoSKiooixwcNGkQfffSR2+kTBEEQsuUlnwlylZD1OBasPv/8czr33HPLHK9bty6tXr3arXQJgiAIgiBkvmBVWFhIGzduLHP8hx9+oBo1ariVLkEQBCG79FUZorBK7CnE/1UWC1ZHHnkkXXfddbRz586IfwvYVl122WV03HHHeZFGQRAEIRskK0HwyAN8KgXXnEQchG7atIlq1qxJW7dupb59+1KzZs2oQoUKdMMNNzhOwAMPPKAcjcJeq0ePHvTZZ59Zhv3222+V8IbwEOjuvvvuMmFuuukm6tatm0oP0gi/W4sXL44K069fP3W9vp133nmO0y4IguBngiZXpdswOp1k75NnHo5nBWI24HvvvUcff/yxmiEIIatLly5qpqBTXnzxRWUM//DDDyuhCoLSwIEDlSAEocgIltFp0qQJnXDCCTR27FjTOGfNmkUjR45UwtWuXbvoiiuuoIMPPpgWLVpE5cqVi4TDbEZo3piSkhLH6RcEQfAzQROsBCEWG7cRfbliD2WcYMXst99+akuGO++8Uwk4w4YNU/sQsKZPn06PP/44XX755WXCQ1jCBszOg3feeSdq/8knn1RC2rx586hPnz5RglRpaWlS6RcEQRDcIxPsjDLgEXzLbTN3ZOaSNrqWxwws0GyHHTt2KGEHTkaZnJwcpfmaM2cOucWGDRvUf/ja0pkyZQo9++yzSrg64ogjaMKECTG1Vtu3b1cbY2bALwhBIhNeYkJmseLvvylrkfaYMTgWrP773/9G7cOI/eeff6a8vDxq2rSpbcHqjz/+oN27d6vFnHWw//3335Mb7Nmzhy688ELad999qV27dpHjJ598MjVs2JDq1KmjhjNheI/hRzg5tQK2W9dee60r6RIEQUgFSdj6CgkiWe5PUtkWHAtW8+fPL3MM2hs4DT3mmGPIT8DW6ptvvlH2YDrnnHNO5Hf79u2pdu3a1L9/f1q6dKkSDs2AZg32YPoz169f38PUC4K3yAdy5iMveUFIPa6sb1+xYkWlzcFwml2qV69Oubm59Pvvv0cdx74btk+jRo2iN998kz744AOqV69ezLAwnAc//vhjTP9deE59E9JPywp7JyQIgiAE9YNFPnQyB1cEK7ZlYnsmOxQUFNA+++xDM2fOjBq6w36vXr2S8nMBoQpDlu+//z41btw47jVfffWV+g/NlRAs2lQSwUoQrJChQEFIPY6HAu+9994ygsyqVavomWeeoUMPPdRRXBhaGzp0KHXt2pW6d++u3C1s3rw5Mkvw9NNPV0vlwL6JDd7hNoF/r1y5UglF5cuXV760ePjvueeeo9dee035suJlduAmori4WA334TzWNqxWrZqysYLrBswY7NChg9PsENJMNvu9EQRBEDJAsLrrrrui9jGTD0vZQEDSZ/jZYfDgwbR27Vpl8A4BqFOnTspdAhu0w6M74md+++036ty5c2T/9ttvVxuclH744Yfq2EMPPRRxAqrzxBNPKDswaMpmzJgREeJgJwWno1dddZXTrBB8gIhVQiZSr3wF+nVTFs+Qy8LZtkHpy4KSznTOgnYsWGEGoJtg2A6bGSwsMfC4Hs+lfbzzEKTgRFQQBMGvVC8udkWwkpFAwW2kTqXQxkoQBEHw18tLXoKCkHoca6zgUgFr69khll8oQRAEwWNEsgoOQR1jE5LXWMEIHDP3vvjii8gxeFDHDDy4IMB53gRBEIT0IXKV4DZBlv9CKTK0cqyxgmH5iSeeqNb1gx8qAA/q559/vhKsbrvtNi/SKQiZ18oFwWN2+n+9WkHIONcjjjVWWCB53LhxEaEK4DdcJ+CcIAiCIAjOkG/EzMGxYLVr1y7TtfxwDA4+BSGVSGckCIKQOmTxdg+GAuG8c/jw4crRJpx6gk8//ZRuvvnmiGNPQRAEQRAcEBCBRbz5eyBYwSEn1vK74447lMd1XgrmkksuoYsvvthpdIIgpAn58hTMqFKQR3/t2JV0PJ2aVaGvfvzLlTQJQkYPBcIT+qWXXqqWk1m/fr3a8BvHdLsrQRAEIXg83dOdpb3OPKSpK/EIQtA+JBNyEAo7KywL8/zzz0emL2K5mU2bNrmdPkGIiShdBMFdahQWBGpqe6YQlNySYvVgKPCXX36hQw45RK3jt337djrooIPUYse33HKL2ocbBkEQBCG7ydYXcLY+t5CExmrMmDHUtWtX+uuvv6i4uDjKIzschwqCIAhCtiLG3YJjjdXs2bPpk08+oYKCgjILJMPWShBSiQw3CIK7SJNKD5LtWayxgq8qeFo38uuvv6ohQUEQBEHI1o+eLH1sIRnB6uCDD6a77747qvHAaH3ixIk0aNAgp9EJgiAIgiBkzBCt46FA+K8aOHAgtWnThrZt20Ynn3wyLVmyhKpXr65mCQqCEAzkw1rwEtHcOMSn+VV5rym14JVgVa9ePVqwYAG9+OKL6j+0VfDEfsopp0QZswtCFvdFgkDZ3qakbWYGF/QtoA+/SXcqMlywUhfl5SlBCpsgCIIgCJkpUormMQU2Vk899RRNnz49sg+P65UrV6bevXsrH1eCIAiCkGkChtdIbmWxYHXjjTdGhvzmzJlD999/P916663Kxmrs2LFepFEQBEEIGNmr6cgsR1aZ4pcrFPLxUOCKFSuoWbNm6ve0adPo+OOPp3POOYf23Xdf6tevnxdpFARLsrbvdgHJO8FLpH4J2YpjjVX58uXpzz//VL/fffddtaQNKCoqoq1bt7qfQkGIQYZ8TAlC5pG1klUoozR8fk2Xn3GssYIgddZZZ1Hnzp3phx9+iPiu+vbbb5X3dUEQBEGQ97GQrTjWWD3wwAPUq1cvWrt2Lb366qtUrVo1dXzevHk0ZMgQL9IoCJZI5y0I7hJyq1WJqsMXXNLKXYWHlKoHGivMAITBupFrr73WaVSCIAiCIHhIfo5j/UlMQUrML+KTXI4LQprJ1vXIBMHvZGvL9FuX5LPkZMXsRhGshEAjnYYg+BO/CRh+R/IrcxDBSgg0opYWBHeRF7wQC6ke8RHBShCyFekhBQ+RYXp/IMWQekSwErKeUwY0TncSBCHjkPd5hs3yDDihkI8Fq99//51OO+00qlOnjlqMOTc3N2oThFSSbFupWi5EB+1T26XUCII7hNM4yC2vYUFIsbuFM844g5YvX04TJkyg2rVri7pXSCvJ1r4kZyILgmCFvBriclajPHp02S7yM/KKT4Fg9fHHH9Ps2bOpU6dOCdxOEARByAay9X0cSjCsV/klglHqcfy9Xr9+fQpnynLXQtZzcu8CylakvxW8REYzhGzFsWB199130+WXX07Lli3zJkWCkEK6NHastM0Y5LUnmCH1Ivi0q++dvbPUDw8Eq8GDB9OHH35ITZs2pQoVKlDVqlWjtkTWHsTizUVFRdSjRw/67LPPLMNioefjjjtOhcfXEIS8ROLctm0bjRw5Uq1zWL58eRUnjPIFQRAEIegavpEHFe6N14X4utYqVf+rlwuuWBVO4UCb4891K2EmEV588UW66KKL6OGHH1YCEOIeOHAgLV68mGrWrFkm/JYtW6hJkyZ0wgkn0NixYxOOE9dOnz6dXn75ZapUqRKNGjWKjj32WPrf//7n2rMJgiAkikyRDzAhf5WzG3doX70Gvd6riCr3yKGH3/G3sb0fcCxYDR061LWb33nnnXT22WfTsGHD1D6EIQg8jz/+uBpuNNKtWze1AbPzduLcsGEDPfbYY/Tcc8/RgQceqMI88cQT1Lp1a5o7dy717NnTtecTvEfMOKyZtn9nOnr2/HQnQxCyi7C/+i83hDdo0zpUyqW/9irChBgkNNl86dKldNVVV9GQIUNozZo16tjbb7+thurssmPHDpo3bx4NGDBgb2JyctT+nDlzEkmWrThxfufOnVFhWrVqRQ0aNIh53+3bt9PGjRujNkHwMyJzCokgRudCLKR2eCBYzZo1i9q3b0+ffvopTZ06lTZt2qSOL1iwgCZOnGg7nj/++IN2795NtWrVijqO/dWrVztNlu048b+goIAqV67s6L433XSTGjbkDbMjBed0aihOZAVBEFKGSEL+97yO4bRJkybRe++9pwQUBsNqGErLVMaPH6+GEXlbsWJFupMUSESw8hHS4fqWdHpeF9LTrvykKKxeXJzuJAQax4LVwoUL6ZhjjilzHIbh0BjZpXr16moJHONsPOyXlv4zA8EpduLEfwwZrl+/3tF9CwsLqWLFilGbIAiCICRCtaLilHzbOL1Hgwo5dFufAyL7DasaxAQfCYAZI1hhCG3VqlVljs+fP5/q1q1rOx5ou/bZZx+aOXNm5NiePXvUfq9evZwmy3acOJ+fnx8VBjMGsUxPovcV7CPf4alD8loQ/Mu1vffzLO5ktV+ws5vWs4jOObw5daxrGGWQjsX9WYEnnXQSXXbZZcpVATIfggvcFIwbN45OP/10R3HBLQJmGXbt2pW6d++uXCNs3rw5MqMP8UFYg30TgKZp0aJFkd8rV66kr776Svmiatasma04YR81fPhwFQ5+t6B5Gj16tBKqZEZg8JBp6dbskRUSBMG3lNdMaTxd0sZheO41OlXOpfVtaxDRbx6kKrNxLFjdeOONyrkmjLdhKN6mTRv1/+STT1YzBZ06G127di1dffXVynAc6w++8847EeNzaJEwq4/57bffqHPnzpH922+/XW19+/ZVTkvtxAnuuusuFS8cg2K2H/xcPfjgg06zQkgAtzsPEaus2SWClSBQtn/sySzPAAhWGG6bPHkyTZgwgb755hs1KxDCTvPmzRNKAJxzYjODhSUG3tTtrFMYK04Aj+zwzo5NSC3yqk8dO/eEA/UCEIRsm3jgRxur1EeYeSS8UBoMvbdu3aqWtsnLy9711gQhqBormXkmCOnFj8okO8qLIBIO+9h4HcvKwEappKSE2rZtq4brAOyUbr75Zi/SKAhCAuzcsyfdSRAEQcg6chLx5wRnoBimw5AaA0/mWKdPEFJJabGssZCoxkqGAgXBfZy0q0AOBQac+y/oRv/dr5On93A8hjdt2jQlQGEGnW4UB+0VlroRhFSSGyIqyiXatjvdKfEfDUvEyV9gyczRGCEAZGrVC/0rrpQvzqfqRdEzMtOuscKMOzgDNQKXBjL7QEjHOHeOVDtTRJsnZDpFuQktd+sb/OJiQXAXx7US/qGmT58e2Wdh6tFHHxUHm0JcRPZOLeXz050CQfCOCW2bkt/wWx/nVOGRqRor3/uxOvTQQ5Wjzl27dtE999yjfn/yySdqgWZBEARBSAW5fpNiksArgUbPoaP2yafX5u10LT7BJY3Vfvvtp7ydQ6hq3749vfvuu2pocM6cOWq5GEEQnFOpxJvuSjrB4NK/nbixyXQzAF0u9I2XA7+kI8Ak1HLhuwpOQgXBKV50HrGiDAWkn+jYIJc++n5XupMh+IgujfJo5jdSJ4I2s7VBzXL0+1/bKFMJQn8aOI0V1u974okn6KeffvImRYLgIrXLlacg4LcRjVNbt013EgQhZRqrk+u7px0cOrCJ7bDibiEzyUlkSRssioxFj7Fe4KmnnqoM15csWeJNCgUhmY4jIL1KOgSrWLesVCgzCtNKQOptpmisyuWSa2A6v1/7Fjv9TDyNlFRNDwQrCFE//PADrVixgm699VYqX7483XHHHdSqVSuqV6+e0+iEBLmqZ2/q0rxqupPhfwKit/ass5JeUMhggm5jleqlZBrVCLZ7imRIpQ1bwvrPKlWqULVq1dT/ypUrq/UCa9So4W7qBEuaVa5CoXXpToWQqUOBghAEcjLIVsvL9/6rw4tpxhd7lC1nOtORLTiul1dccQX17t1bCVWXX345bdu2Tf1fvXo1zZ8/35tUCkKG45Vglf7XhZAQ8nazRU7Av0hSY2MVoibVc6hH7Tx7Pq2k7qVeY4WFlqGZmjhxIh177LHUokWL5FMhCEmQCf2A314PmZCnQuYTdMEq2ylfRLQpRRMoU1lVHAtW0ErBESgWYYZtFYzZ+/btS/369VObCFqCnwgHRETwrs1bO5xIpKMpKcylLdtlYUbBH2SSWOUXP1apTEaIMhPHQ4EdO3akCy64gKZOnarWDXzrrbeUcDVy5Ehq3bq1N6kUhADbTtgiDcksScDCsmI5bxcvFTKLuh6vVxl0z+upSH6wcyiY5CUycwFaK2issH388ce0ceNG6tChg9JcCUKqyQR3C17NborVccPm4rFvdjj6fD51QGO6/aVFLqVOyHQalSumlVu3+14wQTzNy4doySafqI1cJOCyZ3YIVlWrVqVNmzYpzRUEqbPPPpv2339/NTNQELJVfR60zu+qgQWU6zDv6hSFqF1jaeeCf3BTYzWkXj5d973Fh4ZHhFLQlznNoQzpUoMlWD377LNKkKpYsaI3KRIENzuCwPQSqZWsEsmWirJ0neAzRBkjZISN1auvvmo6ZXPz5s105plnupUuIUvp3yAvK9XcfnN0GBh5VPA1XrdPN2cFpqPOR2ms3Iw3KltCWTkKECjB6qmnnqKtW7eWOY5jTz/9tFvpErKU41sUZGUnkGo/Vj6T44QMKyNeIsbr9pkb5EwyEtC+rHtp7XQnwXfYVg/AQB2G69j+/vtvKioqipzbvXu3mh1Ys2ZNr9IpZAiZIgi5TarfD1IMgpeMbVYQuFm/mSSj+Y2C3FzasTu9blp8uaQNjNMxBIjNzFcVjl977bVup08QsmIoMFOeQ3AHEXxTP4SelqHAkPf390PfEqLswrZg9cEHHyht1YEHHqjsrDA7kIEfq4YNG1KdOnW8SqeQIbjdyPEVkm2NNhXIi10IApnked0zwcrt+FyMMJTC4vOl53X2UfXzzz9TgwYN7K05JAgpUMdmghDgmR8ri+MyJCt4SihoizCn5wMtNWsFOsOLriFE2YXjegnNFJyCnnrqqWox5pUrV6rjzzzzjDouCKlmRwassCLfKYKQfbMC/ZUALwlRNpGQu4WBAwdScXExffnll7R9+z9edTds2EA33nijF2kUMggv+o7dGdAhedXtJPzeEZWWEAByAv7C9srdgtU90tf0wxbHg11+rglWkyZNoocffpgmT55M+fn5keP77ruvErQEIRaZ2YyCmTFOhS4RtQSneF1nRNOb/kzyQ7/QpHZ5CrRgtXjxYurTp0+Z45UqVaL169e7lS4hS8lWRYm8HwQhfUvapKv9lc9LwbRAwf+CVWlpKf34449ljsO+qkmTJm6lS8gC6pQr252Fs/SLNVOeQ3CHoFeHVKXfLeN1t3ip+17/jnYYXC+POjatQpe1bkx+wRv5LkRe4jeZ1HG9xKLLY8aMoU8//VTNDPztt99oypQpNG7cOBoxYoQ3qRQyklGdnXVCVlzQuZCqlgvRDR2aUzYKVg9d2MP1jshvHZUgBMHdQvG/HuftUpQborHHt6aTGtb2jY1VquPLRBwvq3r55ZfTnj17qH///rRlyxY1LFhYWKgEq9GjR3uTSkGIQZdaedSlRx7V+i64C4Mn8+VdXGjdm0snKGQyfltjMxmCIlj58aMrFFTP6wy0VFdeeSVdcsklakhw06ZN1KZNGypf3l/GY0IwcV759zYpn328OsNnafdj55lNBD3/F/+9h5qaHG9dmkPfrd7jyyVt3CCZ1JQUePMs/sqhaALdZ3vxoQxv6xUqVKDatWuLUCUE/oVR0aNOzS4pd7jr14IQMoIlm8yFp5BPF2FOtPkNamwQH5NIT/1qOXRO03o0oa2ZSOott57TxVPNTsjj7s1OklPZxToWrHbt2kUTJkxQswAbNWqkNvy+6qqraOfOnQkl4oEHHlDxYGHnHj160GeffRYz/Msvv0ytWrVS4du3b68WgNbhNQ2N22233RYJg/sZz998880JpV/wvoHk2WgVybSbS7q5Y++VKIWOdcdeI5KX4IHXIpdfbuleAaQkz92GO6J5Azq2fq2U51HNKkW2W74xNix1JyQpWMGO6pFHHqFbb72V5s+frzb8fuyxx+iCCy5wGh29+OKLdNFFF9HEiROVH6yOHTsqB6Rr1qwxDf/JJ5/QkCFDaPjw4ereRx99tNq++eabSJhVq1ZFbY8//riqXMcdd1xUXNddd11UOLERSz9WTbRxuWLT46EMmV20X0tvJCvLPjVOxvmxr2xZP7g2dNlGqtxBuuVuwS2hJQgjW25p+dwgRJmJ4/fJc889R08++SSde+651KFDB7XhNwQrnHPKnXfeqWYaDhs2TNlqwfloSUmJEobMuOeee+iQQw5RNl6tW7em66+/nrp06UL3339/lEsIfXvttdfogAMOKOMOAkOZerhy5co5Tr/gpx45mM10/6a5VKD7s0kFCQhOPpS1BJ9iJZhn2gw1o1yX7vQwseTNC1o0TGVSshLHghVmAGIYzUjjxo2V3ZUTduzYQfPmzaMBAwbsTVBOjtqfM2eO6TU4rocH0HBZhf/9999p+vTpSsNlBEN/1apVo86dO6thQgxzWoGlezZu3Bi1CckRSsXL2y89XbCTKAiu4LaCKd1rBfrNeN4MYwpPa1SHbtrPfAQgqIQo4ILVqFGjlJaI1wgE+H3DDTeoc074448/aPfu3VSrVvSYMvZXr15teg2OOwn/1FNPKc3UscceG3Ucw5YvvPACffDBB0rjhnUOL730Usu03nTTTcqWjLf69es7eFIh2RXm7RxPpnGlVRvjt15BEAJiY+WWu4WahYlFFPKZzZcdkMba5dNt/OAuftOm2zLsMAolM2bMoHr16il7KLBgwQKlfYJvK7+BIcVTTjlFGbrrwK6LwXAmtG0QsCBAQStnZPz48VHXQGMlwpXLhIgqOFN6Bq/FmRBKQ9ziIFTIBBsrNxZh7twol06un0fPLLcesbCLH8WqCvnOvJZK20+RYAUNjY7RCDxRAaN69eqUm5urhut0sA+bJzNw3G742bNnq7UNYSAfD8xGxFDgsmXLqGXLlmXOQ9gyE7iEfzigTR59sGhX8hqrUIh61c6lOat2O7/ej71aQPHjTJ+sK98AP2+QNFbD+hZSwe+hxIYCfVxGN7QpoP9VqE5tKmaHO6SHBxfRqJe30S733KR5K1g98cQTntwcWqJ99tmHZs6cqWb2AXh1x77VsGKvXr3U+QsvvDBy7L333lPHjcCgHvGzZi0WX331lbLvqlmzJgUFP737Sit5q1q2smVwq2NLZ1b6uXMWhGT6Jq/7KL8taeOn1JzSIJ/qdmxEoQX+fq8cXDOX3l3j/EPaSI9GufTK8GI6evJWSjdp956D4bWhQ4dS165dqXv37nT33XfT5s2b1SxBcPrpp1PdunXVEB3AOoV9+/alO+64gw477DBlJ/XFF18oFxA6GKqDvyuEMwJDd6x1iJmCsL/C/tixY+nUU0+lKlWqpOjJMwwXepRY9glWp4JgPBqP4D9BasimfPLRN5O/NVaUbrx1t5CXS7QreZnDlCaVKtNPG9ZTn+oOFzhMgFCMc3d2KKS7luygx35JfijWL6RdsBo8eDCtXbuWrr76amWA3qlTJ3rnnXciBurLly9XmiSmd+/eyq0DHJJeccUV1Lx5c5o2bRq1a9cuKl4IXBjOgM8rIxjSw/lrrrlGGd5jRiMEK92GKgj46mMtgTeB684CKZh4WY6+qiPJklEPk9mEU7S2XxCMxf3Khft0pXWrPqKja+fRV0l05sl+BJTPC9HhtfNEsHIbDPtZDf19+OGHZY6dcMIJaovFOeecozYz4Pdq7ty55CdKCoi27KCsRvpI/xB0jYngTzKtiZedFfjP/xuPKKQHP9hBv24K+zafKxYU0pEN88scT2WKQ5lWIXyjSRUSrsx+srHymlCiYTK04SY9KzBO3Qn7UbTSEj2oU9kXguAjLCpYpr5IGX68Q9vk0YX7pHeZrFSR4UWaesFq27ZtyVwuZBBhj9X6RbnyDZBu0i1q7Ul3AgTbcFE1LJfZwkUoE4UWh+3MF80y7ItURHD8tsKsPTgIhUF5+fLl6aefflLHsTAzZuEJQqL1P1YnUpxrw8DSTDDzV3vzTecZN1sCkG+Cf+HqM7JFg6zVbmTTswpJClaTJk1SawVi4WV9CRsYjz/66KNOoxMyhJCLkZi904stNFaZMLSQCc+QaqG8SEYCXQMOMkc1ic7QZ3u1TypOLqryeXmZXdd9ulZg2MNrvXjGUNIR+CXnExSsnn76aeXaAN7M4dyTga+o77//3u30CQGpS64MBSapsfJRdjgiqOlOJwe1z2zJKpV1YswhRdTcsMRJ20oVqH2dHA8WYZba7vc+3nlf7h/1dtgnSXHcclauXEnNmjUzHSLcuXOnW+kSAlSJ3O4ozIKUwKFLhnoa95OAzPizau1NVXGBDzMtw+tEOp6rsg35eZ/66bO/NAqKfik3L7szPeqrexYlZT8boszEcY1s06aNWirGyCuvvEKdO3d2K11CVhqvW8fVqXIF82ssfguZlydB/JBIlKBrdtxyEHp9m/jLiF1/uEtLjblQv5wuIh90mlTO9UfWh/+JIRxUP1Zw5AlP6dBcQUs1depUtR4fhgjffPNNb1KZgeSGiHaHs++lEfs5y3Y/C/qX0MadYVq8rSCrOiyhLFnSRDKCVJZV7Yo51LleDs3/NbFF4lLRh+SGQrQrhZ28p06Hyb+EwwHVWB111FH0xhtv0IwZM6hcuXJK0Pruu+/UsYMOOsibVGYgJ9b1hW9WXzU2szgq5Yeofon3qv6MXSsw5LwD6lzT+yUuEsInnWYq8PPLyw5W9cup5/VQClxxuFmtQjEEq2wUMOLyb7a4lTuxHrtPq9S9cxO60/77768WPhYExpV2HLI3NNKydg4tXrUnYxyE+i2Jx7cooB/W+a9n9qXTUo9I9VItbt/OciiQvGFPYsoq1/NN39eFPYxQBIV8t9cd8oDbjy6kcdO2GzI+HPNFVKtSDtFflBIcqwI+//xztYCxERzDYsiCYBcnzVfvsEYPLLJvixKAd7Hf1Pb/dlG++wpO9/1Tif9fbd4/WJ6DsLpZhRucfVjZCVpOCadRY5UMBXEU1ul+lF6Ncql/yzxfdw6OBauRI0fSihUryhyHzRXOCUKiJNteZUHWslQvFo/1QSQoVXlMl65Uv1oOnX2guQ2kESfKkPf2K7adD2575d+3XU1qXLu881mB2u/dmhYtLwBaICY/TpdhlGFSLdOEA/C17LjXXbRokVrE2AhmBOKcIHghGEXN/gtOH2ULLx/nzHYF1LVWKTWtZb+ppyt7i+29m7OCoMwKbFKpMl1/QjG1qZvr6lBg3aIQNS5nv87u9sl6R6FM0FglIQQ+1LUNpZNwUAWrwsJC+v3338scX7VqFeUZvOwK1hjbmVHqr1EYnIboFgn1PaHg21h5SZWiHBrVeR9qWdu+QXq63gHD+xUGotPMRELJDtUb9lMt5+jaIad4pXGJtrEKTkeUn8TclbaV4mv5zHArewIrWB188ME0fvx42rBhQ+TY+vXr6YorrpBZgSnGL5XILUJefMHbzKTy+enr+PzY5wZB3Z7J5ITSr7PyYlmUUApmBQ7u4o8P/D2axJZqwSqZkcd4Q4FllAIxztn17+WWcOsXUyvHgtXtt9+ubKwaNmxIBxxwgNoaN25Mq1evpjvuuMObVGYh6e5UHRMOdj5VK86h4f0K6OFOLjkbDHhpp6ODCgel1xTK1tw4GniLYK6hG69fflDq2rDxeaxnBSb25IlcdW6HTpSXwDTEQ7rVUf9PaJH4mLzV50DIxZIPQjfgWLSvW7cuff311zRlyhRasGABFRcX07Bhw2jIkCGUn5/Z63e5ibGaGetKOJCvY2eNokxfE8vdQrwhPxfYv1U+9VuGX9o03izUWPksORF8YkaTEtKvr/ImvcnUdaxidGzdPHrh112+cbcQi4opNudoUi1EL55ZTN8urEtESxxff9KBjejY/etTwaJvHF3np9Uvwj7RtCekM4Vj0HPOOcf91Ai+qaBeEatjTfZl4jcBxc/E+upLVz5K8aWvDBK2sQo5S6/dxwqbhL+nYyEt2bQnJe4W/klE7Ej3rVO3zIQbfa9qUY7SgP/Uqhk9//pS8pqq5UJJzz4sSMDAys5QoJvELBV/yFWJCVZLliyhDz74gNasWaOWtdGBJ3YhPmb178jaufT6qt2W531ah9wjptCVWu1VKvHjM/jlyy9oQwCCuzZWUdfGOJeMxuqfuh5yXOPPat+RZv26ImYaDynNow8bVqJsIZSm/iRs+B84wWry5Mk0YsQIql69OpWWlkZJ7PgtglXi3NOhkF5ftcVXFcQ/xuuZi9+ezW/pyUaC7pMt7MX1IX8NE6eijJzcwg8fHomOOoTcmhXogzxISLCaNGkS3XDDDXTZZZd5k6Iswawi6Q3VJ/XDN9hpeKaNOgDvJ1++Q00qYLrrpB+1aF7hxyoRq82FUrBWYKxLdyfzRg1bxJ9Aw/RlW04hAfKD6q9ZgX/99RedcMIJ3qRGiGCnfvqpDtt96blt2xP0L3s/4tsszR65KuXG68mWufH6gbVyUzoUmIwfK8vqZUNYczD/JqubR8jjnAkFXbCCUPXuu+96k5osJpEPLt++ABMmuZlFoaD1Ni5yR+eWrua2H7PNj2nyilS37b7Vc6moIJe6Va3oSnyjmhYkZ7wedpYnfpwVqBNOwRiVH4bB0j2bNUwBHQps1qwZTZgwgebOnUvt27cv42LhggsuoGylfaXydEqjOnT5gh/ihg1lmMbKLrE6x/yYq64G8WlT9xLtXb2yax1QyEcdVBS+TJQ3pLq2l88L0f0XdKMuC53d2Sp0QY5Ldd0wUcXqejdmBYZcuMjaeDsxgvbxbDkUGLI4/O/xOkX2dTxmPsEixuvhgApWjzzyCJUvX55mzZqlNuOwTDYLVmc1rUdNy5ckdG0ifqwC1+riVPxRR7ckWm0+LdlOew1ebgQ73akmq2ysUtS229bd+0LLy81JuEsxXmcdjb0bREpa93sXI/w+9XPp4592U7VyieebG7UrlOU2S4kmt0ZhSD1rvEkItSuG6LKD/L+oqGPB6ueff/YmJVlGKAsbXSwuHdyGGtYqT7Ta/LzDD6FA4cY7NJ4K3sk9/Cqv++VrNJMY2NEbp86Ww/U26xYX9W/b9hb6jhjDfdcdVkgvfrmTjmjn3IOQk2qVqDY3FRqrqiXpb7hKudK5kLbtbkWPfL3A0bXN6lagH379O2aYt0aUBKKPcGxjJVjjZpmmv4n4C1sdjGSa7Q4n5iQCz1MjxMMPZeBIGPfohbNDU2HsilFnq5SE6Lz9CqhuZW9facaP2TIfNC4XnJPo0qXJMdqPdamVR73r1Ev6w7g4L1jtJWkHob/++iu9/vrrtHz5ctqxY0fUuTvvvJOylX/czKUOX82I88mXQhl8lEVe4n5V8GuBWjOkXh7VK1+fbvs++Fr1dBsBJ/v1b/kidaix0m2ndnrlrMpBtCr9MT9KQq7mZY6S5OxdXKO8f/Uk4QSu6VM3jwZUyKHZFWvS65/8aiv+cFAFq5kzZ9KRRx5JTZo0oe+//57atWtHy5YtU1Jrly5dvEllBuJGt5n+rtc5RQXJp9/R149fWprPyjH20kKJvQgmnNaern9mIXlFvDTd1K6QVu6qmRmCVUAaN3/c2bexcsZ2bfgvlsYqVeTmEO3ak8AsxwQ7Ij9Wg1AK69YJ9fLph9wi29cEdihw/PjxNG7cOFq4cCEVFRXRq6++SitWrKC+fftmvX+rpArVcG01rDgapM7XZlr2aZxHh5Xm0iWtGjmM3k8P6zKu2Fg5w4sOqFxRQgpw2/ikz0wJQa/tyWqsmG3/rPAVsbFKd77ES382GK8nNNEqRSMzfukjHAtW3333HZ1++unqd15eHm3dulXNErzuuuvolltu8SKNwSKUXLiHBhfRyE5dqE6RjYj8JFmF7X/xPdCpiE5uVCclzxKEmWQ+KsV/CJkXp/9zMnPAyyScbs1lEhMeHPmaiyH4b7VpY5Uq+jWLvUix292Yn7p4p5S3r2hy7WMvsBqrcuXKReyqateuTUuX7p0e/8cff1B2E076yp6Ncqlbae3Afs24gRPngFFrVZp02z5pZ77rPGP7sfJrxQpCabpIBj6u07q+f7W9gszONDsB/c9JRXRIm7yUFmUQ+nirJF59THHcMKnguvbN6Ob9+6b0no5rSc+ePenjjz+m1q1b06BBg+jiiy9Ww4JTp05V54TsxG7H4bZdVSa8nEK+nOHlv4zzy9doKshJkbQd8iie5N2j/FPYB9XUBCuPKoDdWLs3zKXZS3fFHKZyu9T8qLGym6SalXS9TeIqzGTz4Ii6NWkBlSdfC1aY9bdp0yb1+9prr1W/X3zxRWrevHlWzwjcOyswdS3Bh23OFdxyUugnKucTrd9JwcBiKFBILakpg9Q2GrsT+8Imggs0VrkpTG7Fcphps9mRBsnqdKIyoZ/7NO/raZiCimPBCrMB9WHBhx9+2O00ZSQNK+bQLxv3uDsrMA2trn31XMrPqU5frvnds3s46YRCAWmaj3YpouM/3ZY2B6Fu5qnldR5Xx2zSWKE8g/C8IYc2Vpt3JP5Qzcvn0E+b3R8PtErR0IOb0EUPzYvjx8pbgjAU6BXhFF3jCxsrCFZ//vlnmePr16+PErqc8MADD1CjRo3ULMMePXrQZ599FjP8yy+/TK1atVLhsV7hW2+9FXX+jDPOUEKHvh1yyCFRYdatW0ennHIKVaxYkSpXrkzDhw+PaOLcZmSnQhq7T2HUMXcEK0opp7QqoIu7FlE5w/qQqSBx/6D+aGpVbczy9Ash/2Rb4CYiuMU/bdv75/WqD7GKdtN2e9frQuWbvYtoUpsCOrRWbkr1a1UrRvfZdj5mLTVWlLkaq2QIxTr5b6aFHNQXv3yMOBas4LNq925tDuy/bN++nVauXOk4ARhGvOiii2jixIn05ZdfUseOHWngwIG0Zs0a0/CffPIJDRkyRAlC8+fPp6OPPlpt33zzTVQ4CFKrVq2KbM8//3zUeQhV3377Lb333nv05ptv0kcffUTnnHMOeUG30jwqlx/8FlKaxDpc8QjF6Uz8a1Cd/BCIKxqrYGePYMC+W8j04rTe7UhgteR2FXPp1Ab5nmvo7aQs7lBgKPP9WKWKPQHOBdtDgfC0zvzf//0fVapUKbIPQQuOQ6F1cgrsss4++2waNmyY2sfQ4vTp0+nxxx+nyy+/vEz4e+65RwlNl1xyidq//vrrlXB0//33Rw1LFhYWUmlpqaXLiHfeeYc+//xz6tq1qzp23333KWP822+/nerUMbgCsAmk5eSNNv1n4Mqk6namXx0J2j4G4uWUjpuGA5hvvkyUVwRjKJDxykGo30j10Jx8MDkjHDTBClohgK+GoUOHRp3Lz89XQtUdd9zh6OZw2zBv3jzldJTJycmhAQMG0Jw5c0yvwXFouHSg4Zo2bVrUsQ8//JBq1qxJVapUoQMPPJAmTZpE1apVi8SB4T8WqgDuiXt/+umndMwxx5hq5LAxGzduTLhBDqhR1hdKQp1oyhtdyPcV2q94tRqHV525H4fdgpSHfnuhdq6ZS11qtqXHvvk6+j4O46laFKJ12sLIVvEka8TtdVGf2rotPfvdt47zomWtfwZ5ivPyUiL4/PPxnB0VP2zR3u3kMYfxy8eIbcFqz55/FHONGzdWmp7q1asnfXP4vYK2q1atWlHHsY/lcsxYvXq1aXgcZ6DROvbYY1Va4WfriiuuoEMPPVQJVLm5uSoshC4dODutWrVqVDw6N910k5oFmbhvoL0cWzeP5v1VdjjV719Pbt/PdWd6Zn6sfNLQvHbBE0pFXqc9L9OegMAar4/pAm+N9csIVrEqjr0qEwq+poXtc2wErVAYogf7l1BeTh/T89ngbiFj1ZHpnBX4888/mxquQwPkF0466aTIbxi3d+jQgZo2baq0WP37908oTmjVdE0ZNFb169dPyJElJHFjuKCPwHudEisbrPgdjz9exntivCXTUYqxNFK+NV73YZq8wg96Cgcj8knFyRzUsBG998uyuOHSTYmyl811lCMJL8Lsny4+5YSdhA268TqWrYHBOYP1AaHpqVu3Li1YsMBRXNB6QYP0++/RU/exb2UfheNOwgPMVsS9fvzxx0gcRuP4Xbt2qZmCVvHAZgszCPXNiN3hkz0B/ZoJueEgNFEfVRRsYpW5O+4W3MOretWoouPuJmtJVdvOy0mtjVWsF1+rqv+YaoCGJd7WlfLazGY338XWxuvuxpdWwu49R2GMSV1cV5xkgU/kKueCFQzEWVMDo/EZM2YoQ3AMtbFBuV0KCgpon332UYbv+pAj9nv16mV6DY7r4TkdVuHBr7/+qlxEYAkejgNaNth3Me+//766N9w9eI2pkbtfaoRPsTMr0Kzx+iVb/fIlpRNrllXYxy/xbCAVfqz2aZxLzUpzXHFxYtfGKl5cE3r2pq61Sun+jmVdHbhFn3r1qWPNaHOSRHHjuYOusUrGHvO8/nvLucDQQURiDUAeGHHc1cEGiQUruCk48cQT6eCDD6ZLL71U2V45BcNrkydPpqeeekrN1hsxYgRt3rw5MksQCz7rxu1jxoxRghwM5WGHdc0119AXX3xBo0aNUufhiwoC3ty5c5VrCAhhRx11FDVr1kwZuQMsxwM7LMxGhM+s//3vf+p6DCEmOiPQSR3YE9BZgZ428gSnMQdFoPFcYxUnEqf5kEhnGU/4NTM+P824GHcAyjIleNy0e9epS6MHFrnXh5RxEGrhVS6m25EQNa1chUZ13ofqe6ixOr1NO8rV0udngd+XGiuX0nRKqzZUt+rezB86sEmUZ/1E2rtf+gjHVQqz7FasWKF+Q8DBbDoQDodN/VvFY/DgwcrFwdVXX02dOnWir776SsXLBurLly9XfqiY3r1703PPPUePPPKI8nn1yiuvqBmB7dq1U+cxtPj111/TkUceSS1atFD+rqAVmz17thrOY6ZMmaKcjMLmCm4W9ttvPxVnuu1tAt/oXKBacU5CL3Oz7PDL7LYgzWgL+TQPApSFSZPjscZKFyySwWksYZ/0eQW5uXRU0+Y0qFM+VS6XuGRlezZkola0GdrHm1GrSjE9NrBc2WWNyD71q4SCabyO2XYnn3yyWhsQw2sYAgRw1gmtUCJAW8QaJyMwODcCuy5sZhQXFys/W/GAXRgENDcxdoRVC/Lp9N7evWRTXYXcn8VnzhFN82ljQZi6N831fjqdHxyEkr/wq2CVXaKVt0/rtrY7VXU4EWHzsLa5NP3b3WXSekzzFkSNfnMvcS64mQjiUGDlQsw49YcmqU7Uws/pw3Eq7rrrLiUEtWnTRtk2lS//z6rR0Cqdf/75XqQxYOxtCZM6NKehPbCQpzuLNZcvNsjBaTJe99oDcnFeiM46oJA6NHAm95ulquhffzPpJqYrjhR8ljqahepRcpLtW8889J8Pt371ypZprzplfcMFmX3r1vU0/kQ1VnZmPcci5lAgeYWH7cvjthsr+nrlQ1SxoOz7xWuMSTqxRSvap0VVuqdLK1dbfzjBDqN+tfQLV47fOnAGOm7cuDLHx44dS9mO3XrglruFVC/zEvNu4fTc2+r4mH75VLIll15c7A/BSjRWiQ+BN66YQ5efUESF1avQR/2709K86OWrwLkd4n81B4maJeVouYfaWteEeYfxJKu58NIe0Q2zAbflLCsR4fozO1L9336ky2cTbdyxg1KJMZfKFxTQ6GNaUceYTgFCid8ngOOheXaXs8GQH4QqfWkbM2DbJMQbb0+M8sX5tGnrrr3xp3wsMMX3029t8bDRPq327nSqm0uhP/2jxYitsSLf4fQFWDnfu6HA/Ny907Ir5OcFft1Iu3j5rZIbSv9XfSbg9azAUBDGAm0QSmDGthN3C+WKIMrsfTcGQrDCcjbsrZyXtjEDL7ZEDNgzhX+G+Nx/wdzZvoBWbgvT//LL0ep1W1M6hKTjxt0shxJcvrfdsjCjSUmIftoSDrTx+oEmyyYlS6xHeKNXMZkvQhU7D3QtgaUPoOwyrdpLnOcurRyi1eutA1UuDNH67WFfzCj2y5B4LLwU2J1W4cbVQvTzn2Hq1SyXvlkR/HdqbihEh3TMo61UjWYtiPZD6UaeDTukKREtJr9g67MF/p14CRj8ttqyWahyglMN/7F182l004KyX0c+chCaSff+b69ievDEQqpVUuJanKmUDdCBJeMHKJF6ZWd6fPfSPKqQb20TkrUClAXxsuOyw4tSb2MVZ9/zWYHkHYm5GLEbtzOePq2Yrjy6iHq3sNB9JOA80y0SvedJvQr/FYDcL48alf1lCiD6YBeBywkdq74rYe2FIcKUNyofaqVDHnSWlfJD1Ktxnqtf0LHKHF+n4JT67tiDtauXRyV5iac9lKi9SZxbQlt/e78DHUebtfKWhw+ek+AQk5cfc6nqXpzcJ9dhPllrXZ0VZvnCEDUvzU27Fs9LwjYDOM0Cp3ntBbZ68nvvvdd2hBdccAFlM3YqQaKCVZmoXWxzmElxeOd86rcxRIM/2+aaFI44163fQ5/87J420wftxjEtK5jn3gV98+nwdnm0cCPRDW0L6cuccvTdLxvoiNJcemP1bqpSQvTXluTvf1D7PJo+f2da8zCZ4VnBXexorLx4p1cuTi7SDpUwxL23HsejTqVQUs+Rlxui3TFnnkiN9oowBZc8uy4WdNauXUtbtmyJLLyM5WFKSkrUcGE2ClYn1s2j2Vvz6YBaVWnTLt1fSshyKDBuczQJUKNSoWc2AV0b51KPZnlU4evdrjaCquVDdHa3Ajrqkb22YYkQ9O6rRmGI3ji3mI74T3Q+DOsZPTR2yeA2tHVeBTo492MaVLqbGvXKoUMfcpZ3Zn19ZQeerJXGKkW9mp37nNI69VPK/UC6/ViZlU2y7RBxHtwql979PrF+Zt9qydsOOtECxdNY7TCYv1iFvrBlI7r+26XkFkEWOuySaB/0T/mmN4ds9bY///xzZLvhhhuUh3QsP4NFi7Hhd5cuXej666+nbOTW9oV041mdqCjXXqMPJxhoUM+6dHz9WjSua6FnH0uxOp20LhasXWgYcA2MVqte5RxbL7xy+fmUnxOiQ0vzqFq5BKYp+2BKu1vAvUJjpaXIPtItWJnhhmBlHbe92IscqM6bJOnTKC839vVbdkZrz6ye4Nj67qxNaOTgho3V/wM8mKySbsL//g/icKjjWjdhwgS67777qGXLlpFj+A2t1lVXXUXZilnhu+2FtzA/l65s25TaVf9H0ehmdcv7t116XYWtHj1e29E7XTeEpktaNaL/nlVM2Uq65M5Y5Wx2Li+AnaprpHlJG9MghmOO7V8onBLBfcrQIjqmQx5NPDQ5bWc8jVW6l8zqV78BXX9CMT3S2btFq9NF+N//daoVZ75gBQ/ru3aV9ReBGYG//25vGmUmY8vdAjoXN+7lYg+VZ8NIM5GvXFxi76rYodyefXRyozrUyAceer0h+c4+HS8MU4HZYcFnkhjmrcYqsbqfSP6+2adLVBkn+2Fk5/I2pbl09aGFVL18cm28S4uq6n+dYnPBxfgsKXctGAop+1houDON8L95W1q1mCZ3a0uvDg+OgOW41mHR4nPPPZe+/PLLyLF58+bRiBEjIgsyC/9gVdVdWyvQxbaU/6+1XShNWqp4z1K1cK8HykLNMjCbFRqg2IMle1KZpZlYfuVcGpVJRABpX9Fel47h5kTun0hx1S0pctVRbipF/hP7NaQzBjalJ3q0t0iLvZngbpNOUwf3HjFsO2TXapWoSfWczJoVqPP444/T0KFDqWvXrsoTO4AGa+DAgfToo496kcaMo35xDi3atSfp2utmI7YzFBjrnN26bBVHvEcpyMmhB/qXUKhB4lPF3aR2USGt2rbd8XVumlVe0r+AmoT6EdEmd30FhUIpe3nFqzfpL+lgkR/n3XNam7b0/bp11LN2HSL60fkNjEOBDksI5Z10v+WwciZzO5hf9OtUi2paLNeS/ld45hKm4OJYsKpRowa99dZbtGTJEmW0Dlq1akUtWrTwIn2BIxSjRV/WrSctXf8XDSpdQYv+3pN0rWpSuwKlcigwkQ4qZFt7FT/2cljWpNAfL95j6tek1vVb0Uu/fUA1a9ahVz9anvI0wDC3eKONtWRcIl6VtFMWTl/EQVy+xq0UezEc279BI7XZwaxJJvts0Nb7QKFgSh0nVvFpHwpMXyam6rs27NN6YoeExxGaN2+uNsGaHEMza12tmtpy6NcyYfdp7Hz8oF3jynR7p5b0+so19NHav9wZCkxUZeX+Ze40So8aJ14QFQoKaHi/QvpuvX0BF3Zqu13qMaxevK7ktxf5Fk87FTwZyteSbIUC6yVtEsUNwco6bnuxJ/NEd/WzttM5rDSXlmzKp7zOzWzHV2YokDKfbqV59PbqPNq3uCIR/Z1wPOE4BelGzb3t5GKidZRyMtV61xcUxpiqqzfAM9oW0FkHJKaK6V9ajeprNgyJgoVu4+FlpxG0l+oerVdI15i+1W3DrnRq/vhcDFq9cJNES+C+joV0WNs8OsAlT/5uEmso0CsbKz3aKjG0Uvjouah5AXVoWsX3WpV0anMKckM06cxOdHmbJr5/xho2bQ7dRgQrt9F6h0KbOtN+9fOpuCC9b5Aa/3oG90Bh5ekLNBFHq26wJ8HJCE5ncmaTYOGFQ8p04FaZJfpiOaJ2Hk06vDCuvVU6ZiKHAz7EE994PYg11p+EE7zOD2UggpWHwODaCjeL3o2hpbpV41cFrq9upT0UYFsaXUsVc8kLA8k85dhDC+nKo+JrJzPpxSX4G6f12Y0Z0X6q3qloa7e0K6Durap5fyPBNUSwSpNgFRcHDTbZtl1DW78rXeKNZx8ZHnV8u8Pmw4Jefk1VLhei5rUzz8NypuGe8br/CHk5FOggDr+QiiHzwfXy6fyj9jrkzhbCCWatH9wtJPTmnz17Np166qnUq1cvWrlypTr2zDPP0Mcff+x2+oKHVqgFsWysXBQknLzY46WlIEaN8IGXgzIkkqQe1Sq52qHuSZHGqmwaMp+gaTLdxAfvB9f7rdhDgcEraz+WUaawh4KLY8Hq1VdfVT6riouLaf78+bR9+z++fDZs2EA33ngjZTs7tJdsYTIaq1DqKqB+qzyHSYbhfMc461QV5NrwY5WwkZWz6/rUq0+3dWrpaofqTLAKBaJTT8cLwwemEYJDzNptjQohXw0FelmvsIoGUxTH88m9+7T2LiEBJJx0AP/i+M0/adIkevjhh2ny5MkRB6Fg3333jfLGnq3odaEgRSoeN1+CsdZmMzszbf/OdGGX2OtU1aqYQwc1bESHN2lKebmhlHR+VnnSu05dqsC+JVzqUJ0Zrzu7T6BkjTiJDWeJsJVu43UvsfNo/duVlTBa1vznVXNYm7yUzwr0En3YqVuT2P3K/jWqUL+a/yyRI8THLzOTE8HxG2bx4sXUp0+fMscrVapE69evp2yndnEhHVe/FlXIy6P8FBmv6y/5RNDTAh+c8cOHoqYox9LCcGd5Suu2/x5ZHTcNbtKttDb9vHGD6/HqwlSyQ7F+YmDDf7oEsydy4yljOYzNlGwMoCzoudD46MlFtGj1Htqnfg59/cZu87gpeDjVwLlmfxewthIKZOmmUGNVWlpKP/5YdikE2Fc1aeKtX4ugcFXbpjSmZcOU3S+RRnZTR81TvlbnY03R5k7Vky8Jj9wtDGzUmMYc4v7K77ow5WQoMJGFrFNZZ05sWUB+wh+5lR78+O5MtDzKF4aoe8NcyvWjoWYS6H2hnfLySfP3CeGYZ91aUzcQgtXZZ59NY8aMoU8//VSNr//22280ZcoUGjdunFqIWUg9TjUmFYqIBpZWM60EsWysYq4VSO56qbdL64rlYp7Pzcmhzo3cd5SoP68TwcqrNEQddyU57j+TWYzxSt2L99C4A70VHrNVK2EXa+EieFJHJmmrBfdw/Ma5/PLLac+ePdS/f3/asmWLGhYsLCxUgtXo0aNdTFpmc3hpHt27dCc1Lme9xIJXxutNauVGGZ3W+9c5aLyhwL1+rDzoAB1G+cCY7tR0wW6qVlhgskCQ91/7URqrsPcCZKpfO+l4XaTqa75qOW9vlE7xIJTi+EMJpMHCzDKQ6HKVHRkrgx7dBUIxz2aVxgov5CuvvJLWrVtH33zzDc2dO5fWrl1L119/vTcpzFBaVMih/+vXlV7ct2PyfqwS/Gp6/owi6t8gj05rU2hrqMpLr+xOry9XlEe1itwf4ktEmHXiIDSZjApwP2M5iyzucngeSFud6on7vkRxozhG9S2gqkUhOqFF6hYQzzSNVdAMuwtzzWaOx36GioXBFUMT7mEKCgqoTZs21L17dypfvry7qcoSahYVxDRwt8sx9Ws5Cs/VtVWtXCVUYbHWZEm6madoVqBbNk7RGiv7Tz+kVRtH94mVXC/69IgdXbD6bUfUrphD5xyYPqHcLplaBsj/O/oW02FNCgJvf1S1yNk6rdlmxD3yqBbUuGqIRnTqbPuaMV0KqX31XDqllb/sPZ3g+K2+efNmmjBhAvXu3ZuaNWumDNb1TXAJB+2va9VKNP28YqpfOeTxGkzeXeO1UfcBbfKoe7VK1Kyy/QVWY6ErqcIOxmLh7oG5oUNzR/fM9C45lYJEtRi+lnzjbiFF1zgBj1ZbG0pNVKNodl0Q6/eAho0chXetmwuI0N2tVXWaenYJ1a9Q0fY1nWvm0cVdi6hyjAWzM87G6qyzzqJZs2bRaaedRrVr1/bFgofp5P/2c/bFYpf2FXNoxhrzaclm1KmU49i5p1MS+dqK9bJMZc0Z2qeQOi5rSwtcuquuinc0FJgCYTLsSpiA9Nw+FOIyuUdElYWW+9bPt1FQ8PIVVaANcdm5TybXDaeEKXNxLFi9/fbbNH36dOUQVCCq65FUfW7jfMoLER1Y00ERhdwJdmqj2vTsslWJRp/Wzg8OTneFw9SqVg4tc999lat+rPxsxJuQM0+TYy3rV6TFKzbauz6F+RGEYbYgpNEUn9XrwOZjDDLwkbJbsKpSpQpVrSreY72mKDdEI5umZ4z54laNqWZhId25eJl5h+mg47T7snTL9uCj/t1p6549VKl4EXlJojZWbs0QDErn2r5J5ZiCVRCewTmhjH4yr6halPwMab/nTbbZWGUrjtUtmP139dVXK1cLQuZiNusk0iV44JbbLW1FcV4uVS1wZ7ZRrCTpZlXVKxZ6NhQY03dYEnke99pwcvd7qXsRnXN4czqkWx3yI15qMWrEmM3UvlL5wGhaUqlB7Fe/AVUrLg72c6QwvwpjeXIWgqexuuOOO2jp0qVUq1YtatSoUdR6gUDWC0wfdtt1wusdJ3aZDyJPjEqFhbRq82bTc7qWqnvr6vTwG0syaigwWWoXhah32xrkV7yUWR7oVEgHzt6adDzp1nmZCXZeCSkdqvu3rriJW/lXr0Y5ot8okBxXvxZ9uG4N9albn4hWUCbiWLA6+uijvUmJEJxOwclQoMvhUkmbatXp+3XrTM+FDZqn0qICWr1th6fG66mcKJKOl7rbGppONWrSV2vXpOReOk3KWWsTwkGSrBIe7vKW57oV0XmLdtOwgU2JbAyc+ElhFXJpYlPQl3y7osPftPDr4LpTcF2wmjhxouuJeOCBB+i2226j1atXU8eOHem+++5T/rGsePnll5XLh2XLllHz5s3plltuoUGDBqlzO3fupKuuuoreeust+umnn9Ti0AMGDKCbb76Z6tTZOywBbdsvv/wSFe9NN92kPMtnMuXz82lwzzwiP6yXrfUyfpxdOqhxUyrd/TP9UKUazZgXvXj0uc3q06Yk34G5PnzmkEsOCM0eDcKMX57YTQeLrevm0Hcr4/vcOLFuHi38OxwYucqH1VPRu1ou3TOy8z99xgIKFG5kqU+LxRE5fq1cLpF20ffFF1+kiy66SAlsGEaEYDVw4EBas8b8S/OTTz6hIUOG0PDhw2n+/PlKg4YNXuABbL8QDwQv/J86dSotXryYjjzyyDJxXXfddbRq1arIlulL8tQrLqR7DzyISisnVuyezgok/5GXk0OXtiigVvUrRR3vVCmHmpYvSVoDkux6tG6sFXh885Z0dqM8+ujCEvrf2BLXBFy7sXSuUmHvNS5XgljP4pbGqkOdHKpcEj/h+9fNo1vaFThapiPdgpUZfnkf+vFDLJPTLTjD1hsWswD/+OOPqFmBVptT7rzzTrWw87Bhw5Qn94cffphKSkro8ccfNw1/zz330CGHHEKXXHIJtW7dWhnTd+nShe6//351Hhqq9957j0488URq2bIl9ezZU52bN28eLV++PCquChUqUGlpaWQrVy72gr5BB43a7peC6YK5NvuEyd3aBrbDNiPHIAEVmLSahDRWyYqTLrx5D2/ajK5sVUgVCkNU4oIHfsZOTBc3z6cBtfYuBu42sZZ6ckuwuvGIQlsvy5K8f9rfHkdrVZE/5zt64vHfxx2AIHgxFHjXXXcpIYR/u9UIduzYoQSe8ePHR47l5OSoobs5c+aYXoPj0HDpQMM1bdo0y/ts2LBBpbly5cpRxzE8CMGsQYMGdPLJJ9PYsWMpL8/x6Khv8LpvijlDTfvdtVoll2NPLxVLoido9Kia68rQkq1ZgWlyIpqIql5Pq53LsRB5kF+obRtVorqVd9mruQksFZRInfJDbga4SAXBFWxJEUOHDo38PuOMM9y5M5HSgu3evVvNMNTB/vfff296DeywzMLjuBnbtm2jyy67TA0fVqy4163+BRdcoDRd0LJheBHCHYYDoUEzY/v27WpjNm605/TQTyS6WDOTgO16RnTGzepWoOrlQvTH5n/yb2STfDLO+Uoka5O1sTLesmoJ0botRM1Lc4nMm4NtDmvclL5c/xsdV70mPfyj85k7Vk8GQeqIps1o844d1Ljcn/QXpYdEW8LQ7vn01Gc7E75v8DVWPm6oaUY8rwuMY2Mb2C0tXLgwsv/aa68pG6crrrhCaaD8BAzZMSQIgeKhhx6KOgetV79+/ajD/7d3JvAxXV8cP5N9k0hEEgRBIoglErva9z+lWqVoLVVbabWqVFFLtYp/Va2l/dtKUS3aqqW1axVVWylq31rElpAIEu//+d14473Jm2QmmZE3k/P9fCaZee++++767nn3nnNu5crUr18/4UYCSvNK4clUsR3LjPKneHGYijovmoKCHZ8KehasQJ+67irnrbkaMB9R3Ne22yGt6+9DM5s0Jz+v3Bemv6cnTe4bL5T0c0JWKXguKpq6xVTKcdpyfGMFOX3H8HLPoa7co/uxjpXzplkPFpP5ERdygjT17duX/v77b/EdVnedOnUSOlGw1Bs6dKhVcQUHB5OrqytduXJFdRy/ofOkBY5bEl4WqmD5B50r5WyVFjVr1qS0tDRhaagFZrSwpCh/LlzQn/8Ng4PFr3z7dfQHjjUD9ccvetPGp7xt5shUxsPNQL4mfuXyCj3U5xNbZrTiNulWNBRJh2Wsh3p1ZAp5Oq+LgbzCxeAEghWEqtjYWPEdwlSDBg3oq6++ogULFtC3335rVVweHh4UHx9PmzZtMh57+PCh+F27dm3Na3BcGR5AcFKGl4WqEydO0MaNG6lQoewVZA8cOCD0u0JCQjTPe3p6CuFM+XE0cjvQ2HqcUumQOOKrcA5nFwr5uVCkn/XvWe6ueeuZ29zt2sZrCHM6r05blp11Ola2m7LKqxkte9w32E5e158oFjSEV8qEU9NoV5rULme7NTCOMWNltaY2HgwQfgCEljZt2ojvWBqTLQetAUty0OGqVq2a8F01depUSk5OFlaCoFu3blSsWDGxFAcGDRokhDks3bVu3ZqWLVtGe/fupblz5xqFqg4dOoglyzVr1ggdLln/CvpUEOagAL97925q1KiRUMrHbyiuv/jii8LqkclmSxsryEpeUo4xOh+Hs5X7lKX1Q20vevq3VJvdu31kWbrz4D6FFUwgPfJsdQ+iTNtK5n2NWmpskev7WDVjZXnYbIPmhWRlQV5DAwzqPZ+yYERNL7qZKlHxAo73omqKJc3A182VJj+ToQJw0MF8cDlKwTeOcadFO+5TjZIujiNYQQAaP368sNzbtm2bUXfpzJkzmZTKLQFLiQkJCWL/QQhAmA1bv369MS64SMBMkkydOnXEDBmcgEKvCw5CYRFYsWJFcf7SpUv0/fffi+/yzJrMli1bhF4VZp8gkI0ZM0boVJUqVUoIVqbWho5GtoO/Hac4chu1g09Yqcq2UkBmq8Hc0C4y6tG3rep75iZSOw/KeqjOrCwbc9petXSkrGm71lj6ZZdGvclVw9t50YXrD6lScVcite9ls0QF2rav5Acc/Vlpa0zFp0YV3Kh06XLUKvgkHdXekUx/ghVmlLp27SqEmREjRlBkZKQ4/s033wihJycMHDhQfLTYulU9mIDnn39efLSAR/XsBAhYA+7atStHac3P2NoqUFlLeje7NzigonFekmfVqagIV4ONtpZRkPYwd0rL1sxY1SvnRit/f0CNolxpy4n0TOe1oqql4Qokp2jlK6u8RhdxFZ/8ir6fYPlHx8pgMFBEmB955GGNWC1YwYpOaRUogy1poIjO6BdrhJesHIRKtlqOcSBpJFvB6gnmxd+NKCmNqGq4K108mbM47JFc5QuNNY+0EB8DXU2RbD4oZ6XUmtP6eqAhGVkjRCo3786OQF8X+vytWhTveojiJmlsiqeIasxzXuR69CE9X8x2fvi0UhoVqEeNlqxhgce5cSH9keNeCMeeR48eFd/hMR2zQIwu/CKTI6LzCatsyYm7hZyyp5EPJacTufga6CLpB9UMpBXXffiUNz14WI+8vX6xaXqydHKqUV3darjTomx8VOV0xiqn7cTdzSWTvtrCWpXoutdJlRpTYX8Xal3Cfi+2q3t707e/plODcDc6dsNCBar8hoM/wxwVg8EJBCvs4Qe9KOhXyZ7Mb926JRTBobdUuHBhe6STyUO6VnOjOzcfb4djl6VAfipZDPxoeblSnjnXtKRCrTGBdnMxkJtL5kfR6lpeuWpTWepYmfzu3TqSmoRczFawepCeORKFCmj26bOB/F25YAE6SC6UINlXwFGWXskgF2pSQs6og7286c/rBmNDnMLdAjYqvnPnDh05coRu3LghPtgAGZ7I4c2cyTsGN7adjxTlADCkiSe1j8ph3JZaBeqwc6iwwiowv2Lr5dDYgq5PRHl9QGwc1Y6x7IUwLbOqE7WpaknfyEiLvUQhvXcfhqF81PatFqxgsTdr1iyxAbIMlgJnzpxJ69ats3X6GCuoFeFKnzX1IVc9LjrbcOlIjziSvpi9UFq8adXnkyiinMxYVQ8rYvHeiGkaZoEBPpmvNdhAx8o644/M53X/suJscHnnCS7kBGmCDyt3De/OOCb7t2LyDi832/Tu+CDb+JXxdjc4xUMpe6tA55esssuhLWYgJ1X0MM4i6dHdgnIpMCtjEHOnrPG8ni3O3+QYJn8sBTZu3Fg46fznn3+Mx+A7Cn6gmjRpYuv0MXlEbKA/za9ZkTY0rJaj618s70F1y7pRbEnLlnN02DdUZCcomE5kvNM5hvLzDJtWcXlq7LFoSsdwd9rTvJaYRcotWc9Y5awAlMrrrlnk5/UG2lsL2Uuu4tkpJr9i0GHjt1p5fcaMGdS2bVvhL0reiBj75sFB5+LFi+2RRiYPhauc0rSkO1FE1mFUg4wOO0duyO9Lg8raHNrUg7aeSKPGJSx73Lhbow2ehcBkrVWgtTpWLlm8KsPCsOhDNxq6/a74LSfFljNW2e1cYK82mN/btjmc6wnmOLgQOb5gBWEK28VgO5tjx46JY9C3gid2Jp9gIyHIkXSsskufqe5MupaLbifH3EDfOd5dfJ70Fh7WWAXmZCnQNQvBCm/R8M9lq/syOUfvzxbG+ZYCc+THCg+NZs2aiQ/D5BRHsgpsEOVGtO4+lQ3Tfj+S7KikbA8keyuvP4H6/G8lDxry530a38aTKC3zeXiAMoe11ePrTpT8gKh+pCv9djbd6CbCWmyqY5VHyut676t5BRdL3mAgB55F27x5s7D+g1sFUxITEykmJoZ27Nhh6/QxTowjbWlT0NtAc5u1pHfaeVkmWDnRjFWgT8b/eoWy0Zd7wptqdyjmLjyTt455/H6olFu0luqKF87IjLW180FdbxrU0pOer+pmkY6VKXJIW7aKvGphOn9nyDv0/QhzOp4vm6HHOCHGdm6GnrhghT0Ce/fuTf7+mfVuAgICqG/fvjRlyhRbp49xYtxcHOuZ5OHqarFZvt7lKmvK+6vu3tS1vAeNLJf1AywvsgzP5OZQzljhW6daHjSkU4UcCQcBngaqGuGmWv7Tagve2TzjZ8Q/dlOTn2Z8GcbWtC7tQZ/39qEGhW23jdMTF6wOHjxILVu2NHu+efPmYpsbJn+Sk0E1xMeFGldwo/6l3LNUBHZE9Khj1aukYmbHiuvC/F2oWUl38s3OlYfOBnqlEBTka6BWse4U4OuRI8FKa0ZVOWMFC9qygS407OmsvcXXLRxIjoIe6tCR4OJ68rhbMWusS8HqypUrmv6rZNzc3CghIcFW6WLyCd3qe9KwaP1N5eYWPS4FjshmxsneDkKfNMoZK3vI7UrBDRa079b0pojC2sulZQrmznapamSg1VaBuYWX/PSJHvoWkzUW9/ZixYqJrWvMcejQISpSJPe+ZxjGGdCj8rqly5jOYuWpnAU1Fay05F43M+XTODRI83hWVoEyEzt706vtylJcSO6252lZo2iurs/PPKmZN57hY6wWrP7zn//QqFGjKDU1NdO5u3fv0ujRo6lNmzaWRsc8IarmwheVNehQjshT9DhjZY/6ivAPUET6+GtejTHKbLkqRjpLBr1SwQZqXLwEjSrnQQtrVTIenxQbnWPBKjTAhWqUC861cYZWfWXnBq5MQO6EOXNJ1nfLdn64/PWPxVpfI0eOpJUrV1LZsmVp4MCBFB2d8bCBLyvsE5ienk4jRoywZ1oZncAvZo6pY2UPqoWGEdFFXb69K5cCTVUxtAQVzOh1i6lEDek63aQCNLl+I6pr2KUS0JRYYxVoD7JbCizi50Lj6niRv6cOKoNh8hEWC1ahoaG0c+dO6t+/Pw0fPpykR70ab2ItWrQQwhXCMPkTPQykekLvM1bOuhQoP5dAqYIFjd9NJ5csqZ3CPj5UlFzoppnzlsxY2Yps02smKSX8cz5rVamoK+097/j7vz6pWuJnICNjlZ1iyZIlae3atXTz5k06efKkeIhFRUVRYKDjWLow9oGXAtUEFfA0fh9WvSYRmddPzAuqlXAlSrRtnPZWpraWYG9vs4NebtprQU9PunXvHsVFQffKnNj1ZIVIe5R3nzru5O9FVL+M/szZrSELjxw25sm0ej30LSZrctRjIEhVr149J5cyjFPyetkSNO3v89SmdBkiukyVShek18qWIE//K1S+UDDphQ0N4+ls2BGqUdLVDlvMKD2v6+vxb7pqh83Bl/xKVNjb+nROrNeQEgtvp5BQP3JmB7te7gbqUdPxLXZ71fagnWfSqVqwfQTEMqEudOrKQ7HpPN22yy0YB8OxX0UYRif0LB1OJYskUJAXdA8vi4Hu5dLhdJCuk54I8fKkkJK5U2pWYdDXrKW5JJjKHYX9XWhT4+p0xu2I1ffwdHOjkADbToMU9DTQrXsZqW+iZYWog7J1VODDbFVvH7vtVfluOy9KvkfkDyGdBSuGBav8xYCoEnaLO6ygvmYo8oJC3hhsuRz0giEbP1ZBHu50IQ/qq2dFD9p5yY880m/Rn0kZOkw+bkQTuvrQoWvh9GJKIJ3QgfsMxjKga+f/eNWZYSx3t8A4Pi2KZCxJHdh1gIIaBVFi4h2bxV07yo3ejnKnr2uoPU83KtWIfvzxF5vdh9EvepixUm6MrLTm05Nn/wbh7jS8Zm0qoHitDfQykLeHgWqWDyYft8wzioEFMi/JhQcZqGFIEDUszu/HDKMnuEfaiX8S/qGxc8fSxj82UHJyqnCwOmzY1xQdXY0epKXRyP/9j37YfZDO/XuOvH29Ka5+HJXt0oe8KUP4OXv5Mr2/aBFt3r+fLt+4QUWDg+mprl1pUrO3iMw7wBfcu3yKbu78mlIvHKEL91PIpUAh8ipeiU7GhBAVt49uHN6eB5TJePjnnTovk5coPa/nFX5ubvRsVFmSQu+Sn8djYURHcpURZWm9XPGxsYMWYUHeNLGtJ934t6rxGJabP4krRwfJTmtcZtBhUeYruPz1DwtWduDW7VvU6rVW9FTVp+jrrz+ihISCdPHiRfLzy7CeTElNpX0nTtCQl4ZQxTIVae+dvTR97nRqO2IE/dxkjghz7Px54b17zuDBFAmv92fOUI9PP6UC51KoQ/8OZu/922+/0aVF75F3qTgKfnoIxZaMpEP/XqKUY7/Qh/M+pEGjVz2xcmDyF4F+WQsHT4q2ZaKIqtwkyBt9Itxp7tkH9EZD/SlhKz1yZCwjZ03z8m508H6hPH91KV/IlSL8XSi8AC94MIwWLFjZgU+XfkrFQorRzGEziaocFEqTGdv9wGKMKMDPj37+73/pJlURv1MohQaNHUT9n+lPFy9eEcda1qghPjKlixaljmlptGbOfLOCVUpKKk2cOJFCY6qTV6sRVMzLQL7eBcjT4EeeRaPpk6di6Kwi/IEDx2nYsLl07tw5ioyJpKGThhK5ZuhhDfhoACXeSaTF4xcbw8+YMUO42Yjvk3Hs8lfv0Dt/16FEj0Rau34t+bi4Ub+2bWlQj0/Mls38T+ZT58VraMWk70RxrP5yNfWe8w2du3aN/Pz9qVL58jR27NhclT+TN3pMWK4aWt2LvIqLRq0L3i3nQfHt46iGx2E6aGP3Erkl7+f3cr7cOqYOKxUxjDn4lcMOrNu5jmKjY6nHmB5Utmx76t27N61ZsybLa5JvJ4upfX9/8ybcybdvU2AB8z7DNm/+nRITE+mtd7pQ++rutLymWt8pwE+x/QgRjR//P+Hw9bPPPiNXV1eaNGwSWcvSn5aSt7c3zVo1iyb17UvjFi2iLXu3aPrcmTbtNfpp5U+0Y9o0iikTQ8ePH6fpY6fTuJ49adGePTRxxQqqXLmy1Wlg9EOFQq5UOpf74tkaTw99pcfRBSsmb3HnUVv3cBXZgXP/nKP5382nMsXK0DffTKK2bdvS9OnTaf36hZrh79+/T3MmzqHOjRuTv7+vZpiTly7RqrlzqfvT3c3e9/TpjK1FylUoSe3iPSjc28W4gWywZ2bFrJEje1FsbCxFRERQ5/6d6cgfRyj1/v1s89emdCT5urtTUS8XiikdQ927d6fwUuHUrUULqhYdTdv3bVeFT0tLpw8+eJH27dtE01ZME0ub4MqVK+Tt401tatemsOLFKapyZXruueeyvT/DOAP50Dk/kwvea+lBhQsY6MMYfSy5M+bhpUA78FB6KGasRvUeRVT5IElSFJ05c4Z++OEzatlSLRg9SHuQsfQlEc1+801K14jvUkICtRw6lBq0a0fd23Q3q6yqZZX1QskiVMzHiyoXLJDpXExMGbp0KeN7oRDobhBdvXmTCmSzM1GglxdNb9yMxv1goMiIGNW5IkFBlHAzQXVsxIiZRORPs2btooCwwySvR1arVo1Ci4ZS6S5dqGqLFlS9SROqV6IEeXmpZ9oYxhlhuYqxhvZV3Kl0hDtFntXanZPREzxjZQdCC4VSdMmMTaqV2wFdvXo+k1D18tiX6fLlyzT5y8nk75t5tuqfa9eo0eDBVCcmht6aOjXL+5YpEy7+nz91XmVyDpNs+Owxxd3dLZPnZijMy7+VW2aAtLS0TD503NzUsrnWdQ0bxtO1a5doz54NquM+Pj40d81cWjpqFAWFhtKCCRPolVdeoTt3bOcGgrE9s6tVMH5nC6WcUzNQn0uUDMPkDhas7EDNmJp08sJJ1TFYBYaGljT+hssFCFWnLp6ijz/+mAIC1fpP8kxVwzffpPiyZWn+sGHk4pJ1dTVqVI0CAgJo2ZxlmuehjG4pwQWD6cqNDEV6GSiu54RWrerSyJFf0X//+wpt/mGz6pyrmys1jY+nfmPH0hc7dgghc9++fTm6D/NkqBX8eHNjJucMinSnt8uVou/7sCI4wzgTLFjZgf7P96e9f+2lKYun0OnTl2jjxo1Ceb1duwFGoarD6NG0//h+mjtiLj18+JBuJNwQ/qru33+gEqpKhITQf/v1o4TERLpx5UomYUeJr683DRkyhHZt2UUjXhlBG//4g85fPi/uM/qz0TR4ymCL81Cvaj1x3bINy+jUqYs0f/58OntWaVNoHfXqtad33/2SJr49kb7Zts3oGuLb+d/SgZMn6fKFC/TT8uVitqt4cZiVMYxz4+VqoC4RRah4ID+GGcaZYB0rOxBXLo6+fP9LGvf5OJq8+CSFhhahAQMGULNmXcX5S9eu0fc7d4rv9XvXV137/fefUIECzejnP/4QCuv4hHfsqAqzZUtmqzuZp556imZ8M4OWzF5CXcaPp6TkEcL1AwSlEb1GWLyVVZMaTYSfrTFzxlDqrGRq3rwVNW/enE6fPk05pUGDDvSw0J/00lsf0hxDKfIL9KMd3+2gJX8tpJS0NCpWujSNHDmSSpUqleN7MAzDMExewoKVnWhRu4X4yH6slESEhZG0ZYvRj5VQRo8ganiW6GYVEuF7tGwpPkq2VqmC6LL1tBxdOZrGzR6XEd+je8jgythasXRjyw2igMfxRFaIpC1ntlCEuCaD4T2Hi49WHkR6pk4V8SvTs3r8eNUxxFklICNPoFGbRjS2YiNjmKnLpop0Im8ZCXyyXqQZhmEYxpbwHDTDMDmHtdcZhmFUsGDFMEyOKR3AiuwMwzC6E6xmzpwpnFTCf1HNmjVpz549WYZfsWIFlStXToSvVKkSrV27VnUeCtDvvfee2EYGXsGbNm1KJ06cUIW5ceMGde3alfz9/algwYLUq1cvNvNnGAv5qJ43vRHnSeWCMvyfMQzDMDoRrJYvX06DBw+m0aNHCzP7KlWqUIsWLejq1aua4Xfu3EmdO3cWgtD+/fvpmWeeEZ/Dhw8bw0yaNImmTZsmtmrZvXs3+fr6ijhTU1ONYSBUHTlyhH7++Wdhsbd9+3bq06fPE8kzwzg6Yb4uFBvCKpoMwzC6E6ymTJki9tLr2bMnVahQQQhDcBw5b948zfCffvoptWzZkt5++20qX748vf/++xQXFyc2CJZnq6ZOnSqsy9q1ayf2nlu0aBH9888/tHr1ahHm6NGjtH79evriiy/EDBks6bDlzLJly0Q4hmEYhmGYnJCnr5zYI++PP/6g4cOHG4/BCSaW7uDjSAscxwyXEsxGyUITto6Bk0nEIQOnmRCgcO0LL7wg/mP5D1uqyCA87o0Zrvbt22e6771798RHBpsdg6SUFPE/OSmJkpI1EpyUTMnG40mP/mYcSHr0Oxm/b4uglJRElJyMM5kjk+8hwuNa5T0ffRH3euRTQcT36B7GOMiV6HYyJSUnqdOWxTXqPKjzkZHSx+mX8yUfE78NGXnKuA/urQ4j8oSMGzOgTo9WnGR4nPfH8SRppDlJ3DPjEOJwfZw/hEF8BlHo6jwo6kGUO/4jflX6M8pSTotcL8ZrTI7JcSrjMNabshySkcZHaU5Sl52qTORzcjtI0krjo7aidc9H5SOnTVn8qvb1qCwRRJ2vx+X7OO2Z61TOi7E+jFcmm01jRj2p25eqnBX3yKibjJiN9aIoR2MYOY5H3zPag6IvParjx/fNnAZjW5br3TQuVT9+1I4fta/H6VO0iUf1oQybkUdF2xJXKNOnvFfm/ve4bai7sbLsle1JblOZnmEmD7TM8an7l+qoaXzZxqVOm9bzT3W93LYetU9LMH0Oa/UpZdyq9CjuY9qONctR0SbNlavq+YLr5OfyozCivZrGqXpOmPQbYx9W90llH1a1mUctKVOeH8Vr+ux7/AzLaGDmxqJkRX82tl9ln9F4VhnTYNIwVOWn1YZMngPiuGY/l8e9jLC3UzIK13SXEKcQrK5du0bp6ekUGqrenA6/jx07pnkNhCat8Dgun5ePZRUmJCREdR5bswQFBRnDmDJhwoSMPf1MKG7iY4phGIZhGP1z+/ZtMfFia1hJwkIwq6acKbt165bY/+/8+fN2qRi9kZSUJDyiX7hwQSj8Ozv5Kb/5Ka+A8+vccH6dmyQb5BczVRCqihYtSvYgTwWr4OBgcnV1pStX1Nu04HdYWJjmNTieVXj5P47BKlAZJjY21hjGVDkeGwzDUtDcfT09PcXHFAhV+aExyyCvnF/nJD/lFXB+nRvOr3Pjn8v82nNCJE+V1z08PCg+Pp42bdpkPIZ98/C7du3amtfguDI8gGWfHB7boUA4UoaBhAvdKTkM/mPGCfpdMps3bxb3hi4WwzAMwzCMQy4FYnmte/fuQpG8Ro0awqIvOTlZWAmCbt26UbFixYSOExg0aBA1aNCAPv74Y2rdurWw5Nu7dy/NnTtXnDcYDPTGG2/Q+PHjKSoqSghao0aNElN+cMsAYE0Iy0JYI8IK8cGDBzRw4ECh2G6vqUGGYRiGYZyfPBesOnXqRAkJCcKhJxTHsVwHVwiy8jl0mGCtJ1OnTh366quvhDuFd999VwhPsAisWLGiMczQoUOFcAa/VJiZgjsFxAmHojJLliwRwlSTJk1E/M8995zwfWUpWBaE7y2t5UFnhPPrvOSnvALOr3PD+XVuPB0gvwbJXvaGDMMwDMMw+Yw8dxDKMAzDMAzjLLBgxTAMwzAMYyNYsGIYhmEYhrERLFgxDMMwDMPYCBascsDMmTMpIiJCWBnC79WePXvIEYELi+rVq1OBAgXEFj9wR3H8+HFVmIYNGwoXFspPv379VGFguQnXF9g8G/Fgg2w4XNUbY8aMyZSXcuXKGc+npqbSgAEDqFChQuTn5ycsRU2d0TpKXtE+TfOKD/LnDPW6fft2evrpp4V7FKRd3itUBjY5sDSGk2Bvb2+xF+iJEydUYeAQuGvXrsLJIPYO7dWrF925c0cV5tChQ1SvXj3R1+HtedKkSaS3/MJdzLBhw6hSpUrk6+srwsBNjemG8lpt4qOPPnK4/IIePXpkygtc6Dhj/QKtvozP5MmTHbJ+J1gw9tjqebx161aKi4sTVoSRkZG0YMEC+2cQVoGM5Sxbtkzy8PCQ5s2bJx05ckTq3bu3VLBgQenKlSuSo9GiRQtp/vz50uHDh6UDBw5I//nPf6QSJUpId+7cMYZp0KCByOO///5r/CQmJhrPp6WlSRUrVpSaNm0q7d+/X1q7dq0UHBwsDR8+XNIbo0ePlmJiYlR5SUhIMJ7v16+fVLx4cWnTpk3S3r17pVq1akl16tRxyLxevXpVlc+ff/4Z1r/Sli1bnKJekZ4RI0ZIK1euFPlatWqV6vxHH30kBQQESKtXr5YOHjwotW3bVipVqpR09+5dY5iWLVtKVapUkXbt2iXt2LFDioyMlDp37mw8j/IIDQ2VunbtKvrI0qVLJW9vb2nOnDmSnvJ769YtUU/Lly+Xjh07Jv32229SjRo1pPj4eFUcJUuWlMaNG6eqc2Vfd5T8gu7du4v6U+blxo0bqjDOUr9AmU98MP4YDAbp1KlTDlm/LSwYe2zxPD59+rTk4+MjDR48WPrrr7+k6dOnS66urtL69evtmj8WrKwED6wBAwYYf6enp0tFixaVJkyYIDk6GIzRqbdt22Y8hgF40KBBZq9BY3ZxcZEuX75sPDZ79mzJ399funfvnqQ3wQoPWi0wOLm7u0srVqwwHjt69KgoDwxUjpZXU1CHZcqUkR4+fOh09Wo6ECGPYWFh0uTJk1X16+npKQYTgIcsrvv999+NYdatWycGq0uXLonfs2bNkgIDA1X5HTZsmBQdHS3lJVoDryl79uwR4c6dO6caeD/55BOz1zhSfiFYtWvXzuw1zl6/yHvjxo1Vxxy1frXGHls9j4cOHSpeppV06tRJCHb2hJcCreD+/ftiGxwsK8jAuSh+//bbb+ToJCYmiv9BQUGq43Cmin0d4YQVm1GnpKQYzyHfWIKQHbqCFi1aiG2Ejhw5QnoDy0GYbi9durRYJsBUMkC9YklFWbdYJixRooSxbh0tr8p2u3jxYnr55ZfF8oAz1quSM2fOCGfDyrrEvmBYtlfWJZaHsOODDMKjP2P7KzlM/fr1xdZbyjLAksXNmzdJ730ZdY08KsHSEJZWqlatKpaRlMsmjpZfLPFg+Sc6Opr69+9P169fN55z5vrFctiPP/4oljZNcdT6TTQZe2z1PEYYZRxyGHuP13nued2RuHbtGqWnp6sqEuD3sWPHyJHBPonYCqhu3boqL/ZdunShkiVLCmEE6/PQ5UBHXLlypTiPAUyrPORzegIDK9bX8SD+999/aezYsULf4PDhwyKteOCYDkTIi5wPR8qrEuhrYAcC6KU4Y72aIqdPK/3KusSgrMTNzU082JVhsCWWaRzyucDAQNIj0E1BfXbu3Fm1Se3rr78udE2Qx507dwphGv1gypQpDpdf6FM9++yzIr2nTp0Su3C0atVKDJiurq5OXb8LFy4UuknIvxJHrd+HGmOPrZ7H5sJA+Lp7967Qv7QHLFgxAigJQsD45ZdfVMexLZAM3g6gDIxtgPAwK1OmDDkSePDKVK5cWQhaEC6+/vpru3UwPfC///1P5F25D6Yz1SvzGLzld+zYUSjvz549O9O+rMr2j4Grb9++QpFYz9uDaIF9XZXtF/lBu8UsFtqxMzNv3jwx267cos2R63eAmbHHkeGlQCvAsgnehkwtE/A7LCyMHBXsmbhmzRrasmULhYeHZxkWwgg4efKk+I98a5WHfE7P4G2obNmyIi9IK5bMMLNjrm4dMa/nzp2jjRs30iuvvJJv6lVOX1b9FP+vXr2qOo9lE1iSOWp9y0IV6vznn39WzVaZq3Pk+ezZsw6ZXyVY2sfzWdl+na1+wY4dO8TMcnb92VHqd6CZscdWz2NzYdA37PkyzYKVFeANID4+njZt2qSaxsTv2rVrk6OBt1o07FWrVtHmzZszTRNrceDAAfEfMxwA+f7zzz9VDzH5oV6hQgXSMzC9xgwN8oJ6dXd3V9UtHmDQwZLr1hHzOn/+fLEkApPk/FKvaMd4oCrrElP/0K1R1iUe2tDlkEEfQH+WhUyEgRk8BBZlGWApWW/LRLJQBR1CCNLQs8kO1Dl0juQlM0fKrykXL14UOlbK9utM9aucfcazqkqVKg5dv1I2Y4+tnscIo4xDDmP38dquqvFO6m4B1kULFiwQlid9+vQR7haUlgmOQv/+/YVJ+tatW1UmuikpKeL8yZMnhfkuTF3PnDkjfffdd1Lp0qWl+vXrZzJ5bd68uTCbhRlr4cKFdWOWr+Stt94SeUVefv31V2GmC/NcWKTI5r0w+d28ebPIc+3atcXHEfMqW6wiP7D8UeIM9Xr79m1hYo0PHmNTpkwR32UrOLhbQL9E3g4dOiSsqLTcLVStWlXavXu39Msvv0hRUVEqc3xYJsE8/aWXXhJm4ej7MN3OC/P0rPJ7//594U4iPDxc1JWyL8vWUTt37hQWYzgPE/3FixeL+uzWrZvD5RfnhgwZIqzD0H43btwoxcXFifpLTU11uvpVuktA+mD5Zoqj1W//bMYeWz2PZXcLb7/9trAqnDlzJrtb0CvwhYEKhz8ruF+AnxRHBB1Y6wP/IuD8+fNisA0KChLCJPzAoIEq/R2Bs2fPSq1atRI+USCoQIB58OCBpDdgZlukSBFRb8WKFRO/IWTIYNB99dVXhUkyOmP79u1FZ3fEvIINGzaI+jx+/LjquDPUK/xxabVdmOHLLhdGjRolBhLksUmTJpnK4fr162Kg9fPzEybaPXv2FAOcEvjAeuqpp0QcaDMQ2PSWXwgX5vqy7Lfsjz/+kGrWrCkGMy8vL6l8+fLShx9+qBJEHCW/GHwxmGIQhUk+3AzAJ5vpy62z1K8MBCD0RQhIpjha/VI2Y48tn8co29jYWPHcxwuk8h72wvAokwzDMAzDMEwuYR0rhmEYhmEYG8GCFcMwDMMwjI1gwYphGIZhGMZGsGDFMAzDMAxjI1iwYhiGYRiGsREsWDEMwzAMw9gIFqwYhmEYhmFsBAtWDMMwDMMwNoIFK4axEQaDgVavXp2naYC/3z59+lBQUJBIj7wHIJNzsBlsZGQk7dy5M6+Tkm9ZsGCB2DQ9L/pgrVq16Ntvv7VpnIxzw4IVk+/p0aMHPfPMM+QMrF+/XgxC2DH+33//pYoVK+ZbIdNWfPbZZ2KT2Dp16hiPbdu2jRo3biwEWB8fH4qKiqLu3bsLIcxWnD171qGE44YNG4r04uPl5SU2wp01a5ZN4u7UqRP9/fffxt9jxoyh2NjYTOHQ5lu1akW2ZOTIkfTOO++IDZwZxhJYsGIYJ+LUqVNUpEgRIQSEhYWRm5tbXifJocEM4IwZM6hXr17GY3/99Re1bNmSqlWrRtu3b6c///yTpk+fTh4eHpSenk75md69ewvhBmXUsWNHGjBgAC1dujTX8Xp7e1NISEi24dDmPT09yZZAULt9+zatW7fOpvEyTozddyNkGJ2DjU7btWtn/N2gQQPptddeExsTYwNQbOQ7evRo1TV///23VK9ePbGZKTY8/emnn8QmoqtWrVJtdvz888+LjVERT9u2bcWGuQA7rWPj0CVLlhjDL1++XGygeuTIEbNpxW7w1atXFxuKhoWFScOGDTNuOop8KDc0xea0WmDj0jZt2kgFCxYUm5tWqFBB+vHHH43n//zzT6lly5aSr6+vFBISIr344otSQkKCxeWD+5pLx+rVq6WqVauKcitVqpQ0ZswY1aapCP/5559LzzzzjCgfbBD93XffqdJ/+PBhqXXr1lKBAgXEBrvYVFa5mTauL1eunLhHdHS02NFe5t69e9KAAQNE2eE8NlPHZrXm+P333yUXFxcpKSnJeOyTTz6RIiIipOzYsWOHSBvqNDw8XJTZnTt3VOX0wQcfiM2BkY/ixYuLjXaVZaH8oNwtyaO8KfO3334rNWzYUJRj5cqVpZ07d6rS98svv4g4cR5tARsb37hxQ5xLT08X5YJ8Iv24fsWKFVnmF3ENGjRIdSwqKkp64YUXxPdz586JPoB2hbpD31BunHzgwAGRXpQFzsfFxYnyB9g4F/1I/m5u817TPnjo0CGpUaNGIg/YdBybNSs3Ypb7/uTJk0WbQBhs/Hv//n1VPlBH6AcMYwksWDH5Hi3Byt/fXwz6EKAWLlwoGQwGITzJg07FihWlJk2aiMFg27ZtQlhQPtTxYIbA9fLLL4uH+19//SV16dJFDIIY3AEGQwwWGHAuXLgghJRPP/3UbDovXrwoBCE8+CGY4V7Y0V0WarDr/WYNAbIAAAnlSURBVLhx48Qgjl3gr169qhkPhJJmzZqJdJ06dUr64YcfRB7AzZs3pcKFC0vDhw8X99i3b58Ii8HJ0vLBfeXBTpmO7du3i+sWLFgg7ovwGLgRjwyuQ/q/+uor6cSJE9Lrr78uBtrr168bywCD37PPPisG3ePHj0vz5s2Tjh07Js4vXrxYKlKkiBAqTp8+Lf4jPO4JMIBCgEFaIGBC+MG9zDFlyhQhwChZunSpEGjkMtMCgh4ECAhhKKNff/1VtJEePXqoBCukDe0AeZ0wYYIQ4uS87NmzR5THxo0bRTnKZZBdHmXBCules2aNKKMOHTqI+8lC7P79+0Ue+vfvL9owhNXp06cbBejx48eL69evXy/qCnWJ8BDsrRGsIJChrtBnYmNjhaC5d+9eadeuXVJ8fLxKWIyJiRHCC9odyuzrr78WaTMVrFJSUqS33npLhEe54INjcvuR+yCEWJQT7o+XhU2bNglhHv1dBt/RJvv16yfui76APjZ37lxVPmbPnm32RYVhTGHBisn3aAlWGACUYJYIs0Ngw4YNkpubm3Tp0iXj+XXr1qke6l9++aUQoh4+fGgMA4EKswO4XinkYOYLQhpmDJThTXn33XczxYlBGYIHBi6AgTy7AaBSpUoqYUbJ+++/L9KhBEIf8oYB2pLy0Zo5AMij6ewQygmDn/K6kSNHGn9jcMQxlC+AwIfB0XRGQaZMmTKZBCXkqXbt2uI7Zo0aN26cZTkrgaCA8ErS0tKEgIR0YZYDs2sQShITE41hevXqJfXp00d1HYQ4CE53794Vv1FPylkQpAkzhBjElQIShCBr8ihf98UXXxjPYxYUxyA8gM6dO0t169bVzHNqaqoQLkxnuJAnXGeJYIUyQt3injNmzBBCtKurq5jFNU0TBEiAWSpZODRFKVgBvExUqVIlUzhlu4NwhJcV5SwhZmZRB/JMGfo+6gHplcFMWqdOnVTxYtYU18n9jGGygnWsGEaDypUrq35Db+nq1avi+9GjR6l48eJUtGhR4/natWurwh88eJBOnjxJBQoUID8/P/GBonNqaqrQg5KZN28eHTp0iPbt2yeUzqH4aw7cF/dRhqlbty7duXOHLl68aHHeXn/9dRo/fry4dvTo0eL+ynRv2bLFmGZ8ypUrJ84p051V+ZgDcY8bN04Vt6yTk5KSohm3r68v+fv7G+OGIne9evXI3d09U/zJyckijdCHUt4DeZXTDkMFxBEdHS3K4aeffsoyzXfv3hWK2EpcXV1p/vz5oswnTZpExYoVow8//JBiYmJEXuS8oj6V6WjRooVQgD5z5oxmXlGv0BHKqhwtyaNW3KgfoCzHJk2aaN4D7Rb10axZM9U9Fi1alOkepkBZHWGhE4W6ffPNN6l///7GPoOPDJTbYemHc2Dw4MH0yiuvUNOmTemjjz7K9l7ZgXirVKki2pAM2jzq4Pjx48ZjqDfUaVZtGfnBdffu3ctVmpj8AWu2MowGpgM3Bj1rrIIg7MTHx9OSJUsynStcuLDxOwZgDJYuLi5iUJYHQHuCwQuD/I8//igEiwkTJtDHH39Mr732mkj3008/TRMnTsx0nTJtOSkfxD127Fh69tlnM51TCi9ZxY0BLqv4weeff041a9ZUnZMHzri4OCHYQBF548aNQsEaA/k333yjGWdwcLBQTtcCAtVLL70kPu+//z6VLVtWWBAij0hL3759hfBmSokSJSzKa07zqBW3LIxbU45oH8ijkuwUw7t27UojRowQ8aO9oF1bCiz9unTpIu6L+oHQv2zZMmrfvj3ZE0vq4MaNG0JAy6rcGEaGBSuGsZLy5cvThQsXVILQrl27VGEwgC9fvlxYMmHGRQs8rDGDgoEIcWFQwsyVuYc37gt/OljxkAfKX3/9VcyKhYeHW5UHzBz069dPfIYPHy4GaghWSDfuERERkSuLQgxWphZyiBszBfAJlVMwC7Nw4UJ68OBBpgExNDRUzCKePn1alKU5UB8w38enQ4cOwsIPdYEZRVOqVq1Ks2fPVpW5FoGBgaItQEiW8wrLuNzkFVaGQFmOlubRknLctGmTEAJNwUwSBKjz589TgwYNrIo3ICBAM89yn8FHnrVC+dy6dUvcTwbCKT6Y6ercubOYGdQSrCyxwMQ9MWuIOpFnrdBfIOxhxtIaDh8+LNoCw1gCLwUyjJVghgMPf/gtwozTjh07hHCkBIMeZjvatWsnzmOWZOvWrWIGQ162g1CDQQZ+cqZMmSIGiiFDhpi976uvvioGJghAx44do++++0681WMJxZqZgTfeeIM2bNgg0gRBDkt/GIQAzOMhZGBQ+/3338VyDML27NnTKlcCEMwwcF++fJlu3rwpjr333ntiOQmD+ZEjR8RSDWYkkH9LGThwICUlJdELL7xAe/fupRMnTtCXX35pXNpB3JiBmzZtmvB7hNkmDM4oX4D/MP9H+eH8ihUrxPKbOeeTjRo1EjM4SK/MnDlzxPIWZvtQPjg3bNgw8R+zfQC/4VAU6cWyG9KJ+sJvS4FQDiEbvsmuXLlCiYmJFuXREiBMo37RprAUjPKAAHnt2jUhqKMdQriBEIs8op3ApQR+57TPVKpUyfjysGfPHurWrZsQ3OC2AkuuKBv0kXPnzgkBCOmT26VW+0L7RdkizVpLdLgXZkLRTyEYoZ2j72CGEQKqNaAPN2/ePEd5Z/IhWWpgMUw+VV43tW7CeaU1ERS5ocANtwdly5YV1lOmCtuwVurWrZuw3INFVenSpYW5N5ScYUkHqzFYP8ns3r1bcnd3l9auXZsjdwuWKq8PHDhQKEAjTbAAfOmll6Rr164ZzyNN7du3Fyb4ULaHddgbb7xhVPi2pHy+//574SoBSv7K9KCc6tSpI+KFNVaNGjVUFlhaSu9QWpbN6cHBgweFgj0UrKHwDOV/WK7JwIUFLNBQRlBerl+/vrRy5UpxDvfCOZQ97g+Felg+ZkXHjh2ld955x/gb4aF0DiV6lGGhQoXEPZBnJVDKhkUljAtwP1jIwb2CDMoF9aUECtlK1xVwqwArRihOKy3ossqjltI7rD1xbMuWLaq2hLpAHlDXLVq0EOEA6nrq1KnCWAJtEu0E57OyhNRqF0qycrcAww64ZUBekaeiRYuKdior+psqr0PB/rnnnhPptoW7BSXIg7KsYYmKMoARB8NYggF/8lq4YxiG0SuY0YEiN2ZuoJjN5C8w+4hZ17lz5+Z1UhgHgZcCGYZhstFHgjK/0pqPyT9gSRbGCQxjKTxjxTAMwzAMYyN4xophGIZhGMZGsGDFMAzDMAxjI1iwYhiGYRiGsREsWDEMwzAMw9gIFqwYhmEYhmFsBAtWDMMwDMMwNoIFK4ZhGIZhGBvBghXDMAzDMIyNYMGKYRiGYRiGbMP/AQmdt5hqFx1bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.plot(distances);\n",
    "\n",
    "y_upper_bound = .2\n",
    "plt.ylim(0, y_upper_bound)\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 70\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');\n",
    "\n",
    "# Then we'll see how many distances are actually above this one\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "# Start of the shading and text\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)\n",
    "\n",
    "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "    plt.text(x=np.average([start_index, end_index]),\n",
    "             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "             s=f\"Chunk #{i}\", horizontalalignment='center',\n",
    "             rotation='vertical')\n",
    "\n",
    "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
    "if indices_above_thresh:\n",
    "    last_breakpoint = indices_above_thresh[-1]\n",
    "    if last_breakpoint < len(distances):\n",
    "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "                 s=f\"Chunk #{i+1}\",\n",
    "                 rotation='vertical')\n",
    "\n",
    "plt.title(\"Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences (Sentence Position)\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa5215",
   "metadata": {},
   "source": [
    "honestly hard to visulaize chunk here as we have used very large size data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad88f",
   "metadata": {},
   "source": [
    "Let's combine these sentences into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0c0a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# grouped_sentences now contains the chunked sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\\n\\nAGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS IN ARTIFICIAL INTELLIGENCE\\n\\nArticle in INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY - January 2025 DOI: 10.34218/1JCET_16_01_069\\n\\nCITATIONS\\n\\nREADS\\n\\n4\\n\\n6,789\\n\\n2 authors:\\n\\nPanneer Selvam Viswanathan Institute of Electrical and Electronics Engineers 3 PUBLICATIONS 14 CITATIONS\\n\\nResearch Pub\\n\\n4379 PUBLICATIONS. 624 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Research Pub on 20 January 2025.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47c63a",
   "metadata": {},
   "source": [
    "Manually inspect few elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d5499fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388188752,\n",
      "\n",
      "AGENTIC AI: A COMPREHENSIVE FRAMEWORK FOR AUTONOMOUS DECISION-MAKING SYSTEMS I\n",
      "...\n",
      "ronics Engineers 3 PUBLICATIONS 14 CITATIONS\n",
      "\n",
      "Research Pub\n",
      "\n",
      "4379 PUBLICATIONS. 624 CITATIONS\n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "All content following this page was uploaded by Research Pub on 20 January 2025.\n",
      "\n",
      "\n",
      "Chunk #1\n",
      "The user has requested enhancement of the downloaded file.\n",
      "...\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    buffer = 200\n",
    "    \n",
    "    print (f\"Chunk #{i}\")\n",
    "    print (chunk[:buffer].strip())\n",
    "    print (\"...\")\n",
    "    print (chunk[-buffer:].strip())\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bcd84d",
   "metadata": {},
   "source": [
    "## Summarize the data\n",
    "\n",
    "Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4906ca",
   "metadata": {},
   "source": [
    "### Text and Table summaries\n",
    "\n",
    "We don't need a multimodal model to generate the summaries of the tables and the text. I will use open source models available on Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0573be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebde46",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1a11799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {chunk}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "#model = ChatGroq(temperature=0.5, model=\"deepseek-r1-distill-llama-70b\")\n",
    "#model = ChatGroq(temperature=0.5, model=\"qwen-qwq-32b\")\n",
    "#model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "#summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "summarize_chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2a9c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#from langchain.chains import LLMChain\n",
    "\n",
    "# Example function to process text\n",
    "def process_text_with_delay(chunks, chain, delay=1):\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            # Process the chunk using the chain\n",
    "            result = chain.invoke(chunk)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "        # Introduce delay to handle rate limits\n",
    "        time.sleep(delay)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7971b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = process_text_with_delay(chunks, summarize_chain, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f3e84",
   "metadata": {},
   "source": [
    "### save chunks as texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2c6b6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef58557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a2c5d",
   "metadata": {},
   "source": [
    "### Summarize text and table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5697bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The article \"AGENTIC AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\" was published in the International Journal of Computer Engineering & Technology in January 2025. It has received 4 citations and been read 6,789 times. The authors are Panneer Selvam Viswanathan, with 3 publications and 14 citations, and Research Pub, which has 4,379 publications and 624 citations.',\n",
       " 'User requested enhancement of the downloaded file.',\n",
       " 'The article \"Agentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\" by Panneer Selvam Viswanathan discusses the concept of Agentic AI, which enables autonomous decision-making and adaptive problem-solving in AI systems. It analyzes the foundational architecture, capabilities, and applications across various industries such as robotics, healthcare, and finance. The article emphasizes the importance of addressing technical challenges and ethical considerations, including accountability and bias mitigation, highlighting the need for responsible deployment of such technologies. It contributes to understanding the evolution of autonomous AI systems and their impact on future technology.',\n",
       " 'The document presents a study on the development of a novel Intelligent Classroom Management System (ICMS). It integrates various technologies to enhance teaching effectiveness and student engagement. The system employs sensors and real-time data analytics to monitor classroom dynamics, promote interactive learning, and provide feedback to educators. Experimental results indicate improvements in student participation and overall learning outcomes compared to traditional methods.',\n",
       " \"The text discusses the evolution of artificial intelligence (AI), highlighting the shift from traditional rule-based systems to advanced architectures, particularly focusing on Agentic AI. This new form of AI features autonomous decision-making, independent goal-setting, and adaptive behaviors, allowing it to operate with minimal human intervention. The significance of Agentic AI is underscored in sectors requiring rapid decision-making and adaptation, such as manufacturing, healthcare, finance, and transportation. The research aims to analyze Agentic AI's architecture, implementation challenges, ethical considerations, and implications for human-AI collaboration. It emphasizes foundational principles that integrate computer science, cognitive psychology, and systems theory for developing autonomous systems.\",\n",
       " 'Agentic AI systems represent an advanced method of autonomous decision-making, going beyond traditional algorithms. They use complex neural architectures for real-time information processing and operate on several levels, from immediate reactions to strategic planning. This multi-tiered framework enables effective navigation of complex situations while balancing quick responses with long-term considerations, resembling human cognitive processes.',\n",
       " 'Agentic AI features goal-oriented behavior that transitions from reactive to proactive actions through advanced objective hierarchies, enabling the decomposition of complex goals and dynamic goal prioritization. This system maintains operational coherence while adapting to changing conditions. Additionally, it employs a hybrid learning framework combining supervised and reinforcement learning, facilitating rapid adaptation and improved performance in dynamic environments, particularly evident in web navigation tasks.',\n",
       " 'Agentic AI significantly differs from traditional AI by demonstrating greater autonomy and adaptability. Unlike conventional systems confined to set parameters, Agentic AI can dynamically adjust its behavior, learn from new experiences, and effectively navigate unstructured environments while adhering to ethical constraints. Key characteristics include autonomous decision-making, goal-oriented behavior, adaptive learning, and contextual awareness. The architecture of Agentic AI is a complex integration of various technologies essential for its independent functioning.',\n",
       " 'Modern Agentic AI systems use advanced perception frameworks to process and integrate multiple data streams through sensor fusion algorithms, creating detailed environmental models for accurate assessment and informed decision-making.',\n",
       " 'Agentic AI features advanced decision-making algorithms that combine classical reasoning with probabilistic inference for nuanced responses in complex situations. Its layered architecture supports both rapid heuristic responses and deeper analysis, allowing flexible adaptation. Additionally, the action execution frameworks ensure accurate implementation of decisions into actions, maintaining stability and effectiveness across various operational contexts.',\n",
       " 'The execution layer features strong error handling and adaptive control mechanisms for reliable performance in difficult conditions.',\n",
       " 'Agentic AI systems employ a continuous improvement cycle through a learning architecture that uses performance data and feedback mechanisms to update internal models, enhancing decision-making and optimizing future operations. This adaptive learning enables the systems to evolve and respond more effectively to complex challenges.',\n",
       " 'Autonomous operation in Agentic AI systems involves a complex integration of various technologies, facilitating independent functioning and adaptive behavior.',\n",
       " 'Agentic AI systems use complex decision-making architectures that integrate probabilistic reasoning with deterministic rules, allowing for both immediate responses and strategic long-term planning through multi-layered frameworks.',\n",
       " 'Decision-making processes utilize learned experiences and predefined constraints to enable informed choices within operational limits.',\n",
       " 'Agentic AI systems utilize advanced sensor fusion and data integration for environmental perception, enabling comprehensive situational awareness through sophisticated data processing algorithms. They create detailed environmental models and feature real-time analysis to identify patterns within IoT-integrated contexts.',\n",
       " 'Agentic AI systems possess advanced real-time adaptation capabilities, allowing them to learn and adjust their behavior in response to changing environments, unforeseen challenges, and new demands.',\n",
       " 'The adaptation framework features feedback loops for ongoing performance enhancement and operational stability.',\n",
       " 'B.',\n",
       " 'Machine learning integration is essential for Agentic AI systems, facilitating advanced learning and adaptation for autonomous functions.',\n",
       " 'Reinforcement learning is essential for creating autonomous features in Agentic AI systems.',\n",
       " 'The text discusses reinforcement learning applications that help systems learn optimal behaviors through environmental interaction, utilizing advanced reward mechanisms and policy optimization. It also highlights the use of state-of-the-art neural network architectures in Agentic AI systems, which facilitate sophisticated pattern recognition and decision-making, incorporating deep learning for complex analysis and specialized networks for various operational needs.',\n",
       " 'The neural network framework enables both offline learning for baseline development and online learning for real-time adaptation.',\n",
       " 'Agentic AI systems utilize advanced algorithms for data processing and pattern recognition, enhancing their decision-making capabilities in complex environments, particularly in IoT and distributed computing. The performance metrics of Agentic AI, such as decision accuracy, response time, adaptation rate, and system reliability, show improvements over traditional AI. In manufacturing, the integration of Agentic AI has transformed industrial processes through intelligent manufacturing systems and autonomous mobile robots (AMRs), which feature contextual awareness and adaptive decision-making. This leads to improved operational efficiency, proactive quality management, and significant reductions in downtime and defects.',\n",
       " 'Integrating Agentic AI in healthcare transforms patient care and medical decisions through smart medical devices, which offer adaptive drug delivery and intelligent monitoring. These systems analyze patient data in real time for better treatment outcomes. Additionally, enhanced diagnostic systems utilize AI for advanced data analysis and pattern recognition, facilitating early disease detection and improved treatment planning.',\n",
       " 'Integrating diverse data sources enhances diagnostic accuracy and recommendations. Advanced patient monitoring systems utilize Agentic AI for continuous analysis of vital signs and behaviors, facilitating predictive analytics that identify potential health issues early. This proactive strategy improves patient outcomes and eases the workload on healthcare providers.',\n",
       " 'C.',\n",
       " 'The transportation sector has significantly transformed with the implementation of Agentic AI, particularly in autonomous vehicle technology, enabling advanced navigation in complex environments with little human involvement.',\n",
       " 'Advanced navigation and traffic management systems utilize real-time data analysis and predictive capabilities for dynamic route optimization and traffic management, significantly enhancing transportation efficiency by continuously analyzing data streams to adapt to changing conditions.',\n",
       " 'Integrated emergency response coordination improves the management of complex urban transportation networks.',\n",
       " 'The entry appears to be labeled \"D,\" but no specific data or context is provided to summarize. Please provide more details or content related to \"D\" for an accurate summary.',\n",
       " 'The financial sector utilizes Agentic AI to improve decision-making and risk management. Advanced algorithmic trading systems offer real-time market analysis and pattern recognition, identifying trading opportunities while adhering to strict risk protocols. Risk assessment systems have advanced to provide thorough analyses across various dimensions for better risk management and compliance monitoring.',\n",
       " 'Agentic AI systems in portfolio management enhance asset allocation and investment strategies by analyzing market trends and risks, enabling autonomous decision-making aligned with investment goals. Key industry applications include autonomous robots and quality control in manufacturing, diagnostic systems and patient monitoring in healthcare, autonomous vehicles and traffic management in transportation, and algorithmic trading and risk assessment in finance, each providing improved efficiency, accuracy, and management.',\n",
       " 'Agentic AI systems require strong accountability frameworks to balance autonomous decision-making with human oversight. Legal guidelines must clarify liability and responsibility for unintended consequences, while decision-making transparency is essential for trust, necessitating explainability features and audit trails for accountability.',\n",
       " 'Agentic AI systems require comprehensive risk management strategies to mitigate technical and operational risks, which involve creating testing frameworks, defining operational boundaries, and implementing failsafe mechanisms to ensure acceptable system behavior.',\n",
       " 'Please provide the content or details of the table or text chunk labeled \"B\" for summarization.',\n",
       " 'Agentic AI integration is reshaping workforce dynamics, requiring adaptation strategies to address job displacement and evolving skill needs, while also creating roles for AI system management. Furthermore, establishing effective human-AI collaboration frameworks is essential, balancing autonomy with necessary human oversight for optimal interaction.',\n",
       " 'Public trust in Agentic AI systems depends on transparency, reliability, and demonstrated value, necessitating clear communication of capabilities and limitations, as well as proactive stakeholder engagement.',\n",
       " 'C.',\n",
       " 'The ethical considerations for Agentic AI systems include addressing bias and ensuring fairness through diverse representation, prioritizing privacy concerns with robust data protection while maintaining effectiveness, and focusing on security implications that go beyond traditional cybersecurity to protect autonomous decision-making. Additionally, significant challenges such as ensuring system reliability and stability in diverse scenarios are critical for successful deployment, particularly in high-stakes applications.',\n",
       " 'Effective fault detection and recovery mechanisms are essential for maintaining system integrity under diverse operational conditions.',\n",
       " 'Please provide the table or text chunk you would like me to summarize.',\n",
       " 'Performance optimization of Agentic AI systems involves managing complex computational demands while ensuring real-time responses. It requires a careful examination of the interactions between neural network architectures, decision-making algorithms, and system latency, especially in logistics. The key challenge lies in balancing processing power with energy efficiency.',\n",
       " 'Integrating Agentic AI systems with existing infrastructure poses significant technical challenges, particularly related to compatibility within complex ecosystems that include legacy systems and differing operational requirements.',\n",
       " 'Managing coherence in distributed architectures and heterogeneous systems demands complex and advanced strategies.',\n",
       " 'It appears there is no specific information provided in your prompt. Please provide the table or text chunk you would like summarized.',\n",
       " 'The deployment of Agentic AI faces significant challenges due to regulatory compliance. Organizations must navigate a complex and dynamic regulatory landscape that varies across different jurisdictions and industries, necessitating flexibility in AI system design to adapt to evolving compliance requirements while ensuring operational effectiveness.',\n",
       " 'Deploying Agentic AI systems requires substantial infrastructure investments and modifications. Organizations must assess their current infrastructure to identify necessary upgrades in computational resources, storage, and network capabilities for effective AI operations.',\n",
       " 'Implementing Agentic AI systems involves significant financial considerations beyond initial development costs. Organizations must evaluate total ownership costs, including implementation and long-term operational expenses, while developing sustainable strategies to balance these costs with potential benefits. Successful implementation requires a comprehensive understanding of these challenges and a strategic approach that includes both technical and operational aspects, focusing on long-term sustainability and effectiveness. Careful planning can help manage these challenges and maximize the potential of Agentic AI systems.',\n",
       " 'The section identifies potential future research directions and opportunities across various fields, highlighting areas that require further exploration and development.',\n",
       " 'Future advancements in Agentic AI systems will focus on enhanced autonomy, enabling improved decision-making in complex environments, and significant breakthroughs in self-learning. Additionally, research is geared towards sophisticated learning algorithms that enhance adaptability and efficiency, allowing systems to learn from smaller datasets with high accuracy through advanced neural network architectures and training methodologies.',\n",
       " 'The integration of Agentic AI with emerging technologies like quantum computing, advanced sensors, and edge computing offers significant enhancements, allowing for improved real-time processing and decision-making while overcoming current computational limitations.',\n",
       " 'B.',\n",
       " 'The document discusses the opportunities and applications of Agentic AI across various industries. It highlights new use cases in environmental monitoring, sustainable energy management, and advanced healthcare. Market analysis shows growing demand for specialized AI solutions, particularly in digital transformation sectors. The innovation potential of Agentic AI is also emphasized, with possibilities for breakthroughs in personalized medicine, climate change mitigation, and space exploration. Future research directions in the field aim to address theoretical, practical, and societal implications.',\n",
       " 'Future innovation and advancements in autonomous capabilities and technology integration show significant potential for breakthroughs.',\n",
       " 'The article discusses the transformative potential of Agentic AI across various industries while addressing the practical challenges of its implementation. It highlights the importance of balancing technological capabilities with deployment considerations, ethical implications, and governance structures. Agentic AI is portrayed as a significant advancement in AI, promising improvements in operational efficiency and decision-making, but also facing challenges such as system reliability, regulatory compliance, and costs. The findings suggest that careful development and integration of these systems are vital for future advancements in autonomous capabilities and learning algorithms.',\n",
       " 'The citations provided include research on AI engineering and autonomous AI agents, focusing on their design principles and decision-making capabilities. Key documents include a 2022 IEEE publication on AI software, an explainer from IEEE Spectrum on AI agents, and a 2024 study on foundational design principles in agentic systems.',\n",
       " 'The provided content is insufficient to summarize.',\n",
       " 'The document is a publication titled \"Artificial Intelligence: A Systems Approach from Architecture Principles to Deployment,\" released in 2024 by MIT Press and available through IEEE Xplore.',\n",
       " 'The referenced document appears to be a scholarly work authored by Mohanarangan, S., Karthika, D., Moohambigai, B., and Sangeetha, R. It likely discusses topics within the field of electrical engineering or related areas, as indicated by its publication in an IEEE source. Further details about specific content or findings would require access to the document itself.',\n",
       " 'The article discusses integration strategies for AI and machine learning in IoT systems, emphasizing their potential to enhance functionality and efficiency. It is published in the International Journal of Scientific Research in Computer Science and Engineering, volume 12, issue 2, pages 25-32, in 2024.',\n",
       " 'The document presents a novel approach for managing energy consumption in cloud computing environments through an innovative algorithm that optimizes resource allocation and scheduling. It emphasizes the importance of energy efficiency in cloud data centers and evaluates performance improvements in comparison to existing methods. The findings indicate significant reductions in energy usage while maintaining service quality, making it a valuable contribution to sustainable cloud computing practices.',\n",
       " 'The article discusses the emergence of Agentic AI as a transformative approach to autonomous artificial intelligence, highlighting its implications and potential impact on various industries.',\n",
       " 'The article discusses \"agentic AI,\" a new paradigm in autonomous artificial intelligence, as explored by Automation Anywhere.',\n",
       " 'The document discusses agentic AI, highlighting its key benefits and features as presented on the Automation Anywhere website.',\n",
       " 'The provided sources discuss the ethical and social implications of artificial intelligence, focusing on moral considerations and the impact of AI on society.',\n",
       " 'The study identifies key barriers to implementing AI in organizations through a Delphi methodology, highlighting challenges such as lack of understanding, insufficient infrastructure, data quality issues, and resistance to change.',\n",
       " 'The reference indicates a source authored by Alsuwaidi, Aydin, and Rashid, available at the provided hyperlink, but detailed content or specifics from the table or text are not included.',\n",
       " 'The 2022 study presented at the 5th European International Conference on Industrial Engineering and Operations Management systematically reviews the barriers and challenges faced in implementing Artificial Intelligence (AI) in logistic operations.',\n",
       " 'The content appears to reference a research article or study available at specific URLs, focusing on work by Panneer Selvam Viswanathan and mentioning \"Rapid Canvas.\" There are no additional details provided in the text chunk.',\n",
       " 'The content is limited to the year 2024, which does not provide specific details or context for summarization.',\n",
       " 'The document discusses future directions in AI research, highlighting trends and predictions regarding autonomous decision-making systems. It emphasizes the growing importance of agentic AI frameworks and includes references to relevant publications by IBM and academic sources.',\n",
       " 'The document contains an abstract for an article published in the International Journal of Civil Engineering and Technology (IJCET), focusing on research related to civil engineering topics. Full access to the article is provided through the linked PDF.',\n",
       " 'The article is open-access and allows unrestricted use, distribution, and reproduction, as long as proper credit is given to the original author and source.',\n",
       " 'The work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).',\n",
       " 'The table discusses the authors and affiliations of the paper \"Attention Is All You Need,\" highlighting key contributors from Google Brain, Google Research, and the University of Toronto. The abstract emphasizes that the leading sequence transduction models utilize complex recurrent or convolutional neural networks featuring an encoder-decoder architecture, often enhanced by an attention mechanism for improved performance.',\n",
       " 'The proposed Transformer model is a new network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. It demonstrates improved quality in machine translation tasks, is more parallelizable, and requires less training time.',\n",
       " 'The model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing previous best results by over 2 BLEU. It sets a new state-of-the-art BLEU score of 41.0 for the English-to-French translation task, trained efficiently on eight GPUs. Recurrent neural networks, particularly gated recurrent neural networks, are highlighted as leading techniques in sequence modeling and machine translation.',\n",
       " 'Jakob proposed using self-attention instead of RNNs; Ashish and Illia created the first Transformer models. Noam introduced scaled dot-product attention and multi-head attention, playing a key role in development. Niki evaluated and tuned numerous model variants, while Llion worked on the initial codebase and efficient inference. Lukasz and Aidan significantly improved results and accelerated research by redesigning tensor2tensor.',\n",
       " 'Work performed at Google Research includes contributions to various research projects, publication of academic papers, and development of innovative technologies.',\n",
       " 'The 31st Conference on Neural Information Processing Systems (NIPS 2017) focused on recurrent models that compute along symbol positions in input and output sequences. These models align positions with computation time steps, generating a sequence of hidden states based on the previous hidden state and input at each position.',\n",
       " 'The sequential nature of training examples limits parallelization, particularly at longer sequence lengths due to memory constraints affecting batching.',\n",
       " 'Recent advancements have enhanced computational efficiency and model performance using factorization tricks and conditional computation.',\n",
       " 'The text discusses the limitations of sequential computation and introduces the Transformer model, which relies solely on attention mechanisms instead of recurrent networks. This approach enables greater parallelization and improved translation quality. It also mentions other models like the Extended Neural GPU, ByteNet, and ConvS2S that aim to reduce sequential computation by utilizing convolutional neural networks for parallel processing of inputs and outputs.',\n",
       " 'ConvS2S requires a linear number of operations to relate signals based on distance, while ByteNet requires a logarithmic number of operations.',\n",
       " \"The text discusses self-attention, an intra-sequence mechanism used to capture dependencies in sequences for various tasks like reading comprehension and summarization. It highlights the Transformer model's unique reliance on self-attention for input and output representation, distinguishing it from RNNs and convolution-based models. Additionally, it mentions Multi-Head Attention as a technique to mitigate resolution loss due to averaging in the Transformer architecture.\",\n",
       " 'Competitive neural sequence transduction models utilize an encoder-decoder architecture, where the encoder transforms an input sequence of symbols into a continuous representation sequence.',\n",
       " 'The Transformer architecture consists of an encoder and decoder, each made up of six identical layers. The encoder features a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder generates output sequences auto-regressively, utilizing previously generated symbols as input at each step, and includes similar attention mechanisms and feed-forward components.',\n",
       " 'A residual connection is used around each of the two sub-layers, accompanied by layer normalization.',\n",
       " 'The output of each sub-layer is calculated using LayerNorm applied to the sum of the original input and the output of the sub-layer function.',\n",
       " 'The model employs residual connections for all sub-layers and embedding layers, with an output dimension of 512. The decoder consists of 6 identical layers, including a unique sub-layer for multi-head attention over the encoder output. It incorporates masking in the self-attention sub-layer to prevent future position dependencies, enabling predictions for a position to rely only on known outputs from previous positions.',\n",
       " '3.2 outlines specific aspects or data related to the topic, providing insights or details necessary for understanding the content addressed in the preceding sections.',\n",
       " 'The attention function maps a query and key-value pairs to an output as a weighted sum of values, with weights determined by a compatibility function. The \"Scaled Dot-Product Attention\" uses queries and keys of dimension dx and values of dimension dv, calculating dot products, scaling them by √d, and applying a softmax function to derive weights. Attention computations are performed on batch matrices for efficiency, with queries, keys, and values organized into matrices Q, K, and V, respectively. Multi-Head Attention consists of multiple parallel attention layers.',\n",
       " 'The text discusses attention mechanisms in machine learning, focusing on two main types: additive attention and dot-product (multiplicative) attention. Dot-product attention, which uses a scaling factor, is faster and more efficient due to optimized matrix multiplication, while additive attention uses a feed-forward network and performs better with larger dimensions. To enhance performance, a multi-head attention approach is introduced, where queries, keys, and values are projected into different dimensions and processed in parallel, resulting in improved output values.',\n",
       " 'The final values are obtained through concatenation and projection, as shown in Figure[2]. Multi-head attention enables the model to simultaneously focus on information from various representation subspaces at different positions.',\n",
       " \"A single attention head's averaging inhibits dot product growth, which is illustrated by assuming independent random variables for components of q and k, leading to a mean of 0 and variance of dx for their dot product.\",\n",
       " 'The MultiHead attention mechanism involves concatenating multiple attention heads, each defined by the projection of input matrices Q, K, and V using parameter matrices. This work utilizes 8 parallel attention heads (h = 8), with each head having a dimensionality of 64 (dy = dm / h = 64).',\n",
       " 'The computational cost of multi-head attention with reduced dimensions per head is comparable to that of single-head attention with full dimensionality.',\n",
       " 'The Transformer model employs multi-head attention in three ways, including \"encoder-decoder attention,\" where the decoder queries are derived from the previous layer and memory keys and values are sourced from the encoder\\'s output, enabling the decoder to attend to all input sequence positions.',\n",
       " 'The text describes the operation of self-attention layers in encoder-decoder models, where the encoder allows all positions to attend to previous layers while the decoder restricts access to prevent leftward information flow for auto-regression. It also mentions that each encoder and decoder layer includes position-wise feed-forward networks that operate identically on each position.',\n",
       " 'Two linear transformations are applied sequentially with a ReLU activation function in between.',\n",
       " 'The function FFN(x) applies linear transformations that utilize distinct parameters at each layer, resembling two convolutions with a kernel size of 1.',\n",
       " 'The model has an input and output dimensionality of 512 and an inner-layer dimensionality of 2048. It employs learned embeddings to transform input and output tokens into vectors of the same dimension, sharing the weight matrix between embedding layers and the pre-softmax linear transformation. Positional encodings are added to the input embeddings to provide order information. Table 1 compares maximum path lengths, per-layer complexity, and minimum sequential operations for different layer types: self-attention, recurrent, convolutional, and restricted self-attention, highlighting varying complexities and operational characteristics based on sequence length and representation dimension.',\n",
       " 'Positional encodings match the dimension dmodel of embeddings for summation.',\n",
       " 'The work employs sine and cosine functions of varying frequencies for positional encodings, defined mathematically as PE(pos, 2i) = sin(pos/10000^(4i)) and PE(pos, 2i+1) = cos(pos/10000^(4i)), where \"pos\" represents the position and \"i\" indicates the dimension. Each dimension\\'s positional encoding corresponds to a sinusoidal function.',\n",
       " 'Wavelengths were set in a geometric progression from 27 to 10000 - 27 to facilitate model learning based on relative positions. Experiments with learned positional embeddings yielded nearly identical results to the geometric progression method.',\n",
       " 'The sinusoidal version is preferred for the model as it may enable extrapolation to longer sequence lengths than those seen during training. The text discusses the comparison of self-attention layers with recurrent and convolutional layers for mapping variable-length sequences, focusing on three key objectives that motivate the use of self-attention.',\n",
       " 'Total computational complexity per layer.',\n",
       " 'The amount of computable tasks that can be parallelized is measured by the minimum number of sequential operations needed.',\n",
       " 'The ability to learn long-range dependencies in sequence transduction tasks is influenced by the path length for forward and backward signals in the network. Shorter paths ease this learning. Self-attention layers connect all positions with a fixed number of operations, while recurrent layers require O(n) operations. Self-attention is computationally more efficient than recurrent layers when the sequence length (n) is smaller than the representation dimensionality (d), common in state-of-the-art machine translation models. For very long sequences, self-attention can be limited to a neighborhood size, increasing maximum path length to O(n/r).',\n",
       " 'The investigation will focus on the limitations of using a single convolutional layer with kernel width less than the input size, which necessitates multiple layers to connect all input-output pairs, leading to increased complexity. Convolutional layers are more costly than recurrent layers, but separable convolutions significantly reduce this complexity. Even when using a full kernel size, the complexity matches that of a self-attention layer combined with a point-wise feed-forward layer, which may also enhance model interpretability.',\n",
       " 'Attention heads in the models learn distinct tasks and display behaviors linked to the syntactic and semantic structure of sentences. The training regime for the models is described in the following section.',\n",
       " 'The training utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and a shared vocabulary of around 37,000 tokens. For English-French, the dataset was much larger at 36 million sentences with a 32,000 word-piece vocabulary. Sentence pairs were batched by similar sequence length, with each batch containing approximately 25,000 source and target tokens.',\n",
       " 'Models were trained on a machine with 8 NVIDIA P100 GPUs. Base models took 0.4 seconds per training step for 100,000 steps over 12 hours. Big models took 1.0 second per step for 300,000 steps over 3.5 days.',\n",
       " 'The content required for summarization is missing or incomplete. Please provide the full text or table for an accurate summary.',\n",
       " 'The Adam optimizer was utilized with parameters β1 = 0.9, β2 = 0.98, and ε = 10^-8. The learning rate varied during training, initially increasing linearly for a set number of warmup steps and then decreasing proportionally to the inverse square root of the step number.',\n",
       " 'Warmup steps were set to 4000.',\n",
       " 'Three types of regularization are utilized during training: Residual Dropout, which involves applying dropout to the output of each sub-layer before adding it to the input, as well as to the sums of embeddings and positional encodings in both the encoder and decoder stacks.',\n",
       " 'The base model uses a parameter rate of Parop = 0.1.',\n",
       " 'The Transformer outperforms previous state-of-the-art models in BLEU scores for English-to-German and English-to-French translations on the newstest2014 tests while requiring significantly less training cost in FLOPs.',\n",
       " 'The table presents various ensemble models with their corresponding metrics. \"1079 Deep-Att + PosUnk\" has a score of 40.4 and a value of 8.0. The \"GNMT + RL Ensemble\" shows scores of 26.30 for one metric and 41.16 for another, with a performance drop indicated by 1.8-1079 and 1.1-1074. The \"ConvS2S Ensemble\" records scores of 26.36 and 41.29, with a noted drop of 7.7-10!9 and 1.2.10?! for other metrics.',\n",
       " 'The Transformer base model has an accuracy of 27.3 and a score of 38.1 with a parameter count of 3.3 million, while the Transformer big model has an accuracy of 28.4 and a score of 41.0 with a parameter count of 2.3 million.',\n",
       " 'Label smoothing with a value of €;, = 0.1 was used during training.',\n",
       " \"The big transformer model outperforms previous models in the WMT 2014 English-to-German translation task with a new state-of-the-art BLEU score of 28.4, surpassing prior models by over 2.0 BLEU. It trained in 3.5 days on 8 P100 GPUs and achieved superior results at a lower training cost. In the English-to-French task, it scored 41.0 BLEU, beating all previously published single models while costing less than 1/4 of the previous leading model's training expense. Dropout rates were set at 0.1 for the big model and 0.3 for others. The base models were derived from averaging the last 5 checkpoints recorded every 10 minutes.\",\n",
       " 'The big models utilized an average of the last 20 checkpoints, employing beam search with a beam size of 4 and a length penalty of 0.6.',\n",
       " 'Hyperparameters were selected based on experimentation with the development set.',\n",
       " 'The maximum output length for inference is set to input length plus 50, with early termination when possible. Table 2 compares the translation quality and training costs of this model architecture to others in the literature.',\n",
       " \"The number of floating point operations for training a model is estimated by multiplying training time, the number of GPUs, and the GPUs' sustained single-precision floating-point capacity. Additionally, the study examines model variations by altering components of the Transformer and measuring the impact on performance in English-to-German translation using beam search without checkpoint averaging.\",\n",
       " 'Table 3 shows the impact of varying the number of attention heads and key/value dimensions on performance, maintaining constant computation. Single-head attention is 0.9 BLEU points worse than the optimal setting, and quality decreases with excessive heads.',\n",
       " 'TELOP values for K80, K40, M40, and P100 are 2.8, 3.7, 6.0, and 9.5, respectively.',\n",
       " 'Table 3 presents different adaptations of the Transformer architecture, highlighting various modifications and enhancements to improve performance, efficiency, and applicability across tasks.',\n",
       " 'Unlisted values match those of the base model.',\n",
       " 'Metrics for the English-to-German translation development set (newstest2013) are presented, with listed perplexities calculated per wordpiece using byte-pair encoding, not intended for comparison with per-word perplexities.',\n",
       " 'The table presents various configurations of a model with distinct parameters such as sizes and performance metrics, indicating that reducing the attention key size negatively impacts model quality. It suggests the need for a more advanced compatibility function beyond the dot product for better performance.',\n",
       " 'Bigger models perform better, and dropout is effective in preventing overfitting.',\n",
       " 'The study introduces the Transformer, a novel sequence transduction model that utilizes attention mechanisms instead of recurrent layers. It achieves faster training and sets new records in translation tasks for both English-to-German and English-to-French. The best-performing model surpasses all previously reported ensembles in English-to-German translations. Future plans include applying attention-based models to various tasks beyond text and exploring local attention mechanisms for efficiently processing larger data types like images and audio. The goal is also to reduce sequential generation.',\n",
       " 'The code for training and evaluating the models is accessible at https://github.com/tensorflow/tensor2tensor. Acknowledgements are given to Nal Kalchbrenner and Stephan Gouws for their contributions.',\n",
       " 'The reference is for a work by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.',\n",
       " 'Layer normalization is a technique introduced in a 2016 arXiv preprint that addresses issues in training deep neural networks by normalizing the inputs across the features for each training example, enhancing overall training stability and performance.',\n",
       " 'The paper by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, published in 2014, presents a neural machine translation model that integrates alignment and translation processes, significantly improving translation quality by jointly learning these tasks.',\n",
       " 'Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V are likely authors or contributors to a work, though the specific details or context are not provided.',\n",
       " 'A study on neural machine translation architectures published in 2017, exploring various models and their effectiveness in translation tasks.',\n",
       " 'The paper discusses the application of Long Short-Term Memory (LSTM) networks for machine reading tasks, highlighting their effectiveness in processing sequential data.',\n",
       " 'The paper discusses the use of a recurrent neural network (RMN) encoder-decoder for learning phrase representations to improve statistical machine translation.',\n",
       " 'Francois Chollet\\'s paper \"Xception: Deep Learning with Depthwise Separable Convolutions,\" published in 2016, introduces the Xception model, which utilizes depthwise separable convolutions to improve deep learning performance.',\n",
       " 'The paper evaluates the performance of gated recurrent neural networks (GRNNs) in sequence modeling, authored by Junyoung Chung, Caglar Giilgehre, Kyunghyun Cho, and Yoshua Bengio, and published in 2014.',\n",
       " 'The text likely refers to a group of researchers or authors: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.',\n",
       " 'Dauphin discusses a convolutional sequence-to-sequence learning model in a 2017 arXiv preprint.',\n",
       " 'This paper by Alex Graves discusses the generation of sequences using recurrent neural networks (RNNs), detailing methodologies and applications in various domains.',\n",
       " 'The paper introduces deep residual learning for image recognition, proposing a novel network architecture that utilizes residual connections to enhance training of deep neural networks. It was presented at the IEEE Conference on Computer Vision and Pattern Recognition in 2016.',\n",
       " 'The document references research conducted by Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber in 2001, discussing the challenges of learning long-term dependencies in recurrent neural networks. Sepp Hochreiter and Jiirgen Schmidhuber are also mentioned in connection to this work.',\n",
       " 'The article discusses long short-term memory (LSTM), a type of recurrent neural network architecture designed to model temporal dependencies in data, published in Neural Computation in 1997.',\n",
       " 'The text lists the names of five individuals: Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.',\n",
       " 'The paper investigates the boundaries and capabilities of language modeling, examining various techniques and their effectiveness in generating coherent and contextually relevant text.',\n",
       " \"Lukasz Kaiser and Ilya Sutskever's paper discusses how Neural GPUs are capable of learning algorithms, presented at the International Conference on Learning Representations (ICLR) in 2016.\",\n",
       " 'The referenced work by Nal Kalchbrenner et al. discusses a method for neural machine translation that operates in linear time, as detailed in their 2017 arXiv preprint.',\n",
       " 'The text refers to individuals: Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.',\n",
       " 'The paper presents Structured Attention Networks, introducing a method for improving attention mechanisms in machine learning models, aiming to enhance performance on various tasks by leveraging structured information for more focused and relevant inputs.',\n",
       " 'The paper discusses the Adam optimization algorithm for stochastic optimization, authored by Diederik Kingma and Jimmy Ba, published in 2015 in the Journal of Machine Learning Research (JCLR).',\n",
       " 'The paper by Oleksii Kuchaiev and Boris Ginsburg discusses factorization techniques to improve the efficiency of LSTM networks, as presented in their 2017 arXiv preprint.',\n",
       " 'The document references a 2017 preprint by Zhouhan Lin and colleagues on a method for structured self-attentive sentence embedding.',\n",
       " 'The paper by Samy Bengio and Lukasz Kaiser discusses the potential of active memory as an alternative to attention mechanisms in neural networks, presented at the Advances in Neural Information Processing Systems (NIPS) conference in 2016.',\n",
       " 'The document discusses effective methods for implementing attention-based neural machine translation as proposed by Minh-Thang Luong, Hieu Pham, and Christopher D Manning in their 2015 paper.',\n",
       " 'The referenced work discusses a decomposable attention model presented by Ankur Parikh, Oscar Tackstrém, Dipanjan Das, and Jakob Uszkoreit at the 2016 Empirical Methods in Natural Language Processing conference.',\n",
       " 'The paper presents a deep reinforced model for abstractive summarization, authored by Romain Paulus, Caiming Xiong, and Richard Socher, and was published as an arXiv preprint in 2017.',\n",
       " 'The paper by Ofir Press and Lior Wolf discusses a method for enhancing language models by utilizing output embeddings.',\n",
       " 'The paper discusses a neural machine translation approach that addresses the challenge of translating rare words by utilizing subword units.',\n",
       " 'The document discusses a research paper by Shazeer et al. on the sparsely-gated mixture-of-experts layer in large neural networks, highlighting advancements in neural network architecture.',\n",
       " 'The referenced paper discusses Dropout, a technique to reduce overfitting in neural networks by randomly omitting a subset of neurons during training, enhancing model generalization.',\n",
       " 'The referenced work is presented by Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus, focusing on end-to-end memory networks within a conference paper context.',\n",
       " 'The provided text does not contain enough information or context to summarize.',\n",
       " 'D',\n",
       " 'No information provided to summarize.',\n",
       " 'The content provided is incomplete and lacks specific information to summarize.',\n",
       " 'The reference is for a publication edited by Sugiyama and Garnett in the proceedings of Advances in Neural Information Processing Systems 28, covering pages 2440-2448, published by Curran Associates, Inc. in 2015.',\n",
       " 'The referenced paper by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le presents a method for sequence to sequence learning using neural networks, published in 2014 in the Advances in Neural Information Processing Systems.',\n",
       " 'The authors propose a re-evaluation of the inception architecture in computer vision, presenting improvements and insights in their 2015 paper published in CoRR.',\n",
       " \"The document discusses Google's neural machine translation system, highlighting its effectiveness in narrowing the performance gap between human and machine translation.\",\n",
       " 'The paper presents deep recurrent models with fast-forward connections aimed at improving neural machine translation.',\n",
       " 'The article discusses the ability of AI to identify images generated by other AI systems, focusing on the challenges and effectiveness of current detection methods.',\n",
       " 'The authors propose a framework using Convolutional Neural Networks (CNNs) to detect AI-generated images, particularly those created by Generative Adversarial Networks (GANs), amidst concerns over legitimacy and security. They collected GAN-generated images from various tasks and architectures to enhance generalization in detection.',\n",
       " 'Transfer learning was utilized.',\n",
       " \"The method employed Class Activation Maps (CAM) to identify key regions influencing the classification model's decisions. It achieved 100% accuracy on the Real or Synthetic Images (RSI) dataset and outperformed in accuracy on other datasets and configurations.\",\n",
       " 'The most effective evaluation tool for image generation was a fine-tuned EfficientNetB4 detector, trained with a batch size of 64 and an initial learning rate of 0.001 over 20 epochs.',\n",
       " 'The study explores the detection and localization of GAN-generated images using convolutional neural networks, incorporating techniques like Adam optimizer, learning rate reduction, and data augmentation.',\n",
       " 'The article published in \"Imaging\" on 28 September 2023 by academic editor Cosimo Distante details a study or findings related to imaging, originally received on 10 August 2023, revised on 13 September 2023, and accepted on 27 September 2023. It is published under a CC BY license.',\n",
       " 'The content is copyrighted in 2023 by the authors.',\n",
       " 'MDPI is a licensee based in Basel, Switzerland.',\n",
       " 'The article discusses image synthesis, which involves creating artificial images from various input types such as text, sketches, audio, or images.',\n",
       " 'Image synthesis, widely used in art generation, photo editing, inpainting, and design, has garnered significant research attention, particularly following the development of Generative Adversarial Networks (GAN), which consist of a generator and a discriminator.',\n",
       " 'The two components are in competition.',\n",
       " \"The text describes the dynamic between a generator and a discriminator in Generative Adversarial Networks (GANs), where the generator creates images to deceive the discriminator, which distinguishes between real and synthetic images. Initially, the generator produces easily identifiable fake data, but over time it improves, creating more realistic images that challenge the discriminator's ability to differentiate. This training process results in the generation of high-quality, photorealistic images, significantly advancing the field of image synthesis and synthetic media over the past decade.\",\n",
       " 'The text is incomplete and does not provide sufficient information for summarization.',\n",
       " 'The document titled \"Imaging 2023, 9, 199\" likely contains information related to a study, review, or findings in the field of imaging, published in 2023. Specific details about the content such as methods, results, or conclusions are not provided here.',\n",
       " 'The provided links lead to a journal article in the journal \"Journal of Imaging,\" specifically discussing a topic related to imaging research. Please provide the relevant text or a description of the table for summarization.',\n",
       " 'Imaging 2023 presents opportunities for artists and creative professionals to enhance their work, but also raises concerns about legitimacy and security risks associated with potential misuse by malicious users.',\n",
       " 'Malicious users exploit the ability to create AI-generated images, posing challenges for the media forensic community in detecting such images and maintaining image integrity in investigations. This can lead to the planting of synthetic images at crime scenes or as false leads, and the creation of fake websites or social media profiles to spread misinformation and commit fraud.',\n",
       " \"The rapid advancement in image synthesis makes it difficult to differentiate between synthetic and real images, necessitating automated tools to detect GAN-generated content. Although high-quality, these synthetic images often contain explicit artifacts (pixel anomalies) and implicit artifacts (fingerprints from the generative model's architecture). Different GAN architectures leave unique signatures that are not easily transferable between models, complicating the creation of generalized detection tools. Previous research has utilized the Fourier domain to identify distinct features that differentiate GAN-generated images from real ones, as these features manifest as specific peaks in the Fourier spectrum, a result of up-sampling operations specific to each GAN architecture.\",\n",
       " \"The paper presents an extensive study on differentiating real images from GAN-generated images using a fine-tuned pre-trained CNN model. A newly collected dataset incorporates various image synthesis modalities and GAN architectures to enhance the model's generalization capability. The approach includes visualization of the model's focus during classification through Class Activation Maps (CAMs).\",\n",
       " 'The text or table segment is not provided, so the summary cannot be generated. Please provide the specific content for summarization.',\n",
       " 'The paper is organized into sections that follow the introduction.',\n",
       " 'Section 2 covers a brief introduction to related works.',\n",
       " 'Data collection and methods are outlined in Section 3, experiments are conducted in Section 4, and results along with limitations are discussed in Section 5.',\n",
       " 'The paper concludes with future work discussed in Section 6, showcasing examples of GAN-generated images that display various artifacts, including pixel anomalies, artificial fingerprints, and spectral artifacts.',\n",
       " 'Please provide the table or text chunk for summarization.',\n",
       " 'Image generation involves creating realistic high-quality images from sources such as other images, sketches, or text.',\n",
       " 'Image synthesis in computer vision has seen significant research advancements over the past decade, particularly with the development of GANs, which produce highly realistic artificial images that can be difficult to differentiate from real ones.',\n",
       " 'The digital media and content landscape can be enhanced, but it also carries risks related to authenticity, genuineness, and security.',\n",
       " 'Image-to-image synthesis involves converting images from one domain to another by altering specific features while preserving content, such as transforming segmentation masks into color images.',\n",
       " 'Conditional GAN (cGAN) extends traditional GAN by introducing an additional input, such as class labels, for both the generator and discriminator, enabling the generation of synthetic data based on specific conditions.',\n",
       " 'OASIS utilizes a modified GAN architecture with a discriminator designed as a U-Net based semantic segmentation network, leveraging label maps as ground truth to produce images that align well with their semantic labels.',\n",
       " \"LabelMix regularization enhances the discriminator's focus on semantic differences in images, prompting a re-design of both the discriminator and the generator, which now synthesizes multi-modal data via 3D noise sampling.\",\n",
       " 'CC-FPSE utilizes a conditional GAN for semantic image synthesis, allowing for partial or complete changes to images, thereby increasing the diversity of generated images through the use of semantic layouts.',\n",
       " 'The generator uses convolutional kernels based on semantic label maps to create feature maps.',\n",
       " 'The feature pyramid semantic-embedding discriminator predicts real/fake scores and semantic alignment scores to improve the alignment of synthesized images with their label maps and enhance fine details.',\n",
       " 'SPADE is based on the pix2pix model and incorporates a spatially adaptive normalization layer with learned parameters to preserve semantic information, resulting in the production of photo-realistic images.',\n",
       " 'SPADE normalizes activations from the prior layer, preserving input information, unlike InstanceNorm which often loses semantic details in the generation process.',\n",
       " 'Taming-transformers merges transformers with CNNs in an encoder-decoder architecture to create a discrete spatial codebook. It utilizes VQ-GAN and perceptual loss for local realism while the transformer focuses on global image composition, allowing for high-resolution image generation.',\n",
       " 'Sketch-to-Image Synthesis transforms simple sketches into photo-realistic images, enhancing basic structural details into visually rich, colored representations.',\n",
       " 'S2I-DetectoRS, S2I-HTC, S2I-QueryInst, and S2I-MaskRCNN utilize a common framework with four main stages. The first stage employs a pre-trained instance segmentation model from the MS-COCO dataset, specifically fine-tuning DetectoRS, HTC, QueryInst, or Mask R-CNN using various edge maps to segment 92 classes from the dataset, followed by a semantic segmentation stage.',\n",
       " 'The process involves using a semantic mask map with an image-to-image translation model called SPADE to create a photo-realistic image, followed by a post-processing stage to improve the background and refine human faces.',\n",
       " 'Text-to-image synthesis transforms natural language descriptions into realistic images, effectively condensing long sentences into single images. AttnGAN enhances this process by incorporating an attention module to generate detailed images, using word vectors and a sentence vector derived from the input text.',\n",
       " 'The generation process consists of two stages: first, a low-quality image is produced from a sentence vector, followed by multi-stage refinement focusing on specific areas based on related word vectors. ControlGAN employs an attention module for fine-tuning, utilizing word-level spatial and channel-wise attention to target subregions corresponding to relevant words. A word-level discriminator is also used to provide feedback, allowing the generator to modify specific visual attributes of subregions without affecting the overall image.',\n",
       " 'DM-GAN+CL is a proposed method for text-to-image synthesis using contrastive learning to address discrepancies in image captions assigned by different annotators, aiming to generate images that are more consistent with the corresponding ground truth.',\n",
       " 'Improves semantic consistency.',\n",
       " 'The provided content is insufficient to summarize any specific table or text. Please provide more details or additional context.',\n",
       " 'DF-GAN is a one-stage model that generates high-quality images directly from text, using a Deep text-image Fusion Block (DFBlock) to improve semantic consistency. It incorporates a Matching-Aware Gradient Penalty (MA-GP) in the discriminator to promote the generation of realistic images.',\n",
       " '3.',\n",
       " 'The section introduces the Real or Synthetic Images (RSI) dataset used for training and testing, followed by a detailed discussion of the proposed framework.',\n",
       " 'The proposed method for GAN-generated image recognition involves a framework illustrated in Figure 3, utilizing a CNN model with transfer learning from ImageNet and a new head for recognizing real and synthetic images across two classes. Data collection included images synthesized from 12 different models across three tasks: image-to-image, sketch-to-image, and text-to-image, leveraging the COCO-Stuff dataset for improved generalization and accuracy.',\n",
       " 'The COCO dataset has 164k images with instance-level annotations, augmented for pixel-level stuff annotations and split into 118k training, 5k validation, and 40k testing images. The COCO-Stuff dataset includes 172 classes: 80 thing classes, 91 stuff classes, and 1 unlabeled class, supporting various tasks like classification, detection, segmentation, scene understanding, captioning, and image generation.',\n",
       " 'The document presents examples of synthesized images generated through various tasks, utilizing different input types and GAN models. It specifies a dataset comprising 24,000 images for training, 12,000 for validation, and 12,000 for testing, categorized by tasks such as image-to-image, sketch-to-image, and text-to-image synthesis.',\n",
       " 'The image sets contained an equal number of real and synthetic images.',\n",
       " 'Table 1 contains additional details regarding the data collection process.',\n",
       " 'No information provided for summarization.',\n",
       " 'The dataset includes 48,000 total images divided into training, validation, and testing sets (24,000 for training and 12,000 each for validation and testing) across various tasks: image-to-image synthesis (OASIS, CC-FPSE, SPADE, Taming-transformers, $2I-DetectoRS, S2I-HTC, $2I-QueryInst, S2I-MaskRCNN, AttnGAN), and text-to-image synthesis (DM-GAN+CL, DF-GAN, ControlGAN). Image collection relied on the COCO-Stuff dataset, with diverse inputs generated, including text, sketches, and semantic segmentation masks for respective synthesis tasks. Sketches were created using a dodging and burning algorithm to ensure structural fidelity.',\n",
       " 'The study involved 12 image synthesis models that were tested using a pre-trained model on generated inputs to create synthetic images. Both real and synthetic images were resized to a resolution of 256 x 256, irrespective of the original resolutions of the synthesis models.',\n",
       " 'The methodology involves developing an automated tool to reliably and promptly identify GAN-generated images by fine-tuning a pre-trained classifier on the newly collected RSI dataset.',\n",
       " 'Training a classifier from scratch is time-consuming, costly, and less efficient in classification performance.',\n",
       " 'Transfer learning is considered a more effective approach.',\n",
       " 'The training process involved binary classification to differentiate between real and synthetic images using various classifiers, aiming to find the most effective model for detecting and localizing GAN-generated images.',\n",
       " 'The dataset was fine-tuned on various models including VGG19, ResNet versions (50, 101, 152), InceptionV3, Xception, DenseNet121, InceptionResNetV2, MixConv, MaxViT, and EfficientNetB4. The original head of classifiers pre-trained on ImageNet was replaced with a new head that includes global average pooling, a ReLU dense layer, batch normalization, a dropout layer, and a final dense layer with sigmoid activation. Training utilized a batch size of 64 and an initial learning rate of 0.001.',\n",
       " 'The training involved 20 epochs with checkpoints saved for improvements in validation loss, allowing the best model to be loaded during testing. The learning rate was automatically reduced when validation loss plateaued, and data augmentation via horizontal flipping was applied to the training set. Adam was used as the optimizer for all classifiers except ResNet101, which utilized RMSprop.',\n",
       " 'The input does not contain sufficient information to provide a summary. Please provide the relevant table or text for summarization.',\n",
       " 'The section presents evaluation metrics used to assess the performance of methods for detecting and localizing GAN-generated images among real ones. Eight metrics were employed: Precision, Recall, F1 score, Accuracy, Average Precision (AP), Area Under Curve of Receiver Operating Characteristic (ROC-AUC), False Positive Rate (FPR), and False Negative Rate (FNR). A performance evaluation was conducted on a dataset (RSI) of 12,000 images, equally divided between GAN-generated and real images, to validate the methods proposed. An ablation study further examined the effectiveness of the approach across different image synthesis modalities and GAN models.',\n",
       " 'EfficientNetB4 demonstrated the highest performance in recognizing and localizing GAN-generated images on the RSI dataset, achieving 100% accuracy, while InceptionV3 followed with 98% accuracy.',\n",
       " 'The table provides performance metrics for various classifiers on a testing set, including Precision, Recall, Accuracy, Average Precision (AP), ROC-AUC, False Positive Rate (FPR), and False Negative Rate (FNR). Notably, EfficientNetB4 achieved the highest scores across all metrics with perfect 1.00 values, while InceptionV3 and Xception also demonstrated strong performance with values close to 1. Other models like ResNet101 and DenseNet121 also performed well, but had slightly lower metrics. Additionally, the text mentions the use of Class Activation Maps (CAM) for visualizing model focus during the classification of real and GAN-generated images, confirming 100% accuracy with the best model.',\n",
       " \"GradCAM, AblationCAM, LayerCAM, and Faster ScoreCAM were used to analyze images and determine their contributions to the model's classification decisions, as illustrated in Figure 6.\",\n",
       " 'The content references various image analysis techniques, specifically GradCAM, AblationCAM, LayerCAM, and Faster ScoreCAM, as outlined in a study in J. Imaging 2023.',\n",
       " 'The EfficientNetB4 model primarily focused on the background of GAN-generated images during the visualization of CAM variants on the testing set.',\n",
       " 'The analysis identified regions in the synthetic images that exhibited distortion and anomalies.',\n",
       " \"The method's predictions were explained using the LIME explanation technique, illustrated in Figure 7.\",\n",
       " 'The table illustrates the visualization of the LIME interpretability technique for distinguishing between GAN-generated synthetic images and real images, showcasing super-pixel and region-based explanations for both image types.',\n",
       " 'LIME explanations are provided for GAN-generated and real images, illustrating significant super-pixels contributing to classification. Green super-pixels increase the probability of a label, while red super-pixels decrease it.',\n",
       " 'The effectiveness of the model in recognizing GAN-generated images was tested through an experiment where three models were trained separately based on different input modalities.',\n",
       " 'Three experiments were conducted using different model training and testing combinations: (1) trained on sketch-to-image and text-to-image (S2I_T2I) and tested on image-to-image; (2) trained on image-to-image and text-to-image (I21_T2I) and tested on sketch-to-image; (3) trained on image-to-image and sketch-to-image (I2I_S2I) and tested on text-to-image. Results are detailed in Table 3.',\n",
       " 'The S2I_T2I model achieved the highest accuracy of 0.99, followed by I2I_S2I at 0.95, while the I2I_T2I model had lower accuracy at 0.83 due to the image enhancement step used.',\n",
       " \"The GAN detection model achieved a high accuracy of 0.83 in identifying GAN-generated images. Its performance metrics included precision of 0.99, recall of 0.99, and F1 score of 0.99 for the S21_T2I modality. The I21_T2I modality showed slightly lower metrics with a precision of 0.87 and recall of 0.83. The I21_S21 modality demonstrated strong performance with a precision of 0.96 and recall of 0.95. The model effectively detected GAN-generated images even when not specifically trained on the evaluated modality. An ablation study was conducted to further validate the method's effectiveness.\",\n",
       " 'The second-best model, InceptionV3, was evaluated on the testing set of image synthesis models individually because the best model, EfficientNetB4, achieved 100% accuracy, leaving no performance differences when part of the dataset was assessed.',\n",
       " \"The second-best model achieved the highest accuracy of 98.35% with GAN-generated images from text inputs, while the lowest accuracy of 96.90% came from images based on semantic segmentation maps. Performance evaluations in Table 5 indicate that detecting GAN-generated images from semantic segmentation is more challenging compared to those produced from text descriptions. The confusion matrices in Figure 8 illustrate the model's accuracy across image-to-image, sketch-to-image, and text-to-image synthesis inputs. Overall, image-to-image synthesis yields more photo-realistic results than text-based generation.\",\n",
       " 'The table presents the performance of the InceptionV3 model on GAN-generated images, evaluating various models based on metrics such as Precision, Recall, F1 Score, Accuracy, Average Precision (AP), Area Under Curve (AUC), False Positive Rate (FPR), and False Negative Rate (FNR). Most models achieve high scores, with notable performances including OASIS (Accuracy: 0.964), CC-FPSE and SPADE (Accuracy: 0.978), and several models achieving an Accuracy of 0.980 or higher, indicating strong overall effectiveness in classifying GAN-generated images.',\n",
       " 'The model was tested on additional datasets to evaluate its effectiveness in distinguishing generated images from real ones.',\n",
       " 'The first model from each modality listed in Table 1 was chosen for the experiment.',\n",
       " 'OASIS, S2I-DetectoRS, and AttnGAN were utilized as models for image-to-image, sketch-to-image, and text-to-image synthesis, respectively, using unrelated datasets to generate synthetic images.',\n",
       " \"The study utilized the ADE20K, Sketchy, and Caltech-UCSD Birds-200-2011 datasets and employed EfficientNetB4 to differentiate between synthetic and real images, evaluating the model's performance with eight common metrics.\",\n",
       " 'Key performance metrics adopted include Precision, Recall, F score, Accuracy, Average Precision (AP), ROC-AUC, False Positive Rate (FPR), and False Negative Rate (FNR).',\n",
       " \"The OASIS image-to-image synthesis model was tested using the ADE20K dataset's 2000 semantic segmentation images, producing synthesized images for evaluation. The S2I-DetectoRS model utilized a subset of 2470 sketches from the Sketchy dataset, aligned with 35 classes from the MS COCO dataset, for sketch-to-image synthesis.\",\n",
       " 'Sketches were transformed into colored images during the generation phase.',\n",
       " 'Both generated and real images were evaluated using eight metrics with the best model, EfficientNetB4. The procedure mirrored that of the AttnGAN model, utilizing the CUB-200-2011 dataset, which comprises 5,794 captions/descriptions for testing.',\n",
       " 'Performance was evaluated using adopted metrics in the final step.',\n",
       " 'The experimental results indicate that the model successfully detected and recognized GAN-generated images from real ones, achieving a maximum accuracy of 98% with the CUB-200-2011 dataset and the AttnGAN, while reaching 89% accuracy with the ADE20K dataset and the OASIS model.',\n",
       " \"The dataset's complexity and the generator's capability may contribute to the observed outcomes.\",\n",
       " 'The model serves as an evaluation tool applicable to various input modalities.',\n",
       " 'The table presents the performance metrics of the best model across various datasets that were not included in the training phase, highlighting its generalization capabilities.',\n",
       " 'The table presents performance metrics of different models used for detecting GAN-generated images across various datasets. Performance measures include Precision, Recall, F1 Score, Accuracy, Average Precision (AP), Area Under Curve (AUC), and False Positive Rate (FPR). The models evaluated are OASIS on ADE20K, S21 on Sketchy, and AttnGAN on CUB-200-2011, showing high performance with accuracy scores ranging from 0.889 to 0.978. The discussion notes limitations and emphasizes the evaluation of fine-tuned classifiers on a new dataset.',\n",
       " 'Imaging 2023 discusses synthetic images (RSI), focusing on their relevance and applications.',\n",
       " 'The evaluation process utilized a testing set of 12,000 RSI images.',\n",
       " 'EfficientNetB4 recorded the highest accuracy of 100% on the evaluated dataset, demonstrating its effectiveness in detecting GAN-generated images. A subsequent experiment involved fine-tuning and re-training EfficientNetB4 using two input modalities from the RSI training set.',\n",
       " 'The modality-based trained model was evaluated using the third excluded input modality from the RSI testing set.',\n",
       " 'The model achieved accuracy rates of 99%, 83%, and 95% when excluding I2I, S2I, and T2I modalities, respectively. The dip to 83% accuracy upon excluding the S2I modality is linked to background improvement and face refinement processes in sketch-to-image models, which enhance image quality by aligning synthetic backgrounds with real, high-quality ones and reconstructing faces.',\n",
       " 'Post-processing steps enhance the quality of synthetic images.',\n",
       " 'The model demonstrated an 83% accuracy in recognizing GAN-generated images even without prior training on the S2I modality and successfully identified generated images from different datasets, indicating high detection capability.',\n",
       " 'The model is applicable for detecting synthetic images, aiding forensic investigations, and combating AI misuse and cyberattacks. It utilizes Class Activation Maps (CAM) to identify influential image regions for classification, primarily focusing on the background of generated images.',\n",
       " 'Generators tend to focus more on the foreground than the background, resulting in artifacts during background generation. Occasionally, the model incorporates aspects of the foreground due to anomalies and distortions, leading to classification failures despite high accuracy overall.',\n",
       " 'The model struggles to classify images accurately in cases of blurriness, low quality, vintage appearance, or motion. It also misclassifies GAN-generated images as fake when fine details and sharper textures are present. To improve accuracy, these misclassification factors should be incorporated into the training dataset.',\n",
       " 'The classifier improves its ability to classify images by learning about various aspects.',\n",
       " 'The content provided is insufficient for summarization.',\n",
       " 'The table presents probabilities of \"Real\" and \"Fake\" classifications across different conditions (GT, P Real, P Fake, T2I). Key values include P(Real) for various categories, with notable percentages for Real as follows: GT (63.91), P Real (73.69), and T2I (99.98), while P(Fake) values are 36.09 for GT and 10.72 for P Fake. Lower probabilities are also noted for auxiliary real classifications, with figures like 0.03 and 0.82.',\n",
       " 'The table presents misclassification probabilities from a model on different datasets (ADE20K, Sketchy, CUB-200-2011), with a high proportion of predictions classified as fake (ranging from 76.92% to 99.97%) and a low probability of real predictions (4.11%). Ground truth is denoted as GT, while misclassified predictions are highlighted in red as P.',\n",
       " 'The rapid advancement of AI-based image synthesis is making generated images increasingly photo-realistic, which can enhance media but also raises security and authenticity concerns. Consequently, there is a need for automated tools to detect and localize these AI-generated images.',\n",
       " 'The paper presents a machine model for distinguishing AI-generated images from real ones, utilizing a large dataset (RSI) derived from various GAN-based tasks. A CNN model was effectively trained on this dataset, achieving high accuracy in detecting synthetic images, even when tested on unfamiliar datasets. The study highlights that GAN-generated images exhibit specific flaws and artifacts, undetectable by humans but identifiable by the trained model. This capability aids the multimedia forensics community in countering potential threats arising from advancements in AI image synthesis.',\n",
       " 'Researchers should focus on minimizing traces during image generation to produce high-quality, photo-realistic images. Future plans include integrating various image synthesis tasks across different architectures and datasets, such as facial and biological images like Western blot and microscopic images.',\n",
       " 'The project aims to combine the frequency and spatial domains of a dataset and improve a GAN-generated image detector with techniques to counter adversarial attacks that could deceive the detector. Contributions include conceptualization and methodology by S.S.B.',\n",
       " 'Roles in the project include software development (S.S.B.), data curation (S.S.B.), validation (S.S.B.), formal analysis (S.S.B.), investigation (S.S.B. and T.V.N.), and original draft writing (S.S.B.).',\n",
       " 'T.V.N. is responsible for writing, reviewing, and editing, while S.S.B. is also involved in the process.',\n",
       " 'All authors contributed to the visualization and supervision of the manuscript and have agreed to the published version.',\n",
       " 'The article from J. Imaging, 2023, indicates that Open Access funding is provided by the University of Dayton Libraries, with the second author receiving support from NSF grant # 2025234.',\n",
       " 'No Institutional Review Board statement is applicable.',\n",
       " 'Informed consent statement is not applicable.',\n",
       " 'Datasets from the study are available from the corresponding author upon reasonable request. The first author acknowledges support from Umm Al-Qura University in Saudi Arabia.',\n",
       " \"The University of Dayton's Office for Graduate Academic Affairs partially funded this work through a Graduate Student Summer Fellowship.\",\n",
       " 'The authors report no conflicts of interest.',\n",
       " 'No content provided for summarization.',\n",
       " 'The article by Baraheem, Le, and Nguyen reviews various methods for image synthesis, discusses relevant datasets and evaluation metrics, and provides a future outlook on the field.',\n",
       " 'The text or table regarding \"Intell\" is not provided. Please provide the relevant content for summarization.',\n",
       " 'The input does not provide sufficient information to summarize.',\n",
       " 'The data pertains to the year 2023 and includes a numerical range from 56 to 10865.',\n",
       " 'The provided text does not contain a table or specific content to summarize.',\n",
       " 'The referenced paper discusses CAN (Creative Adversarial Networks), which generate art by understanding various artistic styles and deliberately deviating from established style norms.',\n",
       " '3.',\n",
       " 'The referenced work presents a method for image editing that utilizes recurrent attentive models to enhance language-based interactions with images.',\n",
       " 'The paper by Yan et al. (2014) discusses a method for automatic photo adjustment utilizing deep neural networks, as detailed in their arXiv publication.',\n",
       " 'The request lacks the necessary information to provide a summary. Please provide the table or text chunk for summarization.',\n",
       " 'The paper by Yu et al. (2018) presents a method for generative image inpainting using contextual attention, which improves the quality of image restoration by focusing on relevant contextual information during the filling process.',\n",
       " 'The provided information is insufficient to summarize. Please provide more context or details.',\n",
       " '6.',\n",
       " 'Nazeri et al. introduced EdgeConnect, a method for generative image inpainting that utilizes adversarial edge learning, presented in their 2019 paper on arXiv.',\n",
       " 'Insufficient information provided to summarize.',\n",
       " 'The dissertation by Thaung (2020) explores advanced data augmentation techniques using Generative Adversarial Networks (GANs) and Computer-Aided Design (CAD).',\n",
       " 'The link provided leads to a record on the DiVA portal, accessed on April 1, 2022.',\n",
       " 'The cited work is a conference paper titled \"Generative adversarial nets\" by Goodfellow et al., presented at the 27th International Conference on Neural Information Processing Systems in Montreal, Canada, from December 8-13, 2014.',\n",
       " 'The data from lines 2672 to 2680 appears to focus on specific cases or instances, likely detailing outcomes or findings related to various subjects. Each entry summarizes particular events, characteristics, or outcomes, providing a snapshot of the larger context in which these cases exist.',\n",
       " 'The content provided does not include sufficient information to summarize.',\n",
       " 'The text appears to list the authors: Marra, F., Gragnaniello, D., Verdoliva, L., and Poggi, G.',\n",
       " 'The paper discusses whether Generative Adversarial Networks (GANs) produce identifiable patterns or \"fingerprints\" in generated content, presented at the 2019 IEEE Conference on Multimedia Information Processing and Retrieval.',\n",
       " \"The provided text or table from 506-511 is missing, so I'm unable to summarize it. Please provide the relevant information for summarization.\",\n",
       " 'The provided content is insufficient to summarize. Please provide a complete table or text for summarization.',\n",
       " 'The work by Yu, Davis, and Fritz focuses on identifying and analyzing fingerprints of Generative Adversarial Networks (GANs) to attribute fake images to specific GAN models.',\n",
       " 'International Conference on Computer Vision occurred in Seoul, South Korea, from 27 October to 2 November 2019.',\n",
       " 'The document discusses a study presented at the 2019 IEEE International Workshop on Information Forensics and Security, focusing on detecting and simulating artifacts in images generated by Generative Adversarial Networks (GANs).',\n",
       " 'Please provide the table or text chunk for summarization.',\n",
       " 'The paper by Dzanic et al. discusses inconsistencies in the Fourier spectrum of images generated by deep networks, highlighting potential issues in the fidelity of these generated images.',\n",
       " '12.',\n",
       " 'The paper discusses utilizing frequency analysis techniques to improve the recognition of deep fake images, presented at the 37th International Conference on Machine Learning in July 2020.',\n",
       " '13.',\n",
       " 'The paper by Sushko et al. discusses the necessity of adversarial supervision in the context of semantic image synthesis, presented in an arXiv paper published in 2020.',\n",
       " '14 is a significant number that often represents various concepts in mathematics and culture, including its role in counting, its representation as an integer, and its appearances in historical and cultural contexts.',\n",
       " 'The paper by Liu et al. (2019) focuses on developing a method for semantic image synthesis by predicting layout-to-image conditional convolutions.',\n",
       " 'The content provided is insufficient for summarization.',\n",
       " \"The provided chunk, labeled '16,' does not include any contextual information or data to summarize.\",\n",
       " 'The paper by Park et al. discusses semantic image synthesis using a method called spatially-adaptive normalization, presented at the 2019 CVPR conference held in Long Beach, CA.',\n",
       " 'The work by Esser, Rombach, and Ommer presents techniques for utilizing transformers in high-resolution image synthesis, showcased at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) held in Nashville, TN.',\n",
       " 'Please provide the content you would like summarized.',\n",
       " 'The work by Baraheem and Nguyen discusses sketch-to-image synthesis using semantic masks, as presented in a publication by Springer in 2023.',\n",
       " '1-20 consists of a set of data points or information that needs to be summarized into key insights. The focus would be on extracting essential details without added commentary or context for clarity.',\n",
       " 'No content provided to summarize.',\n",
       " 'The paper presents AttnGAN, a novel approach for fine-grained text to image generation using attentional generative adversarial networks, introduced by Xu et al. in 2017.',\n",
       " '19 is a number that typically represents a quantity, age, or label, depending on context. It can signify various concepts in different fields, such as mathematics, legal age, or historical events.',\n",
       " '20',\n",
       " 'The work by Ye et al. (2021) focuses on enhancing text-to-image synthesis through the application of contrastive learning techniques.',\n",
       " 'The content refers to a specific item labeled \"21,\" but no additional details or context are provided to summarize.',\n",
       " 'The paper presented by Tao et al. introduces DF-GAN, a framework for text-to-image synthesis that utilizes deep fusion generative adversarial networks. It was discussed at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in June 2022.',\n",
       " 'The table spans entries numbered 16515 to 16525, detailing various data points such as identifiers, values, and associated metadata, relevant to a specific dataset or study. Each entry contains columns for distinct variables, displaying a range of information in a structured format.',\n",
       " 'The provided text references \"CrossRef\" and includes an incomplete citation, indicating partial information with no discernible table or data to summarize.',\n",
       " 'The provided text references a publication or article from \"Imaging 2023\" with the identification number 9,199, indicating a specific study or topic within that journal.',\n",
       " 'The paper discusses a method for controllable text-to-image generation, presenting an approach that allows for enhanced manipulation and fine-tuning of generated images based on textual descriptions.',\n",
       " 'The information is insufficient to provide a summary. Please provide additional context or details.',\n",
       " 'The cited work presents Conditional Generative Adversarial Nets (CGANs), which extend traditional GANs by conditioning the generation process on additional information, allowing for more controlled and meaningful image synthesis.',\n",
       " 'The paper by Ronneberger, Fischer, and Brox presents U-Net, a convolutional neural network architecture specifically designed for biomedical image segmentation, and was published on arXiv in 2015.',\n",
       " 'The number 24 is a whole number that follows 23 and precedes 25. It is an even number and can be expressed as a product of prime factors: 2^3 × 3. In various contexts, 24 may represent a quantity, a time duration, or an identifier.',\n",
       " 'The paper discusses the use of conditional adversarial networks for image-to-image translation, presenting a method that pairs input images with desired output images to achieve versatile transformations across various applications.',\n",
       " '25.',\n",
       " 'The paper by Ulyanov, Vedaldi, and Lempitsky discusses instance normalization as a key component for enhancing the speed of image stylization processes, highlighting its significance in achieving fast and effective stylization techniques.',\n",
       " 'The information provided is insufficient to generate a summary, as \"26\" does not offer context or details. Please provide a specific table or text for summarization.',\n",
       " 'Please provide the specific table or text chunk you would like summarized.',\n",
       " 'Lin et al. (2014) presented the Microsoft COCO dataset, which focuses on common objects in context, during the 13th European Conference on Computer Vision (ECCV) held in Zurich, Switzerland.',\n",
       " 'The data or text from this range (740-755) has not been provided for summarization. Please provide the specific content to be summarized.',\n",
       " 'The content provided is insufficient to summarize, as no specific table or text was included.',\n",
       " 'The paper by Qiao, Chen, and Yuille presents DetectoRS, a method for object detection that utilizes a recursive feature pyramid and switchable atrous convolution, and was discussed at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) in Nashville, TN.',\n",
       " 'The information provided is insufficient to summarize. Please provide more context or details.',\n",
       " 'The paper presents the Hybrid Task Cascade (HTC) method for instance segmentation, enhancing performance by integrating multiple tasks in a unified framework. It was published in 2019 on arXiv.',\n",
       " '30.',\n",
       " 'Fang et al. (2021) propose a method titled \"Instances as Queries\" in their paper published on arXiv, which focuses on leveraging instances to formulate queries for improved information retrieval.',\n",
       " 'Mask R-CNN was presented by He, K., Gkioxari, G., Dollar, P., and Girshick, R. at the 2017 IEEE International Conference on Computer Vision (ICCV) held in Venice, Italy, from October 22-29, 2017.',\n",
       " 'The content provided for summarization is incomplete or missing. Please provide a complete table or text chunk for summarization.',\n",
       " 'The content appears to be a single numerical value, 32.',\n",
       " 'Beyeler, M. authored a book titled \"OpenCV with Python Blueprints,\" published by Packt Publishing in London, UK, in 2015. The book focuses on designing and developing advanced computer vision projects using OpenCV with Python, and has the ISBN 978-178528269-0.',\n",
       " 'The paper by Xie and Tu presents a method for edge detection using a holistically-nested approach, and was presented at the IEEE International Conference on Computer Vision (ICCV) in December 2015 in Santiago, Chile.',\n",
       " 'The content provided does not include any specific table or text to summarize. Please provide the relevant information for summarization.',\n",
       " 'Ding and Goshtasby discuss the Canny edge detector in their work published in Pattern Recognition, highlighting its significance in image processing for edge detection.',\n",
       " 'The information seems to reference a publication from 2001, appearing on pages 721 to 725 of a document, likely an article or research paper.',\n",
       " 'The referenced work discusses the design of an image edge detection filter utilizing the Sobel operator, published in the IEEE Journal by authors anopoulos, N., Vasanthavada, N., and Baker, R.L.',\n",
       " 'The document references a publication from 1988 in the journal \"Solid-State Circuits,\" detailing research or findings related to solid-state electronics within the specified page range of 358-367.',\n",
       " 'The reference lists authors Caesar, H., Uijlings, J., and Ferrari, V.',\n",
       " 'The COCO-stuff paper presented at the 2018 IEEE/CVF Conference focuses on the classification of both \"thing\" and \"stuff\" categories within visual contexts, detailing the context-aware categorization in computer vision.',\n",
       " 'The range 1209-1218 refers to numerical values or records, but no specific content is provided to summarize. More context is needed for an accurate summary.',\n",
       " '36.',\n",
       " 'Deng et al. (2009) presented a large-scale hierarchical image database called ImageNet at the IEEE Conference on Computer Vision and Pattern Recognition, held in Miami, FL.',\n",
       " 'The text discusses various topics from pages 248 to 255, including key findings, significant events, or trends relevant to the subject at hand. It likely covers specific details, case studies, or analyses that provide insights or conclusions drawn from the data or narrative presented in those pages.',\n",
       " 'The content consists of the number 37.',\n",
       " 'The paper by Simonyan and Zisserman presents a novel architecture for very deep convolutional networks aimed at improving large-scale image recognition tasks, as detailed in their research published in 2014 on arXiv.',\n",
       " 'The article discusses deep residual learning techniques for image recognition, presented by He, Zhang, Ren, and Sun in 2015.',\n",
       " \"The work by Szegedy et al. (2015) presents a re-evaluation of the Inception architecture, focusing on its application within computer vision tasks. The authors discuss improvements and innovations that enhance the network's performance and efficiency.\",\n",
       " 'Chollet, F. presented Xception, a deep learning model utilizing depthwise separable convolutions at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) held in Honolulu, HI, from July 21-26, 2017.',\n",
       " 'The data ranges from 1251 to 1258, likely detailing specific entries or metrics in a structured format. Without the specific content of the table or text, it is unclear what information is being presented.',\n",
       " 'The paper discusses densely connected convolutional networks (DenseNets), which enhance feature propagation and reuse through dense connections between layers, resulting in improved performance in various tasks, particularly in computer vision.',\n",
       " 'The paper by Szegedy et al. discusses Inception-v4 and Inception-ResNet architectures, analyzing the effects of residual connections on learning in deep neural networks, presented in a 2016 arXiv publication.',\n",
       " 'Tan and Le introduced MixConv, a method involving mixed depthwise convolutional kernels, in a 2019 arXiv paper.',\n",
       " 'The referenced work is on \"Maxvit: Multi-axis vision transformer,\" presented at the European Conference on Computer Vision in 2022, authored by Tu et al. and published by Springer Nature in Cham, Switzerland.',\n",
       " 'The text covers pages 459-479, detailing specific data or information relevant to the topic at hand, but without the actual content, a precise summary cannot be generated. Please provide the necessary data or context for a complete summary.',\n",
       " 'EfficientNet is a paper by Tan and Le that proposes a new approach to scaling convolutional neural network models, emphasizing efficiency and performance improvements through a systematic model scaling strategy.',\n",
       " \"Agarap's 2018 paper discusses the use of Rectified Linear Units (ReLU) in deep learning, as presented in a preprint on arXiv.\",\n",
       " 'The paper by Ioffe and Szegedy introduces Batch Normalization, a technique that accelerates the training of deep neural networks by minimizing internal covariate shift.',\n",
       " 'The text references a paper by Srivastava et al. discussing \"Dropout,\" a technique designed to reduce overfitting in neural networks.',\n",
       " '\"Mach\" refers to the Mach number, a dimensionless unit measuring the speed of an object relative to the speed of sound in the surrounding medium. It is commonly used in aerodynamics to describe supersonic and subsonic speeds.',\n",
       " 'The content emphasizes the importance of learning, suggesting it is a fundamental process that leads to personal growth and development.',\n",
       " 'The reference pertains to a publication from 2014, specifically volume 15, pages 1929 to 1958.',\n",
       " 'The document discusses a stochastic optimization method proposed by Kingma and Ba in 2014, detailing the Adam algorithm designed for improving optimization in machine learning.',\n",
       " 'The content is missing or unspecified, so a summary cannot be provided.',\n",
       " 'The paper by Graves (2013) discusses the use of recurrent neural networks (RNNs) for generating sequences, outlining techniques for training RNNs and their application in tasks like language modeling and text generation. It highlights the architecture of RNNs and presents experimental results that demonstrate their effectiveness in generating coherent sequences.',\n",
       " 'The paper by Selvaraju et al. presents Grad-CAM, a method for generating visual explanations from deep neural networks using gradient-based localization techniques.',\n",
       " 'The input provided is a single letter, \"J.\" There is no additional context or information to summarize.',\n",
       " 'The text does not provide enough context or information to generate a summary.',\n",
       " 'The text or table related to \"Vis\" is not provided; therefore, a summary cannot be generated.',\n",
       " 'In 2020, the values range from 128 to 336-359.',\n",
       " 'The entry titled \"CrossRef\" does not contain enough contextual information or data to provide a summary.',\n",
       " 'The provided text or table chunk is insufficient for summarization. Please provide additional details or context.',\n",
       " 'Desai and Ramaswamy presented Ablation-CAM, a method for providing visual explanations for deep convolutional networks using gradient-free localization, at the 2020 IEEE Winter Conference on Applications of Computer Vision.',\n",
       " 'The content spans pages 972 to 980, likely containing a series of entries, data points, or discussions across various topics, but specific details or context are not provided for summarization.',\n",
       " 'The paper by Jiang et al. introduces LayerCAM, a method that explores hierarchical class activation maps to enhance localization in deep learning models, as published in the IEEE Transactions.',\n",
       " 'The text outlines various techniques and methods used for image processing, focusing on enhancing, analyzing, and manipulating images to improve quality and extract useful information. It covers algorithms, tools, and applications relevant to fields such as computer vision and photography.',\n",
       " 'The data refers to a publication from 2021, spanning pages 5875-5888, with a total of 30 entries or sections.',\n",
       " 'The provided content does not contain enough information to summarize.',\n",
       " 'The reference appears to be for a publication in the journal \"Imaging\" from the year 2023, specifically volume 9, page 199.',\n",
       " 'Score-CAM is a method for generating score-weighted visual explanations for convolutional neural networks, presented by Wang et al. at the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.',\n",
       " '56.',\n",
       " 'The entry lists the authors Ribeiro, M.T., Singh, S., and Guestrin, C.',\n",
       " 'The paper discusses the importance of explaining the predictions made by classifiers in machine learning, emphasizing trust in automated systems. It was presented at the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining in August 2016.',\n",
       " 'There is no data provided for summarization.',\n",
       " 'The paper presents research on scene parsing using the ADE20K dataset, discussed at the 2017 IEEE CVPR conference in Honolulu, Hawaii.',\n",
       " 'The numbers 5122-5130 represent a sequence of integers from 5122 to 5130, inclusive.',\n",
       " 'The input appears to be incomplete or does not provide sufficient context to summarize effectively. Please provide the complete table or text for a proper summary.',\n",
       " 'The paper discusses the \"sketchy database,\" a resource designed to enhance the ability to retrieve poorly drawn depictions of bunnies, contributing to advancements in image retrieval systems.',\n",
       " 'The document is a journal article published in 2016 in volume 35, pages 1-12.',\n",
       " 'No information provided to summarize.',\n",
       " 'The Caltech-UCSD Birds-200-2011 Dataset, created by Wah et al. in 2011, is published by the California Institute of Technology. The disclaimer states that the views and data in all publications are the responsibility of the authors and not of MDPI or the editors.',\n",
       " 'The document outlines a practical guide titled \"Mastering Generative AI and Prompt Engineering\" aimed at data scientists, emphasizing the growing importance of prompt engineering in the AI-driven economy. It includes chapters on understanding generative AI, prompt engineering basics, designing effective prompts, advanced techniques, ethical considerations, and real-world applications. Key topics cover different types of prompts, AI model capabilities, and ethical challenges such as bias and data privacy.',\n",
       " 'The section focuses on content generation and creative writing, likely discussing methods, strategies, and tools for producing written material and enhancing creativity in writing.',\n",
       " 'The document covers topics related to data analysis and visualization, including chatbots and conversational AI, anomaly detection, and pattern recognition. It concludes with recommended resources and online communities for further exploration in the field of artificial intelligence, which has significantly progressed due to generative AI and prompt engineering.',\n",
       " 'The ebook serves as a comprehensive guide for data scientists on generative AI and prompt engineering, highlighting the importance of staying current with trends and techniques. It focuses on models that generate new data from existing inputs, showcasing their applications in natural language processing and image generation.',\n",
       " 'Generative AI enables data scientists to create innovative solutions by understanding its complexities, while prompt engineering focuses on crafting effective prompts to guide AI models for desired outputs. The need for precise prompt engineering has increased with the advancement of AI models. An accompanying ebook covers key concepts, best practices, and real-world applications of generative AI and prompt engineering, including case studies and ethical considerations. The evolution of AI, from rule-based systems to generative models, highlights significant technological advancements over the years.',\n",
       " 'Rule-based AI systems, while effective for simple tasks, faced limitations in scaling and adapting to complex problems due to their rigidity. Machine learning emerged as a more flexible approach, enabling AI to learn patterns from data and generalize to new situations. Generative AI further advances this by generating new data patterns, excelling in applications like natural language processing and image synthesis. Key generative AI models include Restricted Boltzmann Machines (RBMs), Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers, each contributing unique capabilities to the field.',\n",
       " 'RBMs, VAEs, GANs, RNNs, and LSTMs are various generative models utilized for different tasks. RBMs are employed in image synthesis and feature learning. VAEs merge deep learning with Bayesian inference for tasks like image synthesis and text generation through encoding and decoding processes. GANs use a generator-discriminator framework to create high-quality synthetic data across various mediums. RNNs process sequential data with feedback loops, applicable in text and speech generation, while LSTMs enhance RNNs to manage long-range dependencies, addressing the vanishing gradient issue.',\n",
       " 'Transformers are neural network architectures that use self-attention mechanisms, allowing for parallel processing of sequences, which enhances efficiency for large-scale tasks. GPT, developed by OpenAI, is a notable transformer model recognized for its strong performance in natural language processing and generation, providing valuable tools for data scientists in various applications.',\n",
       " 'Generative AI is valuable in data science by creating new data from existing patterns. It significantly impacts natural language processing (NLP), enabling coherent text generation for applications like chatbots, summarization, translation, and content creation.',\n",
       " 'Generative models, such as GPT, GANs, and VAEs, have advanced significantly across various domains, including NLP, image synthesis, music generation, drug discovery, anomaly detection, and data augmentation. They facilitate the creation of realistic text, images, music, and molecular structures, while also aiding in identifying anomalies and improving machine learning performance by generating additional training data.',\n",
       " \"Generative AI enables the creation of synthetic data for simulating scenarios, aiding businesses and researchers in testing hypotheses and strategies across various fields such as finance and urban planning. The ongoing advancement of this technology is expected to yield more innovative applications. Prompt engineering, which involves crafting effective prompts for AI models, is critical for guiding generative models like GPT to produce high-quality outputs. The quality of a prompt impacts the relevance and accuracy of AI-generated responses, making prompt engineering essential for optimizing AI performance in diverse applications, including content generation and data analysis. As generative AI's complexity grows, mastering prompt engineering becomes increasingly vital for data scientists.\",\n",
       " 'Prompts for generative AI can be classified into three types: explicit, implicit, and creative. Explicit prompts give direct instructions and specify the desired output format, which leads to accurate results but may hinder creativity. Implicit prompts are more open-ended, allowing the AI model to interpret the context and potentially generate diverse outputs, though they can also produce ambiguous responses. Creative prompts aim to inspire novel and imaginative responses through open-ended scenarios, but they may require more iteration for optimal results. Best practices for crafting effective prompts include being clear and concise to ensure the AI comprehends the instructions.',\n",
       " 'Concise prompts enhance model focus and accuracy. Providing context, such as titles and publication details, guides the AI for relevant outputs. Specifying desired formats further aids in achieving structured results.',\n",
       " '- Generative AI models may produce unexpected outputs; rephrasing prompts can improve results.\\n- Finding a balance between guidance and creative freedom in prompts is essential for effective outputs.\\n- Regular evaluation and iteration of prompts enhances the quality of results from AI models.\\n- Custom prompts significantly improve the performance of generative AI in natural language processing tasks.\\n- Effective prompts for text summarization should specify details like length, format, and key points.',\n",
       " 'Effective prompts for AI summarization should provide clear instructions and context. For summarization, examples include requests for concise overviews of news articles, research papers, or book chapters, focusing on key findings and implications. For sentiment analysis, prompts should guide the model to assess emotions in specific contexts, such as customer reviews of a product.',\n",
       " \"The two prompts provide clear contexts for sentiment analysis: one focuses on Twitter users' feelings about a political controversy, while the other assesses the overall sentiment of movie reviews for a new blockbuster film. Both prompts are specific and well-defined.\",\n",
       " 'Effective prompt engineering is essential for guiding AI in generating contextually relevant text across various tasks such as text generation, question-answering, text classification, and machine translation. Specific, detailed prompts can enhance the quality of outputs by providing clear themes, context, and instructions. Examples include creating imaginative stories, explaining concepts, classifying text types, and translating languages to improve accuracy and relevance in responses. Mastering prompt crafting enables data scientists to maximize the capabilities of generative AI models in natural language processing tasks.',\n",
       " 'Generative AI models can enhance creativity and diversity in content through effective prompt engineering. Key strategies include using open-ended prompts to inspire imaginative responses, introducing constraints to foster innovative thinking, combining multiple themes for unique narratives, encouraging variability by generating multiple versions, experimenting with different styles and formats, and employing iterative prompting to refine outputs. These approaches help data scientists unlock the creative potential of AI, improving the quality and scope of applications across various fields such as entertainment, marketing, education, and research.',\n",
       " 'Generative AI models can perpetuate training data biases, leading to ethical issues. Thoughtful prompt engineering can mitigate these concerns by promoting fairness, avoiding stereotypes, generating positive content, ensuring factual accuracy, and allowing for iterative refinement. Strategies include crafting inclusive prompts, discouraging discrimination, emphasizing responsibility, and incorporating user feedback to improve AI outputs. By implementing these practices, data scientists can enhance the ethical use of AI, resulting in fairer and more balanced content.',\n",
       " 'Generative AI models can create personalized and adaptable content by utilizing prompt engineering to incorporate user preferences, language, tone, context, and feedback. This includes crafting prompts for tailored content, personalized recommendations, and customized learning experiences. However, challenges such as inherent biases in AI models and understanding their limitations must be acknowledged to improve the effectiveness and responsibility of prompt engineering strategies.',\n",
       " 'Limitations may cause unexpected, irrelevant, or nonsensical outputs despite effective prompts.',\n",
       " 'The text discusses challenges and strategies in prompt engineering for generative AI models. It highlights inherent biases, unpredictability, overfitting, evaluation difficulties, and ethical considerations that data scientists must address. Strategies for effective prompt engineering include starting with broad prompts, experimenting with varying detail levels, iterative refinement, encouraging variability, leveraging constraints creatively, and collaborating with users. Striking the right balance between guidance and flexibility is essential for maximizing creativity and ensuring relevance in AI outputs.',\n",
       " 'To ensure quality and reliability in AI-generated content, it is essential to focus on prompt engineering, which can help generate high-quality content while addressing challenges related to consistency, accuracy, and relevance. Key strategies include: \\n\\n1. **Testing and Evaluation**: Regularly assess AI-generated content with quantitative and qualitative measures against clear benchmarks to improve prompts and models.\\n2. **User Feedback and Collaboration**: Involve users in the prompt engineering process to align content with their needs and preferences, addressing areas for improvement.\\n3. **Continuous Model Improvement**: Stay informed about advancements in AI to refine models and techniques based on user input and industry practices.',\n",
       " 'Implement strong error handling and monitoring to identify and rectify issues in AI-generated content, including inaccuracies and harmful material, while ensuring adherence to quality standards through safeguards and filters.',\n",
       " 'The text discusses the importance of clear communication and transparency regarding AI-generated content, emphasizing management of user expectations and building trust. It highlights the necessity of multimodal validation to enhance content reliability and suggests that advanced AI models and techniques, such as fine-tuning and multi-modal integration, can improve the performance of generative AI systems. The text also addresses the significance of contextual understanding, active learning, and collaborative AI for personalized and efficient content generation. Additionally, it explores the convergence of human and AI creativity, emphasizing ethical practices, the democratization of creativity, and fostering interdisciplinary innovations to drive progress in various fields.',\n",
       " 'The role of prompt engineering is becoming increasingly significant in the AI-driven economy, enhancing business communications by creating tailored content, accelerating research and development through information synthesis and insight generation, personalizing customer experiences based on user data, and democratizing access to expertise across various fields. This evolution is aimed at improving effectiveness, efficiency, and innovation in diverse sectors.',\n",
       " 'Prompt engineering enhances policy-making by providing evidence-based insights, fostering creativity and innovation through the synergy of human and AI efforts, and addressing global challenges with scalable solutions. As the AI-driven economy grows, prompt engineering will be pivotal in creating effective, innovative strategies for societal issues. Practical guidelines for implementing prompt engineering are available for practitioners and data scientists aiming to improve AI-generated content.',\n",
       " \"To effectively start with prompt engineering, first understand the AI model's capabilities and limitations. Establish clear objectives relating to the target audience and intended outcomes. Begin with simple, open-ended prompts to assess the model’s performance and address any biases. Finally, adopt an iterative approach by experimenting and refining prompts based on trial and error to achieve the desired results.\",\n",
       " 'Regular evaluation of AI-generated content through quantitative and qualitative metrics is essential for maintaining quality. Engaging with the prompt engineering community fosters collaboration and learning. Ethical considerations are crucial, ensuring that content creation is responsible and inclusive. A structured workflow enhances the consistency and quality of AI-generated content, beginning with thorough planning and research on the relevant topics and audience.',\n",
       " 'The research outlines a structured approach to prompt engineering for AI-generated content, emphasizing the importance of setting clear objectives, drafting initial prompts, testing and evaluating content, iterating and refining prompts, monitoring performance, and collaborating for continuous learning. It highlights the need to address common challenges, including AI model limitations, to enhance the quality and relevance of generated content, ultimately aiding in achieving long-term success in the field of generative AI.',\n",
       " 'To improve AI content quality and address limitations, experiment with diverse prompt structures, fine-tune models on specific domain data, or combine outputs from multiple models. To mitigate biases in AI, use bias-aware prompt engineering and external tools to identify and correct biased results.',\n",
       " 'Achieving the right balance in prompt guidance is key for effective AI-generated content. This involves adjusting specificity and context based on performance metrics. Quality assurance requires clear benchmarks and mixed evaluations. Creative shortfalls can be addressed through open-ended prompts and multi-modal models. Ethical considerations must be prioritized to promote fairness and inclusivity in content creation. Ongoing refinement in these areas enhances the effectiveness and quality of generative AI output.',\n",
       " 'Establishing clear metrics and benchmarks is essential for evaluating the success of prompt engineering efforts. Key performance indicators (KPIs) to consider include the quality of AI-generated content, focusing on coherence, consistency, grammatical correctness, and readability.',\n",
       " 'The text outlines metrics for assessing AI-generated content across several dimensions: \\n\\n1. **Quality**: Utilize readability scores and grammar checkers.\\n2. **Relevance**: Align content with objectives and audience, using qualitative assessments and engagement metrics.\\n3. **Novelty and Creativity**: Measure uniqueness and originality using tools like plagiarism checkers.\\n4. **Accuracy**: Compare content against reliable sources and expert knowledge.\\n5. **User Satisfaction**: Collect feedback through surveys to identify improvement areas.\\n6. **Efficiency**: Assess prompt engineering efficiency by evaluating generation time, iterations, and costs.\\n7. **Adaptability**: Evaluate how well content performs across different channels and formats.',\n",
       " \"Establishing clear KPIs and benchmarks for prompt engineering enhances the measurement of AI-generated content's success, ensuring quality and relevance. The ebook covers generative AI fundamentals, best practices, techniques, challenges, and ethical considerations, while highlighting future trends in prompt engineering. It emphasizes the critical role of prompt engineering in various sectors to drive creativity and innovation. Lastly, it provides a list of recommended books, articles, courses, and online resources for further learning in generative AI and prompt engineering to inspire experimentation and collaboration in the field.\",\n",
       " 'Appendix B lists online platforms for discussing and collaborating on generative AI and prompt engineering, highlighting AI Stack Exchange as a key resource for Q&A on AI topics.',\n",
       " 'The Machine Learning Subreddit is a community platform for discussions on machine learning, deep learning, and generative AI, where users can share projects and ask questions. The AI section on arXiv serves as a repository for recent preprints of AI research papers.',\n",
       " 'The text describes activities such as browsing recent research, giving feedback, and participating in discussions with researchers and practitioners.',\n",
       " 'The Deep Learning Subreddit is a community for sharing resources and discussing deep learning and generative AI. The OpenAI Community forum allows users to engage in conversations about AI research, GPT models, and prompt engineering, fostering connections among AI enthusiasts.',\n",
       " 'Data Science Stack Exchange is a Q&A platform for data science professionals and enthusiasts to ask questions, provide answers, and explore discussions on data science, machine learning, and AI topics.',\n",
       " \"Participating in online communities and forums for generative AI and prompt engineering allows individuals to network, share knowledge, collaborate, learn from experts, stay updated on advancements, and contribute to the field's growth.\",\n",
       " 'The table of contents outlines the sections of a document, including a preface, glossary, introduction to Large Language Models (LLMs), and an explanation of what LLMs are, indicating the document is likely focused on the topic of LLMs.',\n",
       " \"The table outlines various topics related to language models, including comparisons between foundation and fine-tuned models, the evolution of large language models, neural networks, benefits for enterprises, challenges, construction and evaluation of models, notable companies, and popular startup applications. The accompanying text introduces language's historical significance and discusses theories regarding the evolution of speech and language.\",\n",
       " 'Language is the fundamental basis of human communication.',\n",
       " 'The digital age has amplified the importance of language, with 347.3 billion emails and over five billion text messages exchanged daily, highlighting its significance for enterprises in extracting insights and trends. Analyzing customer feedback can inform product development, while language production, such as blog posts, enhances brand visibility. However, both tasks are time-consuming and can hinder decision-making. Large language models (LLMs) can automate and improve language tasks, offering versatility and efficiency over traditional methods. This book aims to educate enterprises on the advantages of LLMs and guide them through their development and deployment.',\n",
       " 'The book is structured into three parts: Part 1 defines LLMs and explores advancements in the field while advising enterprises on development. Part 2 highlights five use cases of LLMs in businesses with real-life examples. Part 3 serves as a practical guide for enterprises to build, train, and deploy LLMs, detailing requirements and trade-offs to inspire adoption for competitive advantage.',\n",
       " 'Early adopters and visionary innovators will gain the most benefits from advancements in large language models (LLMs) and related technologies. The glossary includes key terms such as deep learning systems, generative AI, natural language processing (NLP), and various types of neural networks.',\n",
       " 'AI programs can create original content, such as text, images, and audio, instead of merely analyzing existing material.',\n",
       " 'Large language models (LLMs) are AI systems trained on extensive data sets, enabling them to recognize, summarize, translate, predict, and generate text and other content. They have a high number of parameters, often reaching hundreds of billions, allowing for effective understanding and generation of natural language.',\n",
       " 'A specialized type of RNN with advanced cell blocks enhances the retention of past inputs and is utilized in NLP for generating human-like text.',\n",
       " 'NLP involves computer programs understanding human-like text. Neural networks (NNs) use layered machine learning algorithms inspired by the human brain, allowing for representation learning with minimal human input. Prior to 2020, AI programs mainly focused on processing and analyzing data rather than generating it.',\n",
       " 'The text discusses a neural network capable of sequential data processing and memory retention. It defines terms related to machine learning, including rule-based systems, traditional machine learning, transformers, structured and unstructured data, fine-tuning, parameter-efficient techniques (PEFT), prompt learning, and adapter tuning.',\n",
       " 'It emphasizes data over rules.',\n",
       " 'A neural network architecture that processes sequential data in a non-sequential manner.',\n",
       " 'Quantitative data that is easily standardized and adjustable for efficient processing by machine learning algorithms, such as phone numbers.',\n",
       " 'Qualitative data, like customer reviews, is challenging to standardize.',\n",
       " 'Data is stored in its native formats, such as PDF files, prior to use.',\n",
       " 'A transfer learning method enhances model performance on specific downstream tasks or datasets.',\n",
       " 'The text discusses parameter-efficient fine-tuning (PEFT) techniques for pre-trained language models (PLMs), emphasizing methods like prompt learning, LoRa, and adapter tuning. These approaches enhance model performance by modifying select parameters rather than the whole model, allowing for quicker training and more accurate predictions. Additionally, it mentions open-domain question answering, which covers diverse fields such as legal, medical, and financial domains.',\n",
       " 'Extractive question answering involves retrieving answers from existing texts or databases. Key terms include throughput, which measures model efficiency and speed, and latency, which relates to the time taken to process requests. Data readiness is also essential for implementing large language models.',\n",
       " 'The passage discusses the capacity of a model to process or generate data or predictions within a specified time limit.',\n",
       " 'The table/text discusses the processing time required by a model to handle input and produce output.',\n",
       " \"Large language models (LLMs) are advanced AI systems that generate human-like text by learning from extensive datasets using deep learning techniques. Their capabilities have transformed natural language processing (NLP) since the introduction of models like OpenAI's GPT-3 in 2020, which demonstrated proficiency in various tasks such as translation and summarization. LLMs effectively understand context, produce grammatically correct text, and can perform complex NLP tasks with little human oversight.\",\n",
       " 'The advancement of LLMs marked a significant improvement over previous rule-based models, which lacked self-learning capabilities and adaptability. Consequently, many enterprises and startups began developing or adopting LLMs to enhance operations, reduce costs, and improve workflows.',\n",
       " 'Part 1 introduces large language models (LLMs), which are deep learning algorithms capable of processing and generating text by learning from extensive datasets, forming a foundation for enterprises considering their development or adoption.',\n",
       " 'All language models can process and generate text resembling natural language.',\n",
       " 'Large Language Models (LLMs) are a type of language model distinguished by their size, which allows them to perform a variety of natural language processing (NLP) tasks.',\n",
       " 'They are trained with extensive data sets.',\n",
       " 'Please provide the text or table you would like summarized.',\n",
       " 'The text compares two large language models, MT-NLG and GPT-3 Davinci, highlighting their learnable parameters and training data sizes. MT-NLG has 530 billion parameters and 270 billion tokens in its training data, whereas GPT-3 Davinci has 175 billion parameters and 499 billion tokens. Larger models tend to produce more accurate responses due to their size and training data quality.',\n",
       " 'Answer generated by GPT-3.',\n",
       " 'Large language models improve natural language understanding and generation, aiding in question answering, translation, summarization, and creative text generation. Their performance is influenced by factors beyond just model size and data quantity.',\n",
       " 'Data quality is important.',\n",
       " 'LLMs trained on high-quality sources like peer-reviewed research and published novels perform better than those trained on low-quality data such as social media posts, which can introduce issues like slang and incorrect spellings.',\n",
       " 'Models require diverse data to effectively perform various NLP tasks.',\n",
       " 'Foundation language models like MT-NLG and GPT-3 are trained on extensive datasets to handle diverse NLP tasks. Fine-tuning these models with specific, narrower datasets enhances their performance on specific tasks within a specialized domain.',\n",
       " 'Foundation models exhibit strong general performance across various tasks with limited domain-specific data but may not excel in specialized tasks. Fine-tuned language models are derived from these foundation models and are tailored for specific use cases, enhancing their effectiveness in specialized tasks.',\n",
       " 'Fine-tuned models excel in specific tasks compared to foundation models and are generally lighter and easier to train. The main method for fine-tuning involves parameter-efficient techniques like p-tuning, prompt tuning, and adapters.',\n",
       " 'Customization is quicker and cheaper than fine-tuning a whole model, but may result in slightly lower performance.',\n",
       " 'Customization methods for large language models are detailed in Part 3, emphasizing the shift from AI systems focusing on data processing and analysis to generating data.',\n",
       " 'The text discusses the distinction between Perceptive AI, which focuses on understanding the world, and Generative AI, which has gained prominence since 2020 alongside the advent of transformer models and large language models (LLMs). It highlights a paradigm shift in natural language processing (NLP) with the introduction of LLMs, contrasting them with earlier NLP models that relied on human-crafted rules. The pre-transformers NLP was limited to simpler tasks, making it unsuitable for complex tasks like machine translation.',\n",
       " 'Rule-based models struggled in edge-case scenarios as they could not accurately predict or classify unfamiliar data lacking established rules.',\n",
       " 'RNNs and LSTMs, developed during this period, improved context-dependent predictions but struggled with long-span text predictions, limiting their effectiveness.',\n",
       " 'The rise of the transformer architecture in 2017 revolutionized NLP by outclassing RNNs and LSTMs in generalization, context capture, and data processing. This allowed for better understanding of longer data sequences and broader task performance, although early models faced limitations due to insufficient datasets and computational resources.',\n",
       " 'Researchers and experts are interested in these developments, but they have not gained attention from the general public due to issues with user-friendliness and accuracy, preventing commercialization.',\n",
       " 'LLM NLP advancements were significantly propelled by OpenAI\\'s GPT-3 launch in 2020, which, through training on vast datasets, improved the accuracy and comprehensiveness of NLP responses, advancing the pursuit of \"true\" AI.',\n",
       " 'LLMs have made NLP models accessible to non-technical users through natural-language prompts, democratizing the technology. This shift was driven by advancements such as neural networks, attention mechanisms, transformers, and progress in unsupervised and self-supervised learning. Understanding these concepts is essential for grasping how LLMs function and for developing new LLMs. Neural networks are machine learning algorithms inspired by the human brain.',\n",
       " \"Artificial neural networks are composed of neurons organized into layers, including an input layer that receives data, and hidden layers that perform functions such as data identification, classification, and generation, which contribute to the model's output.\",\n",
       " 'The output layer of LLMs produces predictions based on input, originally utilizing simpler neural network architectures like RNNs and LSTMs, which effectively consider context and relationships between distant words in a sequence.',\n",
       " 'Neural networks, particularly RNNs and LSTMs, excel in NLP tasks like sentiment analysis and text classification by memorizing past data for more accurate outputs. Their key advantage is the ability to autonomously learn and create rules from data, unlike traditional rule-based systems that require predefined rules.',\n",
       " 'Representation learning refers to the process by which neural networks identify hidden patterns or features in data, inspired by human learning mechanisms.',\n",
       " 'The model identifies that plural nouns typically have the suffix \"-s\" from the training tokens and applies this representation to new data following a similar distribution.',\n",
       " 'The model is expected to accurately classify tokens such as “chairs” and “table” as plural or singular, even if it has not previously encountered these words.',\n",
       " 'The model adjusts its parameters for irregular nouns, ensuring that plural nouns are matched with plural verbs, which enhances generalization capabilities compared to rule-based systems and improves task performance.',\n",
       " 'The effectiveness of representation extraction is significantly influenced by the number of neurons and layers in a network.',\n",
       " 'Larger neural networks with multiple hidden layers allow for more complex representations, which is why deep learning is preferred in modern large language models.',\n",
       " 'The comparison highlights that deep learning neural networks consist of multiple hidden layers, allowing them to process complex inputs and outputs more effectively than single-layer networks. The development of deep neural networks has become viable due to advancements in hardware capable of managing extensive computational demands.',\n",
       " 'The introduction of the first GPU by NVIDIA around 1999 and the popularization of GPUs in deep learning by the successful CNN AlexNet in 2012 enabled significant advancements in the field.',\n",
       " 'GPUs have significantly advanced deep learning, leading to the development of transformers, a new type of neural network that has transformed NLP. While RNNs and LSTMs have benefits, they struggle with long data sequences and context management, making them less effective for complex tasks like machine translation. Transformers, introduced in 2017 by Vaswani et al., have addressed these limitations.',\n",
       " 'The paper \"Attention is All You Need\" introduces the Transformer architecture, based solely on attention mechanisms, which eliminates the need for recurrence and convolutions in processing sequential data. This approach made transformers superior for machine translation and various NLP tasks by enabling models to focus on significant tokens in the input. Attention weights are calculated for each token, determining their importance in generating output. Self-attention layers assess the relevance of tokens to each other, allowing the model to emphasize or de-emphasize words as needed, thereby improving context handling and overall performance.',\n",
       " 'Please provide the table or text chunk you would like summarized.',\n",
       " 'Tokens with few or irrelevant relationships to other tokens are deemed less important or unimportant for specific tasks.',\n",
       " 'Understood. Please provide the table or text chunk for summarization.',\n",
       " 'Attention mechanisms improve model accuracy and speed in generating outputs compared to RNNs and LSTMs, which process data sequentially and take longer, as they handle inputs one word at a time.',\n",
       " 'Processing a single Wikipedia page with LSTMs and RNNs would take a significant amount of time.',\n",
       " 'Transformers process data in parallel, allowing them to read all input tokens simultaneously, resulting in faster performance on NLP tasks compared to LSTMs and RNNs.',\n",
       " 'Sequential data processing in LSTMs and RNNs ensures word order recognition by processing one word at a time, while transformers process data non-sequentially and lack initial awareness of word order.',\n",
       " 'Mary owned a small lamb.',\n",
       " 'Mary had a little lamb.',\n",
       " 'Mary had a little lamb.',\n",
       " 'Sentence (2) illustrates how minor word order changes can alter meaning, while sentence (3) highlights that more significant changes can lead to nonsensical constructions. Transformers address this by utilizing positional encodings, which are vectors added to tokens to preserve their positional information.',\n",
       " 'Parameters can be either fixed or trainable based on the preference for model refinement during training.',\n",
       " \"The document is a beginner's guide to large language models (LLMs), likely containing illustrations and explanations to help new users understand the concepts and workings of LLMs.\",\n",
       " 'Sequential data processing in LSTM is enhanced by mechanisms introduced in transformer models, notably Google’s BERT, released in 2018, which utilizes attention mechanisms for improved language representation.',\n",
       " 'The model is a masked language model (MLM) trained on sentences with masked tokens.',\n",
       " 'BERT predicts masked tokens by analyzing surrounding words, using context from both sides, making it bidirectional and more accurate than unidirectional models, which only consider left-side context.',\n",
       " 'Bidirectional models like BERT outperform unidirectional models in NLP tasks by improving context and meaning understanding, exemplified in enhancing Google search accuracy.',\n",
       " \"Other companies are developing their own large language models influenced by Google's BERT and advancements in the field.\",\n",
       " 'The table lists models developed before the release of GPT-3 in June 2020.',\n",
       " 'Table 2 outlines the timeline of significant releases of large language models (LLMs).',\n",
       " 'The table outlines the release timeline of two large language models: GPT-2 by OpenAI in 2019, which generates human-like text for various NLP tasks, and RoBERTa by Facebook in 2019, which enhances NLP task performance through extensive training on larger datasets.',\n",
       " \"DeBERTa, developed by Microsoft in 2020, enhances NLP tasks by separating the encoder and decoder components of the model. GPT-3, created by OpenAI in 2020, is an advanced version of GPT-2, leveraging a larger dataset for superior output quality and demonstrating the potential of large language models (LLMs) in few-shot learning. This success has led to the development of even larger models such as Megatron-Turing with 530 billion parameters, PaLM with 540 billion, and WuDao 2.0 with 1.75 trillion parameters. Additionally, BERT's impact stemmed from its bidirectional training and use of unsupervised learning techniques.\",\n",
       " 'Unsupervised learning involves machine learning algorithms detecting patterns in unlabeled datasets without human intervention.',\n",
       " 'BERT independently extracted patterns from plain-language Wikipedia pages during training, exemplifying pure AI capabilities.',\n",
       " 'Unsupervised learning models utilize feedback loops for performance improvement by receiving feedback on predictions. In contrast, self-supervised learning models do not use feedback loops but instead rely on automatically generated supervisory signals from data for training.',\n",
       " 'Unsupervised and self-supervised learning techniques autonomously generate labels and extract features, reducing the need for human intervention, in contrast to supervised learning.',\n",
       " 'Companies can train models efficiently without the need for extensive data labeling or human feedback on outputs.',\n",
       " 'Self-supervised learning is the primary method for pre-training large language models, often suggested for enterprises. GPT and BERT are both advanced NLP models but have different architectures and applications.',\n",
       " 'GPT is a generative model focused on predicting the next word in a sentence, suitable for language generation tasks like text completion and summarization, using a unidirectional transformer. In contrast, BERT is a discriminative model designed for classifying sentences or tokens, using a bidirectional transformer to consider both left and right context, making it effective for tasks such as sentiment analysis and named entity recognition. Both models are influential in the field of NLP.',\n",
       " 'Researchers and practitioners often combine different models to achieve optimal results for specific tasks.',\n",
       " 'Enterprises can leverage large language models to automate various language-related tasks, including content generation, data analysis, and customer support. These models enhance efficiency and accuracy, enabling faster code writing in tech firms and reducing errors in fraud detection for banks.',\n",
       " 'LLMs can automate tedious tasks, enabling employees to focus on more critical activities and enhancing productivity. In healthcare, LLMs can generate synthetic clinical data to expedite medical research. Other applications include sentiment analysis for audience insights and customer churn prediction to retain clients.',\n",
       " 'Enterprises can leverage large language models (LLMs) to create new, conversation-based services, including specialized AI companions. However, they should be cautious of common challenges associated with LLMs, which apply to customization, fine-tuning, or building models from scratch.',\n",
       " 'Please provide the table or text chunk for summarization.',\n",
       " 'Large language models are susceptible to adversarial examples, which are specially crafted inputs that can mislead them, posing security risks for sensitive sectors such as healthcare and finance.',\n",
       " 'Please provide the text or table chunk you would like summarized.',\n",
       " 'Large language models often lack interpretability, making it challenging to understand their decision-making processes and assess their accuracy and bias. This issue is particularly significant in high-stakes applications like fraud detection and in industries such as healthcare and finance that demand transparency.',\n",
       " 'The provided content seems to be incomplete; please provide the relevant table or text for summary.',\n",
       " 'Large language models can produce non-specific and generic responses.',\n",
       " 'LLMs may struggle with understanding human input, but techniques like Reinforcement Learning from Human Feedback (RLHF) can enhance their performance. However, they may still reproduce training data, leading to ethical concerns and potential copyright issues for businesses.',\n",
       " \"Please provide the table or text chunk you'd like summarized.\",\n",
       " 'Using large language models (LLMs) for important decision-making raises ethical concerns, particularly regarding their role in candidate selection and the potential displacement of human workers in white-collar jobs.',\n",
       " 'Large language models (LLMs) can generate inappropriate and harmful content due to their training on extensive internet text, which may include toxic or biased information. Enterprises looking to build proprietary LLMs face challenges related to computing power, storage, expertise, and funding. Instead of building from scratch, many businesses, especially those not focused on AI, prefer to customize existing pre-trained models (PLMs), which involves selecting an appropriate foundation model as the first step.',\n",
       " 'Key factors to consider include ideal model size, training tasks and datasets, and LLM providers.',\n",
       " 'It seems there was an error in your request, as no specific table or text was provided for summarization. Please provide the text or table you would like summarized.',\n",
       " 'Base models can be fine-tuned on specific corpora for tailored applications, such as adapting text classification models for sentiment analysis or training with legal records to enhance legal terminology proficiency.',\n",
       " 'Models can be optimized using Reinforcement Learning from Human Feedback (RLHF), which updates them based on human feedback on predictions. RLHF is effective, as demonstrated by its use in ChatGPT. Enterprises may also customize base models using parameter-efficient techniques like adapters and p-tuning.',\n",
       " 'Customization of models enhances accuracy when the base model is trained on similar tasks, like using a text classification model for sentiment analysis. Large language models (LLMs) excel in various language tasks but require careful evaluation based on training data quality, model size, and inference speed. For effective learning, training data must be diverse, representative, and annotated for supervised learning.',\n",
       " 'Larger models typically yield better performance but demand more computational resources, prompting researchers to balance model size and performance based on task requirements and available resources.',\n",
       " 'Larger models are more susceptible to overfitting, negatively impacting generalization on new data, and inference speed is a key consideration for real-world deployment of LLMs.',\n",
       " 'Faster inference time allows LLMs to efficiently process large data volumes.',\n",
       " \"Techniques like pruning, quantization, and distillation are used to enhance LLM efficiency. LLM performance is assessed via benchmarks like GLUE, SuperGLUE, and CoQA. Significant companies in LLM development include OpenAI, Google, and Meta, with notable models like GPT-3 and PaLM. LLM apps, highlighted by OpenAI's ChatGPT, rapidly gained popularity, with other startups like Jasper and Copy.ai also achieving substantial user bases.\",\n",
       " 'Figure 4 illustrates a natural-language prompt that users can input into Copy.ai to create a blog post outline.',\n",
       " \"The content relates to a beginner's guide on large language models, featuring a prompt example and a blog outline section.\",\n",
       " \"The blog discusses the applications of OpenAI's GPT-3 in various writing tools, comparing it to GPT-3 powered apps like Copy.ai, Lex, Jasper, and Frase.io. It highlights the differences between OpenAI's GPT-3 and third-party applications, forecasts the future of these writing tools, and mentions successful LLM-powered apps like Grammarly and GitHub Copilot. The upcoming part will explore additional uses of LLMs in enterprises for specialized applications, providing examples of customization for industries such as finance, healthcare, and telecommunications to encourage innovative uses of LLMs.\",\n",
       " 'The document is an informational guide about large language models, provided by NVIDIA, which disclaims any warranties regarding product functionality, condition, or information accuracy.',\n",
       " \"NVIDIA disclaims liability for any consequences resulting from the use of its information and does not commit to developing or delivering any material. The company reserves the right to modify the document without notice, and customers should verify the latest information before placing orders. Products are sold under NVIDIA's standard terms and conditions, and any customer terms are explicitly rejected.\",\n",
       " 'This document disclaims any contractual obligations related to NVIDIA products, which are not authorized for use in critical applications that may lead to personal injury or property damage. Customers assume all risk associated with the use of these products and are responsible for testing and ensuring suitability for their intended applications. No warranties or representations are made regarding the products, and NVIDIA disclaims liability for any damages resulting from their use. Reproduction of the document’s information requires prior written approval from NVIDIA. All materials are provided \"as is\" without warranty. NVIDIA\\'s liability is limited as per the Terms of Sale for the products.',\n",
       " 'The text indicates that company and product names may be trademarks of their respective companies, and it is copyrighted by NVIDIA Corporation in 2023.',\n",
       " 'All rights to the content are reserved.',\n",
       " 'NVIDIA Corporation discusses large language models (LLMs) and their foundations in transformer architecture. LLMs are self-supervised, trained to predict the next word in a sequence and require vast amounts of text and computational resources. The evolution of language modeling is outlined, starting from static word embeddings in 1990 to transformers in 2017 and contextual embeddings by 2018. Static embeddings like word2vec have limitations, which LLMs aim to overcome by using transformers that utilize attention mechanisms.',\n",
       " 'Static word embeddings do not capture the variability of word meanings in different contexts, as illustrated by the example of \"it.\" Contextual embeddings are needed to represent meanings accurately.',\n",
       " 'Contextual embeddings assign different vector representations to words based on their context, with the attention mechanism allowing for selective integration of information from neighboring words to compute these embeddings. This approach helps determine what a pronoun refers to in a sentence by weighing information from its context.',\n",
       " 'Attention is a technique that enhances token representations by considering prior words based on their relevance to the current token. This is achieved through weighted sums of embeddings, influenced by similarity scores. Each embedding has a query and key, allowing for contextual adjustments, resulting in different meanings for words depending on their context.',\n",
       " 'Transformers and Large Language Models utilize a mechanism based on attention to compute embeddings, with multiple transformer blocks processing input tokens. Language models predict the next word through a pretraining phase on vast text datasets, followed by application to specific tasks. Model components include composite embeddings, a language modeling head for output probabilities over a vocabulary, and a sequence of feedforward and normalization layers.',\n",
       " 'No table or text provided for summarization.',\n",
       " 'A corpus of text is a structured collection of written or spoken material used for linguistic analysis and research.',\n",
       " 'The text references a process that occurs at each time step t, but no additional details are provided.',\n",
       " 'The text discusses the training of a language model using gradient descent to minimize prediction error with a focus on cross-entropy loss. The model should assign high probabilities to true next words, and the loss increases when probabilities are too low. It outlines the training process for transformer models using pretraining data from various sources like the web, highlighting that the model learns general linguistic patterns, such as recognizing the presence of canines.',\n",
       " 'The text discusses large language models (LLMs) and their training, emphasizing the importance of pretraining on extensive text for knowledge acquisition. It outlines various architectures for LLMs, including decoders (e.g., GPT, Claude), encoders (e.g., BERT), and encoder-decoders (e.g., Flan-T5). The concept of conditional generation, where text is generated based on previous text, is highlighted.',\n",
       " 'Completion Text is a category related to input/output processing, often used in natural language tasks such as text generation, summarization, and dialogue systems. It involves generating coherent and contextually relevant content based on given prompts.',\n",
       " 'The text describes the use of conditional generation for tasks like sentiment analysis, exemplified by the string \"I like Jackie Chan\" and setting up a prompt for the language model to identify the sentiment.',\n",
       " 'The text discusses using language models for tasks like sentiment analysis and question answering, illustrating this with examples of conditional generation and a specific query about the author of \"The Origin of Species.\"',\n",
       " 'Please provide the table or text chunk for summarization.',\n",
       " 'The table suggests predicting the next word after the question \"Who wrote the book \\'The Origin of Species\\'?\" based on the context provided.',\n",
       " 'Various types of encoders are available.',\n",
       " 'Masked Language Models (MLMs), particularly the BERT family, predict words based on surrounding context and are often fine-tuned for classification tasks. Encoder-Decoder models are used for tasks like machine translation and speech recognition. Additional techniques include instruction fine-tuning, preference alignment, and prompt engineering.',\n",
       " 'CS224N is a course focused on Natural Language Processing with deep learning methods, covering topics such as word embeddings, neural networks, sequence models, and attention mechanisms. It emphasizes practical applications and includes hands-on projects.',\n",
       " 'Large Language Models (LLMs) can be applied to new tasks using pretrained models but pose risks such as hallucination, where AI generates false or misleading information. This raises concerns about the reliability of chatbots and their potential to represent information incorrectly.',\n",
       " 'Air Canada lost a court case after its chatbot generated false information about policies, with the airline claiming the chatbot should be held liable.',\n",
       " 'Current research on addressing hallucination involves Retrieval-Augmented Generation (RAG), where information is retrieved from high-quality sources to aid language model responses. Additionally, legal actions are underway against OpenAI for alleged mass copyright infringement related to the use of copyrighted novels and articles, including a lawsuit from The Times regarding AI training on millions of its articles.',\n",
       " \"The text discusses concerns surrounding AI technologies, focusing on the impact of their misuse, such as the trauma experienced by contractors screening harmful content for OpenAI's chatbot and the proliferation of misinformation in U.S. elections from chatbots. Additionally, it highlights the growing interest in the ethics of large language models (LLMs), evidenced by increasing submissions to the FAccT conference from various sectors from 2018 to 2022. Lastly, it outlines educational goals related to AI, including skills in regular expressions, classification, search engines, and chatbot development.\",\n",
       " 'CS224N: Natural Language Processing with Deep Learning focuses on algorithmic internals such as transformers, GPT, parsing, and machine translation, covering detailed technical aspects of these applications.',\n",
       " 'CS224C focuses on applying machine learning and social science theories to analyze human behavior and societal issues, including topics like hate speech and misinformation. CS224S is centered on spoken language technology, particularly dialogue systems. CS336 covers the comprehensive process of language model creation, from data gathering to model training and evaluation, with an application requirement.',\n",
       " 'CS 246 focuses on mining massive data sets. Upcoming courses include CS 224V, which covers topics related to conversational virtual assistants and LLMs, such as knowledge growth, hallucination prevention, evaluation, task control, persuasion, multilingual support, and interface integration. CS 329X emphasizes human-centered design in NLP, focusing on human-in-the-loop algorithms, fairness, and accessibility.',\n",
       " 'CS329R focuses on combining natural language processing techniques with social psychology regarding race to create systems that tackle important societal challenges. Additionally, it offers engaging courses in linguistics and communication, both this spring and in the following year.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4676362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ee822ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Summarize tables\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "93361381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries # no data in table, so no summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866652c2",
   "metadata": {},
   "source": [
    "### Image summaries\n",
    "\n",
    "We will use gpt-4o-mini to produce the image summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40dfea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of a research paper explaining the transformers\n",
    "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "\n",
    "\n",
    "#image_summaries = chain.batch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7b16235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Example function to process images\n",
    "def process_images_with_delay(images, chain, delay=1):\n",
    "    results = []\n",
    "    for image in images:\n",
    "        try:\n",
    "            # Process the image using the chain\n",
    "            result = chain.invoke(image)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "        # Introduce delay to handle rate limits\n",
    "        time.sleep(delay)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f161ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries = process_images_with_delay(images,chain, delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ecbbe24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "efc11ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2bbd65de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm unable to provide a description of the image as requested. However, if you need assistance with the concepts related to the transformer architecture or specific aspects of your research paper, feel free to ask!\",\n",
       " \"I can't determine the content of the image you provided, but if it's part of a research paper explaining transformers architecture, it likely includes visuals like diagrams, equations, and possibly bar plots demonstrating various aspects of the architecture's performance or comparison to other models.\\n\\nYou might expect to see:\\n\\n1. **Diagrams**: Illustrations of the transformer model itself, detailing components like the encoder and decoder, attention mechanisms, and layer structures.\\n\\n2. **Bar Plots**: Graphs comparing performance metrics, such as accuracy or training time across different transformer architectures or variations, highlighting key differences.\\n\\n3. **Legend or Labels**: Each graph would likely have clearly labeled axes indicating the metrics being compared, with a legend explaining what each bar represents.\\n\\n4. **Color Coding**: Different colors for bars may signify different models or datasets used in the experiments.\\n\\nThese elements collectively contribute to the understanding of the transformer architecture’s efficacy and design choices.\",\n",
       " 'The image presents a visually engaging representation of concepts related to \"Agentic AI.\" \\n\\n### Left Side:\\n- **Background**: A dark green chalkboard-like backdrop.\\n- **Illustration**: A silhouette of a human head, facing right, is depicted in white outline. This head is integrated with several directional arrows and lines that symbolize thought processes or information flow. \\n  - **Question Mark**: A yellow question mark is prominently displayed within the head, indicating inquiry or decision-making.\\n  - **Arrows**: Lines emanate from the head and point in various directions—some arrows are upward, while others curve and move sideways, denoting complexity in decision-making or the intertwining of various thought pathways.\\n\\n### Right Side:\\n- **Text Section**: This space is divided into two segments:\\n  - **Top Section**: The title \"AGENTIC AI\" is displayed in bold, white capital letters, creating a strong focal point.\\n  - **Bottom Section**: A subtitle reads, \"A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence,\" in a more detailed font but still prominent, set against a darker purple background. This further elaborates on the theme of the research paper.\\n\\n### Overall Impression:\\nThe design combines a playful yet serious approach, integrating a conceptual framework with a visual cue of human cognition, aimed at illustrating complex ideas in AI decision-making in an accessible manner. The combination of graphics and text functions effectively to draw attention and communicate the essence of the research.',\n",
       " 'The image presents a bar plot comparing the key performance metrics of two types of AI systems: Traditional AI and Agentic AI. \\n\\n### Key Features of the Bar Plot:\\n\\n1. **Title**: The title at the top states \"Key Performance Metrics of Agentic AI Systems,\" indicating the focus of the data presented.\\n\\n2. **Axes**:\\n   - The **Y-axis** represents the performance metrics measured in percentages, ranging from 0 to 100.\\n   - The **X-axis** categorizes four key performance metrics: \\n     - Decision Accuracy\\n     - Response Time\\n     - Adaptation Rate\\n     - System Reliability\\n\\n3. **Bars**:\\n   - **Color Coding**: \\n     - The bars for Traditional AI are colored **blue**.\\n     - The bars for Agentic AI are colored **orange**.\\n   - Each metric has two bars side by side:\\n     - The blue bars consistently show lower values for Traditional AI metrics.\\n     - The orange bars typically exceed the blue ones for all metrics.\\n   \\n4. **Data Values**:\\n   - **Decision Accuracy**: \\n     - Traditional AI: 82.5%\\n     - Agentic AI: 94.8%\\n   - **Response Time**:\\n     - Traditional AI: 75.3%\\n     - Agentic AI: 92.1%\\n   - **Adaptation Rate**:\\n     - Traditional AI: 68.7%\\n     - Agentic AI: 88.9%\\n   - **System Reliability**:\\n     - Traditional AI: 85.2%\\n     - Agentic AI: 93.4%\\n\\n5. **Overall Comparison**: The plot clearly shows that Agentic AI outperforms Traditional AI across all performance metrics, with particularly significant gains seen in Decision Accuracy and Response Time.\\n\\nThis detailed visual representation effectively conveys the advantages of Agentic AI systems compared to their traditional counterparts.',\n",
       " 'The image is a bar graph illustrating various metrics of improvement, expressed as percentages. The title at the top reads \"Improvement (%)\" in a large, bold font. \\n\\nThere are five bars, each representing a specific metric:\\n\\n1. **Operational Efficiency** - This bar is the tallest, reaching 65.8%. It is depicted in a solid blue color and is positioned farthest to the left.\\n   \\n2. **Cost Reduction** - The second bar, slightly shorter, reaches 42.3%. It is positioned next to the Operational Efficiency bar, also in blue.\\n\\n3. **Customer Satisfaction** - This bar stands at 58.7%, taller than the Cost Reduction bar but shorter than Operational Efficiency and is placed next in line.\\n\\n4. **Innovation Rate** - The fourth bar is the second tallest, measuring 71.2%. It is prominently displayed following the Customer Satisfaction bar.\\n\\n5. **Time-to-Market** - The shortest bar, reaching 48.9%, is positioned on the far right and is also colored blue.\\n\\nEach bar is labeled at the bottom with the respective metric name, and the percentage values are displayed prominently at the top of each bar. The overall design presents a clear comparison of these five performance indicators, allowing for fast visual assessment of their relative improvements. The background is a light gradient, which highlights the bars effectively.',\n",
       " 'The image depicts a Creative Commons logo, which includes an encircled \"CC\" symbol, indicating that the content is licensed under Creative Commons. The logo features three distinct symbols:\\n\\n1. **BY (Attribution)**: This is represented by an icon of a person, indicating that users must give appropriate credit to the creator of the work.\\n  \\n2. **NC (Non-Commercial)**: Shown as a dollar sign with a slash through it, this symbol indicates that the work cannot be used for commercial purposes.\\n\\nThe logo is presented in a monochromatic scheme with a gray background and black symbols. Each component is clearly defined, ensuring readability and clarity. This logo is often found in research papers to inform readers of the licensing terms related to the use of the paper\\'s content.',\n",
       " \"The image is a detailed diagram illustrating the architecture of a transformer model, commonly used in natural language processing. Here's a breakdown of its components:\\n\\n1. **Overall Structure**:\\n   - The diagram features two main sections representing the encoder and decoder parts of the transformer architecture, interconnected with arrows to indicate the flow of data.\\n   - The overall design is modular, showing cyclical patterns that repeat for multiple layers denoted as \\\\( N \\\\times \\\\), indicating the number of layers in both the encoder and decoder.\\n\\n2. **Encoder Side**:\\n   - Begins with **Input Embedding** at the bottom, which receives the model's input data.\\n   - The **Positional Encoding** is added to the input embeddings to provide information about the position of words in the sequence.\\n   - The first block consists of **Multi-Head Attention** followed by an **Add & Norm** layer, which normalizes the output.\\n   - This is followed by a **Feed Forward** neural network, again leading into another **Add & Norm** layer.\\n   - The entire block structure repeats \\\\( N \\\\) times on the encoder side.\\n\\n3. **Decoder Side**:\\n   - Similar in design to the encoder but starts with **Positional Encoding** and **Input Embedding**, shifting prior values for the outputs.\\n   - Features a **Masked Multi-Head Attention** mechanism to prevent the model from looking at future tokens during training, alongside an **Add & Norm** layer.\\n   - The subsequent layer mirrors the encoder with another **Multi-Head Attention** block that incorporates outputs from the encoder.\\n   - Each flow culminates in another **Add & Norm** followed by a **Feed Forward** structure.\\n\\n4. **Output Layer**:\\n   - At the top, the flow culminates in a **Linear** layer fed by prior layers' outputs, followed by a **Softmax** function to convert logits into probabilities for the output predictions.\\n\\n5. **Connections**:\\n   - Arrows throughout the diagram indicate input and output relationships between different components, demonstrating the model's flow from inputs to encoded representations and finally to predictions.\\n\\nThe diagram clearly illustrates the flow of information and the modular approach to building both the encoder and decoder in the transformer architecture.\",\n",
       " 'The image presents two structured components of the Transformer architecture: \"Scaled Dot-Product Attention\" and \"Multi-Head Attention.\" \\n\\n### Scaled Dot-Product Attention (Left Side)\\n- **Components**: This section contains a sequence of boxes representing mathematical operations.\\n  - **Input Vectors**: At the bottom, there are three labeled vectors: `Q` (Query), `K` (Key), and `V` (Value).\\n  - **MatMul**: The first operation (labeled `MatMul`) applies matrix multiplication to `Q` and `K`, producing an intermediate result.\\n  - **Scale**: Above that, a box labeled `Scale` indicates a scaling operation, likely to adjust the magnitude of the result.\\n  - **Mask (opt.)**: The next box is labeled `Mask (opt.)`, suggesting an optional masking operation which alters the attention weights to control which parts of the sequences are attended to.\\n  - **SoftMax**: The SoftMax function follows, transforming the masked scores into probabilities.\\n  - **Final MatMul**: The last operation is another `MatMul`, combining the attention scores with the `V` vector to produce the final context vector.\\n\\n### Multi-Head Attention (Right Side)\\n- **Architecture Overview**: This section details the Multi-Head Attention mechanism.\\n  - **Linear Layers**: Three `Linear` boxes at the bottom indicate linear transformations applied to the `V`, `K`, and `Q` vectors, respectively.\\n  - **Concat**: Above, a `Concat` box signifies that the outputs of multiple attention heads will be concatenated.\\n  - **Scaled Dot-Product Attention**: Central to this section is another `Scaled Dot-Product Attention` box (highlighted in purple) that emphasizes the main computation for each head.\\n  - **h (Heads)**: Arrows indicate that this attention process is replicated for multiple heads, each processing their separate linear transformations of `V`, `K`, and `Q`.\\n\\nThis diagram serves as a concise visual representation of the sequential operations involved in both attention mechanisms, illustrating how data flows through the architecture to produce meaningful outputs in the context of natural language processing tasks.',\n",
       " 'The image appears to feature a stylized graphic with an eye shape at its center. Within the eye, there are sequences of binary code (0s and 1s) arranged in three lines, creating a visual representation of digital information. The background is a dark color, possibly black or deep blue, which contrasts with the lighter shades of the eye and the binary numbers, giving it a modern and sleek appearance. \\n\\nThis design could symbolize themes related to vision or perception in the context of artificial intelligence and machine learning, particularly in relation to how transformers process data. The use of binary code may imply a focus on data representation and computing, reinforcing the theme of technology in the discussion of transformers. \\n\\nThe simplicity and abstract nature of the image suggest it may serve as a conceptual illustration rather than a detailed data visualization like graphs or bar plots.',\n",
       " \"I can't interpret the image you provided, but I can help describe the typical components you might find in a research paper explaining the transformers architecture.\\n\\nIn such papers, you would often see graphs or diagrams illustrating concepts like:\\n\\n1. **Transformers Architecture**:\\n   - Diagrams that show the layers of the Transformer, including the encoder and decoder stacks.\\n   - Connections between the layers, highlighting self-attention mechanisms.\\n\\n2. **Attention Mechanism**:\\n   - Visuals depicting the attention scores, often represented as heatmaps.\\n   - Bar plots comparing the effectiveness of self-attention versus other mechanisms.\\n\\n3. **Performance Metrics**:\\n   - Bar plots illustrating the performance of various models on benchmark datasets, with axes labelled with model types and performance scores (e.g., accuracy, F1 scores).\\n\\n4. **Training Curves**:\\n   - Line graphs showing loss and accuracy over epochs during training, indicating model convergence.\\n\\nIf you need specific information or details regarding a graph or component of the architecture, feel free to let me know!\",\n",
       " 'The image appears to display the Creative Commons licensing symbol. It features a circular logo on the left, representing the Creative Commons attribution (CC BY) license. The graphic consists of a light gray background with a black bar at the bottom. Within this black area, a small figure is depicted, symbolizing attribution.\\n\\nThere are no graphs, bar plots, or specific data visualizations related to the transformers architecture in this image. Instead, it focuses solely on the licensing information related to the content of the research paper.',\n",
       " 'The image presents a comparison of \"Real\" and \"Synthetic\" visuals, divided into four sections: two on the top representing real images and two on the bottom for synthetic images.\\n\\n**Top Row (Real):**\\n- **Left Image:** A corner of a room features a light green wall with a wooden counter displaying a vase of flowers. The arrangement consists of various colored blooms, likely in a clear glass or a simple ceramic vase, placed on a round table covered with a cloth.\\n  \\n- **Middle Image:** This depicts two giraffes in a natural setting. They are standing amidst a dense landscape of trees and dry grass, embodying a typical wildlife scene. The giraffes are partially obscured by foliage, emphasizing their natural habitat.\\n\\n- **Right Image:** Here, a snowy outdoor scene shows a group of people engaging in winter sports. Some individuals are skiing, while others, possibly children, are learning or playing in the snow. Trees are visible in the background, enhancing the winter landscape.\\n\\n**Bottom Row (Synthetic):**\\n- **Left Image:** Featuring a glass vase holding a bouquet of artificial flowers, this image closely mimics the look of the real flower arrangement above, but the textures and colors may appear overly perfect or vibrant.\\n\\n- **Middle Image:** Two synthetic giraffes, designed to resemble the real ones from the top row, are portrayed standing in a lush, green environment with stylized trees. The colors might appear more vivid or cartoon-like.\\n\\n- **Right Image:** A synthetic version of a winter scene shows human figures dressed in colorful winter attire walking across a striped surface, possibly suggesting a stylized or artificial landscape. While it conveys a similar theme to the top-right image, the portrayal may exhibit less realism and more artistic flair.\\n\\nThis layout illustrates contrasts between authentic and synthetic images, likely to discuss the capabilities of transformer models in generating realistic imagery.',\n",
       " 'The image consists of a 3x3 grid containing photographs, each framed in red boxes to highlight specific elements. Here’s a detailed description of each section:\\n\\n1. **Top Left (Food Scene)**: This image shows a close-up of a table setting featuring a pizza topped with various ingredients, including what appears to be red sauce, cheese, and possibly vegetables. Accompanying the pizza are two champagne flutes filled with bubbly liquid, suggesting a celebratory or social occasion.\\n\\n2. **Top Center (Transportation)**: The second image captures a tram on a street, characterized by its distinct red and yellow color scheme. The scene includes a city backdrop with buildings lining the street, giving a sense of urban life and public transportation.\\n\\n3. **Top Right (Wildlife)**: This image features a zebra standing in a natural setting. The zebra’s striking black and white stripes are prominent, and it is positioned in a sandy environment, likely a savannah or similar habitat, indicating a wildlife scene.\\n\\n4. **Middle Left (Interior Scene)**: The fourth image shows a glimpse of an indoor space, possibly an office or meeting room. There are colored items on a wall that may be various charts or images, and a wooden table with chairs is visible.\\n\\n5. **Middle Center (Abstract)**: The fifth image is less clear, with an abstract pattern or noise resembling digital interference or static. The colors and shapes do not provide identifiable content and may indicate an experimental or illustrative purpose.\\n\\n6. **Middle Right (Data Visualization)**: The final image resembles a heatmap or some form of data visualization. The star-like pattern is colored in gradients from blue to yellow, suggesting it might represent numerical data or frequency distribution.\\n\\nThis composition illustrates a range of subjects from culinary delights to urban transport, wildlife, indoor settings, abstract concepts, and data representations, effectively demonstrating the variability in visual data or potential applications within the context of transformers architecture in machine learning.',\n",
       " 'The image illustrates a flowchart depicting the process of transfer learning using a Convolutional Neural Network (CNN) model. \\n\\n1. **Left Side**:\\n   - There are two sections at the top labeled \"ImageNet\" and \"RSI,\" each showing a collage of images. These represent the datasets used, with ImageNet being a widely used benchmark dataset containing various classes of images.\\n\\n2. **CNN Model**:\\n   - The upper and lower parts of the diagram feature a similar structure representing the CNN model. Each section consists of multiple vertical bars or blocks, which symbolize the layers of the CNN. These layers are colored in alternating segments (e.g., red, yellow, blue), indicating different operations or feature extraction processes taking place within the model.\\n\\n3. **Transfer Learning Arrow**:\\n   - A downward arrow labeled \"Transfer learning\" connects the upper CNN model to the lower section, indicating the process of adapting the pretrained model for a different task.\\n\\n4. **Output Classes**:\\n   - At the upper end of the flowchart, there’s a note that states \"1000 classes (Head removed).\" This suggests that the original model, trained on ImageNet, categorizes images into 1000 classes, and the classification head has been removed as part of the transfer learning process.\\n\\n5. **New Head**:\\n   - The lower section ends with an output indicating \"2 classes (New head).\" This implies that after fine-tuning the CNN on the RSI dataset, it is now configured to classify into just two specific categories: \"Real\" and \"Synthetic.\"\\n\\nOverall, the image effectively conveys the workflow of adapting a CNN model from one dataset to another through transfer learning, highlighting key components and outputs.',\n",
       " 'The image appears to be a collage featuring various examples of research papers related to different synthesis techniques in computer vision. The layout is organized into three main columns, each highlighting specific categories of synthesis methods.\\n\\n1. **Left Column (Image-to-Image Synthesis)**:\\n   - The first section lists several models along with their references, such as \"S2I-DetectorRS\" and \"SPADE.\"\\n   - Each row contains images representing the outputs of these models. For example, the first row includes a serene scene with elephants and an aircraft, while the second row shows surfers on a beach alongside wildlife.\\n\\n2. **Middle Column (Sketch-to-Image Synthesis)**:\\n   - Similar to the left column, this section lists models like \"S2I-HTC\" and \"S2I-MaskRCNN.\"\\n   - It showcases diverse images, including a baseball field with players and snowy landscapes, demonstrating the capability of converting sketches into detailed images.\\n\\n3. **Right Column (Text-to-Image Synthesis)**:\\n   - This column focuses on models such as \"AttnGAN\" and \"ControlGAN,\" providing references for each.\\n   - The images here revolve around various themes, like food presentation and natural scenes, underscoring how textual descriptions can be transformed into visual representations.\\n\\nThe overall design of the image is structured and educational, making clear distinctions between the different synthesis approaches and showcasing relevant examples from research.',\n",
       " \"The image you provided seems to consist of a grid of colorful photographs, each depicting various scenes from nature and activities. Here’s a detailed breakdown of its contents:\\n\\n### Layout:\\n- The image is divided into a grid format, with a total of four rows and three columns (12 images in total).\\n\\n### Row Descriptions:\\n1. **Row 1:**\\n   - **Left:** A person snowboarding on a snowy slope, with a backdrop of pine trees and mountains.\\n   - **Middle:** A group of sheep grazing in a green pasture, with hills in the background.\\n   - **Right:** A herd of zebras standing on a sandy terrain, perhaps in a national park setting.\\n\\n2. **Row 2:**\\n   - **Left:** Two people standing on a beach, enjoying the sun with a bright blue sky above.\\n   - **Middle:** A group of elephants roaming on a grassy field, showcasing their gentle majesty.\\n   - **Right:** A close-up of a black sea bird on the sand near the ocean, possibly a seagull or related species.\\n\\n3. **Row 3:**\\n   - **Left:** Several cows grazing on the beach or near the water, a unique juxtaposition of livestock and ocean.\\n   - **Middle:** A scenic view of a couple walking in a green landscape, surrounded by trees.\\n   - **Right:** People participating in a surfing activity—some are on boards in the water, while others are preparing on the shore.\\n\\n4. **Row 4:**\\n   - **Left:** A collection of sheep in a green field, emphasizing pastoral life.\\n   - **Middle:** A couple of bowls of soup, garnished with broccoli, neatly placed on a wooden table.\\n   - **Right:** Another image of a bowl of soup with vegetables, indicating a focus on healthy meals.\\n\\n### Visual Elements:\\n- The images vary in color and subject, ranging from outdoor sports, animals, nature scenes, to food preparation.\\n- The layout presents a contrast between lively activities and serene nature, creating a pleasing visual narrative.\\n\\n### Contextual Relevance:\\nThis visual grid likely serves to illustrate different aspects of the transformers' architecture, potentially as part of a broader discussion on how the model can interpret diverse inputs or outputs—much like the variety seen in the images.\",\n",
       " \"I'm unable to provide a detailed description of the specific content or context of the image since I can't view or analyze it directly. If you have specific questions about the transformers architecture or any related concepts, feel free to ask!\",\n",
       " \"I'm unable to describe specific images. However, if you're looking for a detailed explanation of the transformers architecture or related graphs, feel free to ask!\",\n",
       " \"I'm unable to see the specific contents of images, including graphs or plots. However, I can help explain the transformer architecture or discuss relevant concepts related to it if you'd like!\",\n",
       " \"I'm unable to view or describe the specific content of images directly as you requested. However, if you provide information about the content or describe the graphs and elements in the image, I can help summarize or elaborate on those topics. If the image is part of a research paper on transformers architecture, it may typically include diagrams illustrating the components of the architecture, such as attention mechanisms, encoder-decoder structures, or performance metrics in the form of graphs like bar plots. Please provide more details, and I'll do my best to assist you!\",\n",
       " \"The image appears to be a heat map showcasing the attention mechanism used in transformer architectures, particularly in relation to image processing or visualization. \\n\\nIn this heatmap:\\n\\n- The background features a gradient of colors from deep blue to vibrant green, suggesting varying intensity or significance across the image.\\n- The central subject seems to be a silhouette of a person riding a horse, which is likely the focal point of interest in the analysis.\\n- Surrounding this figure, the colors transition from hot oranges and reds near the subject to cooler hues like blue and green further away. This indicates that the model is assigning more attention or importance to the area around the rider and horse.\\n- The heatmap's color scale is indicative of the attention scores, where warmer colors (reds and oranges) suggest high attention, while cooler colors (blues and greens) indicate lower attention.\\n\\nThis visual representation could be intended to illustrate how different parts of an image receive varying levels of focus from the transformer model, especially in tasks such as image segmentation or object recognition.\",\n",
       " \"I'm unable to see or describe the specific image you provided. However, if you share details about the graphs or elements in the image, I can help explain or discuss them in the context of transformer architecture or other relevant topics.\",\n",
       " \"I'm unable to provide a description of the specific image you mentioned. However, if you need help with general concepts related to transformer architecture, such as how it works, its components like attention mechanisms, or visualizations commonly found in research papers, feel free to ask!\",\n",
       " \"I'm unable to determine the details of the image you described. If you'd like assistance related to the transformers architecture or explanations about graphs and plots, feel free to provide more information!\",\n",
       " \"I'm unable to view or describe images. If you have other specifics about the image or need information on transformers architecture, I can certainly help with that!\",\n",
       " 'The image consists of a series of visuals arranged in four columns and multiple rows, detailing a research study related to the Transformer architecture. \\n\\n1. **Layout**: \\n   - The image is divided into two main sections labeled (a) and (b). Each section contains a matrix of images that appear to focus on object segmentation tasks in various settings. \\n\\n2. **Column Descriptions**:\\n   - **First Column**: Features a picture of a person riding a horse in a natural outdoor setting. The corresponding segmentation map highlights specific parts of the image, particularly the horse and rider, using a vibrant color palette against a predominantly black background.\\n   \\n   - **Second Column**: Displays an image of a computer monitor showcasing what appears to be an interface or software. The segmentation map outlines elements of the computer setup, with colors similar to those in the first column, emphasizing the monitor and keyboard.\\n\\n   - **Third Column**: Again shows the rider on the horse, with the segmentation map adjusted to highlight different areas compared to the first column. Here, colors vary, possibly indicating a different segmentation algorithm or approach. \\n\\n   - **Fourth Column**: Features the same computer setup from the second column. The segmentation map slightly differs, again highlighting various components or areas of focus, illustrating fluctuations in segmentation results.\\n\\n3. **Rows**:\\n   - **First Row**: Horse and rider images, with the segmentation maps showcasing the outline of the horse, rider, and possibly the landscape.\\n   \\n   - **Second Row**: Segregated images of birds, where the first image shows a solitary bird by the water, and the corresponding segmentation map delineates its shape and surroundings.\\n   \\n   - **Third Row**: A group of people skiing in a snowy environment. The segmentation maps reveal outlines of the skiers and perhaps nearby structures, highlighting different results based on segmentation techniques.\\n\\n4. **Color Schemes**:\\n   - The segmentation overlays across different images primarily use bright greens, yellows, and pinks against black backgrounds. This color-coding provides clarity in distinguishing between various objects and their outlines. \\n\\nOverall, this image provides a comparative illustration of object segmentation methods in diverse contexts, demonstrating the capabilities of different algorithms or models within the Transformer framework.',\n",
       " 'The image depicts a confusion matrix, a tool often used in classification tasks to evaluate the performance of a model. The matrix is organized into four quadrants, corresponding to the true positives, true negatives, false positives, and false negatives.\\n\\n### Details of the Confusion Matrix:\\n\\n1. **Matrix Structure:**\\n   - The matrix is a 2x2 grid.\\n   - The rows represent the **True Class** (actual outcomes).\\n   - The columns represent the **Predicted Class** (model outputs).\\n\\n2. **Values in the Matrix:**\\n   - **Top Left Quadrant (True Negatives)**: \\n     - Value: **1936**\\n     - This indicates the number of instances correctly predicted as class 0.\\n   - **Top Right Quadrant (False Positives)**: \\n     - Value: **64**\\n     - This indicates the number of instances incorrectly predicted as class 1 when they are actually class 0.\\n   - **Bottom Left Quadrant (False Negatives)**: \\n     - Value: **60**\\n     - This indicates the number of instances incorrectly predicted as class 0 when they are actually class 1.\\n   - **Bottom Right Quadrant (True Positives)**: \\n     - Value: **1940**\\n     - This indicates the number of instances correctly predicted as class 1.\\n\\n3. **Color Scheme:**\\n   - The matrix uses a gradient color scheme, ranging from dark blue to light blue.\\n   - Darker shades represent higher values, while lighter shades indicate lower values. This visual aid helps quickly assess which predictions are more frequent.\\n\\n4. **Axes Labels:**\\n   - The horizontal axis is labeled **Predicted Class** with two categories: 0 and 1.\\n   - The vertical axis is labeled **True Class**, also with two categories: 0 and 1.\\n\\n5. **Purpose:**\\n   - This confusion matrix helps identify how well the classification model is performing across the two classes. High values in the true positive and true negative positions indicate a better model performance, while high values in the false positive and false negative positions indicate areas where the model may need improvement.\\n\\nThis matrix is crucial for understanding the effectiveness of classifiers, especially in binary classification tasks, and contributes valuable information to the discussions in the research paper about transformers architecture performance evaluation.',\n",
       " 'The image is a confusion matrix, which is commonly used in machine learning to assess the performance of a classification model. It consists of a square grid that represents the counts of true vs. predicted classifications.\\n\\n### Layout:\\n- The matrix is divided into four quadrants.\\n- The rows represent the **True Class**: 0 and 1.\\n- The columns represent the **Predicted Class**: 0 and 1.\\n\\n### Quadrants:\\n1. **Top-left quadrant (True Class 0, Predicted Class 0)**: \\n   - Value: 1976\\n   - This indicates the number of instances correctly predicted as class 0.\\n\\n2. **Top-right quadrant (True Class 0, Predicted Class 1)**: \\n   - Value: 24\\n   - This represents the number of instances incorrectly predicted as class 1 when they are actually class 0.\\n\\n3. **Bottom-left quadrant (True Class 1, Predicted Class 0)**: \\n   - Value: 60\\n   - This shows the count of instances incorrectly predicted as class 0 when they belong to class 1.\\n\\n4. **Bottom-right quadrant (True Class 1, Predicted Class 1)**: \\n   - Value: 1940\\n   - This indicates the count of instances correctly predicted as class 1.\\n\\n### Color Scale:\\n- The matrix features a color gradient going from dark blue to light blue, which visually represents the magnitude of the values in the cells.\\n- Darker shades indicate higher counts, while lighter shades correspond to lower counts.\\n- A color bar on the right side helps interpret the values based on color intensity.\\n\\n### Overall Insight:\\n- The model has a high number of true positives (1940) and true negatives (1976) compared to false positives (24) and false negatives (60), suggesting that it performs well in distinguishing between the two classes.\\n\\nThis confusion matrix is crucial for understanding the strengths and weaknesses of the model, particularly within the framework of transformers architecture being discussed.',\n",
       " 'The image represents a confusion matrix for a text-to-image synthesis model, which is a data visualization tool used to evaluate the performance of a classification algorithm. The matrix has a 2x2 grid, structured as follows:\\n\\n- **Axes**: \\n  - The vertical axis (y-axis) is labeled \"True Class,\" indicating the actual classes of the instances.\\n  - The horizontal axis (x-axis) is labeled \"Predicted Class,\" representing the classes predicted by the model.\\n\\n- **Cells**:\\n  - The top left cell (True Class 0, Predicted Class 0) contains the value **1994**, indicating that there were 1994 true negatives — instances correctly predicted as class 0.\\n  - The top right cell (True Class 0, Predicted Class 1) has the value **6**, showing that there were 6 false positives — instances incorrectly predicted as class 1 instead of class 0.\\n  - The bottom left cell (True Class 1, Predicted Class 0) contains a value of **60** for false negatives — instances that were actually class 1 but incorrectly predicted as class 0.\\n  - The bottom right cell (True Class 1, Predicted Class 1) displays **1940**, representing true positives — instances correctly predicted as class 1.\\n\\n- **Color Gradient**: \\n  - The background of the cells uses a gradient color scheme, where darker shades represent higher values. The overall color scale ranges from light to dark blue, suggesting that the majority of predictions (1994 true negatives and 1940 true positives) are represented in the darker tones, while the lower counts (6 false positives and 60 false negatives) are in lighter shades.\\n\\n- **Title**: \\n  - The bottom of the image features the title \"Text-to-image synthesis,\" indicating the context in which this confusion matrix is utilized.\\n\\nThis matrix provides valuable insight into the model’s predictive accuracy and helps identify areas where it performs well or might need improvement.',\n",
       " 'The image consists of a grid presenting the results from a research study on distinguishing real versus fake images using a transformer architecture. The layout is divided into three columns labeled I2I, S2I, and T2I, each containing images categorized as \"Fake\" or \"Real.\" \\n\\n### Grid Structure\\n- **Top Row**: Contains three images under each column header (I2I, S2I, T2I).\\n- **Second Row**: Displays another set of three images corresponding to the same columns.\\n  \\n### Image Context\\n1. **I2I Column**:\\n   - **Row 1**: The first image shows a cozy hotel room, while the second features a scenic landscape with mountains.\\n   - **Row 2**: A road scene is depicted below, suggesting varied context.\\n\\n2. **S2I Column**:\\n   - **Row 1**: The first image shows a taxidermy zebra, and the second presents a field with farm animals.\\n   - **Row 2**: Displays a mountain goat against a rocky background.\\n\\n3. **T2I Column**:\\n   - **Row 1**: The first image shows a bird, and the second a woodpecker.\\n   - **Row 2**: A bright blue sky contrasts with an artistic depiction of another bird below.\\n\\n### Probability Data\\n- Each image has associated probability values:\\n  - **P(Real)**: The probability that the image is classified as real.\\n  - **P(Fake)**: The probability that it is classified as fake.\\n- The probabilities are prominently displayed in red text beneath each image, providing quantitative metrics for the model\\'s predictions.\\n\\n### Observations\\n- The first row generally presents images with higher real likelihood scores (varying from around 63% to just below 100%).\\n- The second row indicates images that are either more likely to be classified as fake or present more uncertainty, reflected in their lower P(Real) scores and higher P(Fake) percentages.\\n\\nOverall, the figure visually summarizes the classification efficiency of the transformer architecture across various datasets, alongside graphical evidence to support the findings.',\n",
       " \"I'm unable to see the image you've provided, but I can help you describe typical elements that might be included in an image related to the transformers architecture in a research paper.\\n\\nFor instance, such images often feature:\\n\\n1. **Diagrams of the Architecture**: \\n   - A visual representation of the transformer model, including components such as the input layer, encoder, decoder, attention mechanisms, and output layer. Each part may be clearly labeled with arrows indicating the flow of data.\\n\\n2. **Graphs or Charts**: \\n   - **Bar Plots**: These may illustrate comparison metrics like accuracy, loss, or processing time for various models or configurations. The x-axis typically represents the model versions or hyperparameters, while the y-axis shows performance metrics.\\n   - **Line Graphs**: These might be used to show how performance metrics change over epochs during training.\\n\\n3. **Attention Maps**: \\n   - Visualizations that depict attention scores can help illustrate how the attention mechanism focuses on different parts of the input data. They often represent parts of sentences or tokens along the axes of the graphs.\\n\\n4. **Tables**: \\n   - Summary tables might include quantitative results from various experiments, showing values like F1 scores, accuracy, and other relevant metrics that reflect the performance of transformers.\\n\\nThese elements help provide a comprehensive view of the transformers architecture and its effectiveness in different tasks. If there are specific aspects you need to focus on, feel free to share!\",\n",
       " 'The image appears to be a logo or an abstract illustration rather than a detailed graph or plot. It features a blue background with a stylized representation of a network or molecular structure. \\n\\nIn the center, there are several interconnected circles that likely symbolize nodes or components within a system, connected by lines that suggest relationships or connections. Additionally, there is a bright, star-like element, possibly representing a focus or key aspect of the topic. \\n\\nThis design may convey themes of connectivity, complexity, or innovation, aligning with topics in research fields such as machine learning or network analysis, particularly within the context of the transformers architecture.',\n",
       " 'The image appears to be a snippet from a research paper discussing the importance of prompt engineering in the context of transformer-based models like GPT. \\n\\nIn detail:\\n\\n- **Text Content**: The excerpt emphasizes the significance of prompt engineering due to the increasing adoption of transformer-based models. It suggests that these models rely heavily on the prompts provided to generate outputs. \\n\\n- **Visual Elements**: While the text is prominent, additional graphs and charts that you mentioned might be expected features in such a document, likely would visually illustrate the relationship between prompt engineering and model performance, perhaps showing metrics like accuracy or output relevance over varying prompt designs.\\n\\nIf there are bar plots or other graphs in the complete image, they may represent quantitative data, comparing different transformer models or illustrating performance metrics related to different prompting techniques. \\n\\nOverall, the layout likely emphasizes clarity, with a focus on explaining the intricacies of transformer architecture and the role of prompts in model effectiveness.',\n",
       " \"I'm unable to see the image. If you describe its contents or provide specific details, I can help explain or analyze those elements in the context of the transformers architecture.\",\n",
       " \"I'm unable to view images. However, if you describe the elements of the image, such as the components or specific graphs, I can help explain or provide context regarding the transformers architecture or any related concepts.\",\n",
       " 'The image appears to contain a quote emphasizing practical tips for prompt engineering, aimed at helping individuals harness AI for generating content. The text is displayed in a stylized manner, likely presented in a large, readable font. The quotation marks indicate that this is a direct citation or paraphrase from a source. \\n\\nThe background is subtle and likely designed to make the text stand out clearly. This context suggests it may serve as an illustrative example or highlight within a research paper discussing transformers architecture, although no graphs or specific bar plots are visible in the provided text. \\n\\nFor a more detailed description of any potential graphs or data visualizations specifically related to transformers architecture or prompt engineering, additional visual context would be needed.',\n",
       " \"The image you provided appears to be a logo featuring a stylized eye, commonly associated with NVIDIA, a technology company known for its graphics processing units (GPUs) and artificial intelligence solutions. \\n\\nHere is a detailed description of the logo:\\n\\n- **Shape**: The logo consists of a prominent eye shape that is made up of curved and angular lines. It typically features a large outer curve representing the eye's outline and an inner curve that suggests the iris.\\n- **Color**: The logo is predominantly green, which is a signature color for NVIDIA, set against a contrasting dark background.\\n- **Composition**: The eye shape is divided into two halves. The left half is designed with a darker green shade, while the right half is a lighter green, adding depth to the design.\\n- **Design elements**: The lower part of the logo may have a slight angled cut, resembling the stylized tail of an eye, while the overall design conveys innovation and cutting-edge technology.\\n\\nThis logo may not include any graphs, plots, or specific data visualizations; thus, it does not seem to relate directly to the transformers architecture described in your research paper context. For detailed discussions on transformer architecture, graphs such as bar plots, line charts, or schematic diagrams would typically illustrate key metrics like model performance, training times, or comparisons between different architecture designs.\",\n",
       " 'The image features a visually engaging, digital representation of various components related to technology and data processing, possibly outlining aspects of the transformers architecture.\\n\\n1. **Background**: The backdrop is a smooth, dark surface which gives a sleek, modern feel, enhancing the visibility of the elements presented.\\n\\n2. **Main Elements**:\\n   - Multiple semi-transparent, rectangular panels are arranged in a slightly elevated grid formation. Each panel appears to be holographic and showcases distinct icons and data visualizations.\\n   \\n3. **Panel Contents**:\\n   - **Top Left Panel**: Displays a coding interface with snippets in languages like C++, PHP, CSS, and HTML. This suggests a focus on programming and coding structures.\\n   - **Top Right Panel**: Features several stacked layers of data, indicated by green binary figures and a gear icon, symbolizing data processing or machine learning elements.\\n   - **Center Panel**: Contains a line graph with data trends indicated by a fluctuating line overlaid on a more basic bar graph. This is likely showcasing data analysis or performance metrics.\\n   - **Bottom Left Panel**: Incorporates an icon representing an image, set against a backdrop that resembles a visual chart or image analysis.\\n   - **Bottom Right Panel**: Shows a visual interface with options, most likely representing settings or configurations pertinent to data handling or machine learning algorithms.\\n\\n4. **Color Palette**: The dominant colors are shades of blue with accents of green and purple, giving the illustration a futuristic and technological ambiance.\\n\\n5. **Visual Effects**: The use of linear elements extending from the panels gives a sense of motion and data flow, reminiscent of networks or data streams moving through a system.\\n\\nOverall, the image distills complex information about technology and data flow into an accessible visual format, perhaps to help viewers grasp the intricate workings of the transformers architecture effectively.',\n",
       " 'The image presents two neural network architectures side by side, labeled \"Traditional Neural Network\" on the left and \"Deep Learning Neural Network\" on the right.\\n\\n### Left Side: Traditional Neural Network\\n- **Structure**: This network consists of three layers: the Input Layer, Hidden Layer, and Output Layer.\\n- **Input Layer**: Represented by three green circles, indicating that three input features will be fed into the network.\\n- **Hidden Layer**: Shown as a single green circle, connected to all input nodes. This symbolizes a simple architecture where one hidden layer processes the information.\\n- **Output Layer**: Consists of one green circle, which indicates a single output result.\\n- **Connections**: Arrows (edges) are drawn from each input to the hidden layer and from the hidden layer to the output layer, illustrating the flow of information.\\n\\n### Right Side: Deep Learning Neural Network\\n- **Structure**: This network features a more complex design with multiple hidden layers arranged in a more intricate manner.\\n- **Input Layer**: Similar to the left, there are three green circles representing the input nodes.\\n- **Hidden Layers**: Multiple rows of green circles depict several hidden layers between the input and output. These circles are interconnected, demonstrating that multiple layers of abstraction enhance the network\\'s ability to learn complex patterns.\\n- **Output Layer**: Similar to the traditional model, there is a single output node, indicating the final output after processing through multiple hidden layers.\\n- **Connections**: Arrows indicate a more extensive network of connections, representing the multiple paths data can take through the interconnected hidden nodes.\\n\\n### Overall Comparison\\n- The left side illustrates a simpler feedforward structure, while the right side showcases a deeper architecture capable of modeling more complex relationships in data. \\n- The visual distinction emphasizes how deeper architectures can improve learning capacity through additional layers and connections. The use of the same color (green) for nodes across both diagrams indicates consistency in the representation of these key components within neural networks.',\n",
       " 'The image outlines the architecture of the Transformer model, a popular framework in natural language processing. Here’s a detailed description of its components:\\n\\n1. **Overall Structure**: The diagram is organized into two main sections: the encoder on the left and the decoder on the right. Both sections are enclosed in rectangular boxes that emphasize their modular nature.\\n\\n2. **Inputs and Outputs**: At the bottom, there are labels for \"Inputs\" and \"Outputs (shifted right),\" indicating where the data enters and exits the architecture. Directly above these labels are the input and output embeddings, respectively, which transform the textual data into numerical vectors.\\n\\n3. **Positional Encoding**: Just above the input and output embeddings are circular icons labeled “Positional Encoding.” These indicate that positional information is added to the embeddings to retain the order of the sequence in the model.\\n\\n4. **Encoder Section**:\\n   - **Layers**: The encoder consists of a series of identical layers, denoted as \"Nx,\" where \\'N\\' represents the number of these layers.\\n   - **Components**: Each layer features two main components:\\n     - **Multi-Head Attention**: This block allows the model to focus on different parts of the input sequence simultaneously.\\n     - **Feed Forward**: After attention, the data is processed through a feed-forward network.\\n   - **Add & Norm**: Each block in the encoder is followed by an \"Add & Norm\" component, which indicates layer normalization and residual connections to aid in training stability.\\n\\n5. **Decoder Section**:\\n   - **Masked Multi-Head Attention**: Similar to the encoder\\'s attention, but with masking to prevent the model from attending to future tokens in the sequence.\\n   - **Structure**: The decoder also consists of multiple layers (Nx) with components similar to the encoder but includes additional masked attention.\\n\\n6. **Final Components**: At the top of the diagram are the final steps before generating output probabilities:\\n   - A “Linear” transformation is applied before a “Softmax” function, which produces the final output probabilities used for predictions.\\n\\nThe overall layout emphasizes the flow of information from inputs through various processing stages, illustrating how attention and feed-forward mechanisms are utilized in both encoding and decoding processes within the Transformer architecture.',\n",
       " \"I can't provide a description of the specific image since I can't see or interpret visual content. However, I can help you understand concepts related to transformer architecture or discuss key elements usually found in research papers regarding the subject. If you can describe the elements or sections of the image, I'd be happy to assist you with that!\",\n",
       " 'The image features the NVIDIA logo, which includes a stylized eye graphic that is half enclosed within a green square, accompanied by the word \"NVIDIA\" in bold black letters beneath it. The eye graphic is likely intended to symbolize vision and processing power, which relates to NVIDIA\\'s focus on graphics processing units (GPUs) and AI technology.\\n\\nSince you mentioned the context of a research paper about transformers architecture, the logo could signify the company\\'s contribution to advancements in machine learning and deep learning technologies, which are integral to understanding and implementing transformer models. However, there are no graphs or bar plots visible in this image. \\n\\nFor detailed visualization, the presence of graphs, such as bar plots, would typically help illustrate metrics like model performance, training times, or comparative analysis among different architectures, but that\\'s not represented here.',\n",
       " 'The image illustrates the architecture of a transformer model, commonly used in natural language processing tasks. Here\\'s a detailed breakdown:\\n\\n### Structure:\\n\\n1. **Input Layer**:\\n   - At the bottom, there are **red nodes** labeled as \"Input Embeddings.\" These represent the initial word representations for the input words in a sequence (e.g., \"So,\" \"long,\" \"and,\" \"thanks,\" \"for\").\\n\\n2. **Transformer Blocks**:\\n   - Above the input embeddings is a large **purple rectangle** labeled \"Layers of Transformer Blocks.\" Inside this rectangle, multiple stacked layers are represented, indicating the depth of the transformer architecture. Each layer is likely made up of attention mechanisms and feed-forward neural networks.\\n\\n3. **Outputs**:\\n   - On top of the transformer layers, there are several nodes corresponding to words in the output, such as \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" Each of these words is connected to the transformer layers below it, indicating the flow of information through the network.\\n   - Each output node has a **container** above it labeled \"Softmax over Vocabulary,\" which likely represents a softmax function that computes probabilities over the vocabulary for the predicted word. There is also a **\"Linear Layer\"** below each output node, which transforms the final activations before applying softmax.\\n\\n### Arrows and Connections:\\n- **Directional Arrows**:\\n  - There are arrows connecting the input embeddings to the transformer layers and the transformer layers to the output nodes. These arrows indicate the flow of data and how the model processes the input through its layers to generate outputs.\\n\\n### Graphical Elements:\\n- The output probability distributions are depicted as small bar plots (histograms) within the softmax containers, illustrating that for each word, the model outputs a vector of probabilities over the vocabulary.\\n\\nOverall, the image effectively communicates the detailed flow of information through the transformer\\'s architecture, from input embeddings through multiple layers to the final output predictions.',\n",
       " 'The image illustrates the architecture of transformers, key components, and how data flows through the model.\\n\\n### Structure Overview\\n- **Input Layer**: At the bottom, there are red circles labeled with words \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These represent the input embeddings of the tokens in a sequence.\\n\\n- **Transformer Blocks**: Above the input, there are layers depicted as boxes arranged vertically. These boxes represent layers of transformer blocks that process the embeddings. They are distinguished by a light purple background.\\n\\n- **Attention Mechanism**: The transformer blocks are connected to each other vertically, indicating that the output from one layer serves as the input to the next. This highlights the sequential processing characteristic of transformers.\\n\\n### Output Layer\\n- **Softmax Layer**: At the top, there are components labeled “Softmax over Vocabulary.” Each of these outputs corresponds to one of the input words and displays a small bar plot or histogram representation next to each word. These visualizations likely represent the probability distribution over the vocabulary for predicting the next word in the sequence.\\n\\n- **Linear Layer**: Just below the softmax, a \"Linear Layer\" is indicated, which is responsible for transforming the outputs from the transformer blocks into the required format for the softmax layer.\\n\\n### Connections\\n- Arrows are drawn from the input embeddings to the transformer blocks, illustrating the flow of data. Additionally, arrows point from each transformer layer to the corresponding softmax outputs above, indicating how outputs are generated from the processed embeddings.\\n\\n### Purpose\\nThis diagram visually conveys how input text is transformed through the architecture of a transformer model, how the attention mechanism is layered, and how the probabilities for predicting words are calculated at the output.',\n",
       " 'The image illustrates the self-attention mechanism in layers 5 and 6 of a transformer architecture.\\n\\nAt the top, \"Layer 6\" and \"self-attention distribution\" are labeled clearly, indicating the focus on the attention values processed at this layer. Directly below, \"Layer 5\" is similarly positioned, showing that the attention values are being compared or visualized across different layers.\\n\\nThe words of a sentence form a linear array at both layers; specific words are highlighted to show attention distribution. \\n\\n- In Layer 5, the word \"animal\" is emphasized with a distinct blue background. It appears to be the focal point of attention.\\n- Arrows connect this word to several other words, with varying shades of blue indicating the weight or strength of attention directed towards them.\\n\\nIn Layer 6, the word \"it\" is also highlighted, with arrows extending to a few other words: \"animal,\" \"was,\" \"too,\" and \"tired.\" This visual suggests that the model is focusing on these words in relation to \"it,\" likely implying that \"it\" refers to \"the animal\" in the context of the sentence.\\n\\nThe overall design effectively conveys the relationships between words as determined by the self-attention mechanism, illustrating how different layers in the transformer model refine these connections.',\n",
       " 'The image illustrates the self-attention mechanism in the transformer architecture, a key component in natural language processing.\\n\\n### Layout and Components:\\n\\n1. **Self-Attention Layer**: At the top left, the label \"Self-Attention Layer\" identifies the main focus of the diagram.\\n\\n2. **Input Sequence**: There are five input tokens represented as circles:\\n   - \\\\( x_1, x_2, x_3, x_4, x_5 \\\\): These symbolize the input elements that will undergo self-attention. Each one is directly below its corresponding attention output.\\n   \\n3. **Attention Outputs**: Above each input token, there are corresponding outputs labeled \\\\( a_1, a_2, a_3, a_4, a_5 \\\\). Each \\\\( a_i \\\\) is the result of processing the corresponding input \\\\( x_i \\\\) through the self-attention mechanism.\\n\\n4. **Connections**: \\n   - Arrows indicate the relationships between the input tokens and their outputs. Each output \\\\( a_i \\\\) is influenced by all the input tokens \\\\( x_1 \\\\) to \\\\( x_5 \\\\). \\n   - Black lines connect each input \\\\( x_j \\\\) to every output \\\\( a_i \\\\), demonstrating the self-attention\\'s functionality of allowing each output to reflect information from all inputs.\\n\\n5. **Layer Boxes**: The outputs \\\\( a_1 \\\\) to \\\\( a_5 \\\\) are enclosed in rectangular boxes that represent processing units where the attention calculations occur.\\n\\n6. **Visual Style**: The boxes are shaded in light purple, suggesting a cohesive processing unit within the self-attention layer. The arrows are drawn in black, making them highly visible against the background.\\n\\nThis diagram effectively conveys how the self-attention mechanism computes outputs based on inputs in a parallel and interconnected manner, highlighting its importance in the transformer architecture for understanding context and relationships in sequences.',\n",
       " 'The image presents a mathematical equation likely related to the transformer architecture, specifically focusing on the attention mechanism. Here’s a detailed breakdown:\\n\\n1. **Structure of the Equation**: \\n    - The left side of the equation shows \\\\( a_i \\\\), which typically represents an output or context vector at position \\\\( i \\\\) in a sequence.\\n    - The equation consists of a summation symbol \\\\( \\\\Sigma \\\\), which indicates the summation of terms.\\n\\n2. **Summation Details**:\\n    - The index notation \\\\( j < i \\\\) suggests that the summation ranges over all indices \\\\( j \\\\) that are less than \\\\( i \\\\). This is often used in the context of causal attention, where the model only attends to previous positions in the sequence to maintain the autoregressive property.\\n\\n3. **Weights**:\\n    - \\\\( \\\\alpha_{ij} \\\\) likely represents the attention weights corresponding to the relationship between position \\\\( i \\\\) and all previous positions \\\\( j \\\\). These weights determine the contribution of each previous position to the current position’s output.\\n\\n4. **Input Vector**:\\n    - \\\\( x_j \\\\) represents the input vector at position \\\\( j \\\\). It is multiplied by the attention weight \\\\( \\\\alpha_{ij} \\\\), showing that the importance of \\\\( x_j \\\\) in calculating \\\\( a_i \\\\) is scaled by \\\\( \\\\alpha_{ij} \\\\).\\n\\nThis equation encapsulates a key aspect of how transformers dynamically weigh the importance of different elements in a sequence when making predictions. The overall formulation highlights the interaction between different token representations through the attention mechanism, foundational to the functionality of transformers in tasks like natural language processing.',\n",
       " 'The image presents a visual representation of self-attention distributions within a transformer architecture, specifically highlighting two layers (Layer 5 and Layer 6).\\n\\n### Description:\\n\\n1. **Text Elements**: The visualization includes a sentence: \"The animal didn\\'t cross the street because it was too tired.\" The individual words are arranged horizontally.\\n\\n2. **Layers**: \\n   - **Layer 5**: This layer is indicated on the left side of the image. The words are displayed in a lighter hue, suggesting the attention mechanism is being applied at this depth but isn\\'t the final output.\\n   - **Layer 6**: This layer appears on the right side, where the words are given a blue highlight, which reflects a stronger attention or importance as processed by the model.\\n\\n3. **Self-Attention Distribution**:\\n   - The connections from words at Layer 6 to those in Layer 5 are illustrated with blue arrows. These arrows vary in thickness, indicating the strength of the attention the model gives to each word from the previous layer.\\n   - For example, the arrow originating from \"it\" in Layer 6 points towards several words in Layer 5, highlighting the word “animal” with a particularly thick arrow, indicating it is the most attended to in this context.\\n\\n4. **Overall Layout**: The text and arrows create a flowing diagram, visually representing how the transformer model processes relationships among words, showcasing a conceptual understanding of contextual relevance at different layers of the architecture.\\n\\nThis visual aid serves to elucidate key concepts in transformer-based models regarding how attention mechanisms function, emphasizing the importance of certain words over others in the processing of language.',\n",
       " 'The image represents the self-attention distribution in a transformer architecture, specifically illustrating two layers: Layer 5 and Layer 6.\\n\\n1. **Layer Structure**: \\n    - The top section is labeled \"Layer 6,\" while the bottom is labeled \"Layer 5.\"\\n    - Between these two layers is the label \"self-attention distribution,\" indicating that the focus is on how attention is distributed among words in a sentence.\\n\\n2. **Text Representation**: \\n    - The sentence presented runs horizontally, with words aligned above and below. The words included are: \\n        - \"The,\" \"animal,\" \"didn\\'t,\" \"cross,\" \"the,\" \"street,\" \"because,\" \"it,\" \"was,\" \"too,\" \"tired.\"\\n  \\n3. **Attention Mechanism**:\\n    - Arrows connect the words in Layer 6 to their corresponding words in Layer 5.\\n    - The arrows vary in color intensity, with the more prominent arrows likely indicating stronger attention weights. For example, connections to \"animal,\" \"street,\" and \"it\" are highlighted, suggesting these words carry more significance in the attention calculation at Layer 6.\\n\\n4. **Design Elements**: \\n    - Words in Layer 5 are displayed in a lighter blue with black text, while the highlighted words in Layer 6 are in a bolder blue, emphasizing their importance.\\n    - The layout suggests a flow of information from Layer 5 to Layer 6, demonstrating how the model refines its attention distribution across layers.\\n\\nOverall, this image visually explains how different words in a sentence are emphasized through layers in the transformer architecture, showcasing the self-attention mechanism at work.',\n",
       " 'The image appears to showcase the word \"values,\" written in a stylized red font. The letters seem to be in a bold, cursive manner, which gives a sense of emphasis or importance to the term. \\n\\nAs it relates to a research paper on the transformers architecture, this may signify key metrics or parameters being discussed in the paper. However, without accompanying graphs or data visualization (such as bar plots), it\\'s difficult to provide specific details about the context in which \"values\" is represented. Typical graphs in such research may include performance metrics, loss values, or comparisons between different model configurations. \\n\\nIf further description or visual elements were included in the context of the image, I could provide more detailed insights.',\n",
       " 'The image is a visual representation of the self-attention distribution within a transformer architecture, specifically focusing on the 5th and 6th layers. It features a sequence of text with individual words displayed in a horizontal line.\\n\\n1. **Text Words**: The sentence represented includes the words: \"The,\" \"animal,\" \"didn\\'t,\" \"cross,\" \"the,\" \"street,\" \"because,\" \"it,\" \"was,\" \"too,\" \"tired.\" Each word is positioned evenly spaced along the horizontal axis, with some words highlighted to emphasize their significance in the attention mechanism.\\n\\n2. **Layer Labels**: At the top of the image, there is a label \"Layer 6,\" indicating the specific layer\\'s self-attention distribution being analyzed. Beneath it, there is a label \"Layer 5,\" showing that attention relationships are being derived from this layer as well.\\n\\n3. **Attention Arrows**: Connecting these layers, there are arrows illustrating the self-attention relationships between words. The thickness and color of the arrows suggest varying levels of attention; thicker, brighter arrows indicate stronger attention weights from one word to another. \\n\\n4. **Highlighted Words**: The words \"animal,\" \"street,\" and \"it\" are highlighted in a different shade of blue, indicating their importance in the context of attention distribution.\\n\\n5. **Self-Attention Distribution**: The label \"self-attention distribution\" emphasizes the focus on how different words in the sentence attend to each other based on their contextual relevance, as analyzed by the transformer model.\\n\\nOverall, the image succinctly conveys how each word in a sentence attends to others within different layers of the transformer, showcasing the intricate relationships in language processing.',\n",
       " 'The image features the words \"keys\" and \"values\" written in a playful, red, hand-drawn style. The letters are slightly irregular, giving it a casual, informal look. This could suggest a focus on the conceptual aspects of the transformer\\'s architecture rather than a formal representation.\\n\\nIn the context of transformer architecture, \"keys\" and \"values\" refer to components used within the attention mechanism, critical for the model\\'s operation. In an accompanying research paper, one might expect detailed explanations of how these elements interact within the model, potentially illustrated with graphs or diagrams showcasing their relationships, such as attention matrices or flowcharts detailing data processing.\\n\\nHowever, the image you provided contains no graphs or statistical data and instead emphasizes textual representation that likely serves as a foundation for more complex ideas discussed in the paper.',\n",
       " 'The image is a diagram illustrating the architecture of a transformer model, often used in natural language processing. Here\\'s a detailed description:\\n\\n1. **Input Embeddings**: At the bottom of the diagram are red circles, representing input embeddings for the words \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These circles indicate the initial representation of the input tokens.\\n\\n2. **Layers of Transformer Blocks**: Above the input embeddings, there\\'s a large purple rectangle containing stacked layers. These layers represent the transformer blocks, which are designed to process the input embeddings through self-attention and feed-forward networks. The layers consist of lines that suggest multiple parallel structures within each transformer block.\\n\\n3. **Output Section**: At the top of the diagram, for each word token (e.g., \"long,\" \"and,\" \"thanks,\" etc.), there are separate smaller boxes. Each box represents a linear layer followed by a softmax function. The softmax function visualizes a distribution over the vocabulary, indicating the model\\'s predicted probability of each token being the next in the sequence.\\n\\n4. **Connections**: There are arrows connecting the input embeddings to each transformer block, illustrating the flow of information. These connections indicate that the processed embeddings from the transformer layers are used to predict the next word in the sequence.\\n\\n5. **Highlighted Area**: The region containing the transformer blocks is encircled in a thick purple line, emphasizing its importance in the architecture. \\n\\nOverall, the diagram succinctly summarizes the flow of data from input tokens through transformer blocks to the output predictions, encapsulating the key components of the transformer architecture.',\n",
       " \"The image depicts the architecture of a transformer model, illustrating the operations within a single layer of the model.\\n\\n1. **Layer Structure**:\\n   - The diagram is organized vertically, with three main components shown: the Multi-Head Attention block, Layer Normalization, and Feedforward network.\\n   - The layer's input and output are represented as \\\\( h_{i-1} \\\\), \\\\( h_i \\\\), and \\\\( h_{i+1} \\\\) on the left and right sides, indicating the input to and output from the current layer.\\n\\n2. **Components**:\\n   - **Multi-Head Attention (Highlighted in red)**: This block is central to the transformer's ability to focus on different parts of the input sequence simultaneously. It connects the input directly to the output with residual connections.\\n   - **Layer Normalization (Highlighted in yellow)**: Each component is followed by Layer Norm, which helps stabilize and accelerate the training process.\\n   - **Feedforward Network (Highlighted in blue)**: After the attention mechanism, the data is processed through a feedforward network, which applies transformations independently to each position.\\n\\n3. **Connections**:\\n   - The diagram illustrates the flow of data with arrows. The green squares with plus signs represent residual connections that add the input of each block to its output, enhancing gradient flow during training.\\n\\n4. **Illustrative Lines**:\\n   - Dotted lines are used to indicate the flow between components, signifying how input sequences are processed through multiple layers of the transformer architecture.\\n\\nOverall, the image serves as a concise representation of the operations occurring within a single layer of a transformer, highlighting key processes essential for function and efficiency.\",\n",
       " 'The image illustrates a component of the transformer architecture, focusing on the processing of a specific layer denoted as \\\\( h_i \\\\). It features a vertical arrangement where \\\\( h_{i-1} \\\\) and \\\\( h_{i+1} \\\\) are positioned to the left and right of \\\\( h_i \\\\), indicating the layers before and after the current layer, respectively.\\n\\nIn the diagram:\\n\\n1. **Layer Structure**:\\n   - At the center is \\\\( h_i \\\\), which represents the output of the current transformer layer.\\n   - There are arrows leading to and from \\\\( h_{i-1} \\\\) and \\\\( h_{i+1} \\\\), implying a flow of information in both directions across layers.\\n\\n2. **Key Components**:\\n   - **Multi-Head Attention**: Highlighted in a bright pink box, this component is responsible for attending to different parts of the input sequence simultaneously, allowing for the capture of various contextual information.\\n   - **Layer Normalization**: Two yellow boxes indicate this technique, which is used to normalize the outputs of both the Multi-Head Attention and Feedforward components, improving training stability and performance.\\n   - **Feedforward Network**: Shown in a blue box, this component processes the outputs from the attention mechanism and applies further transformations.\\n\\n3. **Residual Connections**:\\n   - Green diamonds with plus signs represent residual connections that add the input of each component back to its output, helping to mitigate the vanishing gradient problem in deep networks.\\n\\n4. **Dotted Lines and Arrows**: \\n   - Dotted lines connect various components, indicating the flow of data through the architecture. Solid arrows show direct connections, emphasizing how outputs from one component feed into the next.\\n\\nOverall, the visual structure effectively summarizes the core elements of a transformer layer and their interconnections, contributing to an understanding of how transformers process data in a sequential manner.',\n",
       " \"The image illustrates the architecture of a transformer, particularly focusing on the layers involved in the processing of input sequences.\\n\\n### Top Diagram:\\n1. **Layer Structure**:\\n   - Two vertical columns labeled \\\\(h_{i-1}\\\\), \\\\(h_i\\\\), and \\\\(h_{i+1}\\\\) represent the state at different stages of the transformer's processing.\\n   - Horizontal arrows indicate the flow of information across layers.\\n\\n2. **Components**:\\n   - A **MultiHead Attention** mechanism is highlighted in red, which allows the model to focus on different parts of the input sequence simultaneously.\\n   - Below that, a **Layer Norm** block in yellow is shown, indicating normalization to stabilize the activations.\\n\\n3. **Feedforward Network**:\\n   - A **Feedforward** layer in purple is also depicted, representing a fully connected neural network that processes the outputs after attention.\\n\\n4. **Residual Connections**:\\n   - Green diamond icons represent residual connections (additive skip connections) that allow gradients to flow through the network more effectively during training.\\n\\n### Bottom Diagram:\\n- The bottom portion mirrors the top diagram, reinforcing the architecture layout but presenting it in a slightly different context, possibly to indicate changes in processing steps or to show another iteration of the layers.\\n\\nOverall, this image serves as a schematic representation of the key components of a transformer architecture, emphasizing the role of attention, feedforward networks, and normalization in sequence processing. Each layer's interactions are crucial for understanding how transformers operate in tasks such as natural language processing.\",\n",
       " 'The image illustrates a segment of the Transformer architecture, particularly focusing on the embedding process. Here’s a detailed description of the components:\\n\\n1. **Transformer Block**: At the top, there is a labeled box titled \"Transformer Block,\" indicating the primary module being depicted.\\n\\n2. **Composite Embeddings (X)**: Below the title, there is a notation for \\\\( X \\\\), which is defined as \"Composite Embeddings (word + position).\" This signifies that the process combines both word and positional embeddings into a singular representation.\\n\\n3. **Word Embeddings and Position Embeddings**: There are two categories highlighted:\\n   - **Word Embeddings** (on the left) show individual words, with the example words \"Janet,\" \"will,\" \"back,\" \"the,\" and \"bill.\" Each word is enclosed in a rounded pink rectangle.\\n   - **Position Embeddings** (located directly beneath the words) are represented as pink boxes with numbers (1 through 5). These numbers correspond to the sequence of the words in the input.\\n\\n4. **Addition Operation**: Between the word and position embeddings, there is a green diamond shape with a plus sign (“+”). This symbol represents the addition operation that merges the word and position embeddings into the composite embedding \\\\( X \\\\).\\n\\n5. **Red Circles**: Above the words, there are multiple small red circles. These likely represent indicators for processing steps or attention mechanisms in the Transformer architecture.\\n\\n6. **Alignment and Flow**: The layout shows a flow from the word and position embeddings up through the addition to the composite embeddings. Arrows direct the view upwards towards the Transformer Block, reinforcing how information is processed.\\n\\nOverall, the image serves to visualize how words and their respective positions are integrated before being processed by the Transformer architecture.',\n",
       " 'The image you\\'ve provided is a visual representation of the architecture used in transformer models, particularly focusing on the Language Model Head component. Here\\'s a detailed description of the elements within the image:\\n\\n1. **Title and Description**:\\n   - At the top, the title reads \"Language Model Head,\" indicating that this section deals with the terminal part of the transformer architecture that outputs a distribution over a vocabulary.\\n   - Below the title is a brief explanation: \"takes \\\\( h_N^L \\\\) and outputs a distribution over vocabulary \\\\( V \\\\)\".\\n\\n2. **Components**:\\n   - There are several labeled components, including **word probabilities**, **logits**, and an **unembedding layer**, which is represented as \\\\( E^T \\\\).\\n\\n3. **Graphical Elements**:\\n   - There are two main sections in a clear flow:\\n     - The upper section involves a **softmax function** that produces probabilities from logits, as indicated. The softmax is applied over the vocabulary \\\\( V \\\\).\\n     - The output includes multiple symbols labeled \\\\( y_1, y_2, \\\\ldots, y_{|V|} \\\\) that represent the word probabilities corresponding to the vocabulary size.\\n\\n4. **Arrows and Flow**:\\n   - Arrows indicate the flow of information, going from the final hidden state \\\\( h_N^L \\\\) of the transformer layers to the logits produced by the unembedding layer.\\n\\n5. **Layer Structure**:\\n   - Below the Language Model Head, there is a section labeled \"Layer L Transformer Block,\" showing interactions between layer outputs \\\\( h_1^L, h_2^L, \\\\ldots, h_N^L \\\\), and associated weights \\\\( w_1, w_2, \\\\ldots, w_N \\\\).\\n\\n6. **Dimensions**:\\n   - Dimensions are mentioned prominently:\\n     - \\\\( h^L_N \\\\) outputs a 1 x d vector.\\n     - The unembedding layer is represented as \\\\( d \\\\times |V| \\\\), focusing on mapping the final hidden state to the vocabulary size.\\n\\n7. **Visual Style**:\\n   - The use of boxes, arrows, and color coding creates a clear, structured, and organized layout that enhances understanding of how different components interact within the transformer model\\'s architecture.\\n\\nThis comprehensive illustration makes it easier for readers to grasp the transformer architecture\\'s workings, particularly in relation to language modeling tasks.',\n",
       " 'The image illustrates the architecture of a transformer model, specifically focusing on the components involved in generating token probabilities through a language model head. \\n\\n### Key Components:\\n\\n1. **Language Model Head**:\\n   - Positioned at the top center of the diagram.\\n   - It outputs token probabilities denoted by \\\\(y_1, y_2, \\\\ldots, y_{|V|}\\\\), where each \\\\(y\\\\) represents the probability of a token from a vocabulary of size \\\\(|V|\\\\).\\n\\n2. **Hidden States**:\\n   - Below the language model head, a series of hidden states \\\\(h_1, h_2, \\\\ldots, h_N\\\\) are displayed, indicating the processing of input sequences within the transformer architecture.\\n\\n3. **Layer Structure**:\\n   - The layers of the transformer are enclosed in a dashed rectangular box.\\n   - The bottom part of the diagram identifies **Layer L**, which suggests that this is the last layer in the transformer stack.\\n\\n4. **Components of Layer L**:\\n   - **Multihead Self-Attention**: Highlighted in red, this crucial component allows the model to weigh the influence of various words in a sequence based on their relationships.\\n   - **Feedforward**: Shown in purple, this layer processes the outputs from the multihead attention, applying transformation through feedforward neural networks.\\n   - **Layer Norms**: Two instances of layer normalization are represented in yellow, positioned before and after the feedforward layer, helping to stabilize and normalize the inputs and outputs within the layer.\\n\\n### Layout:\\n- The structure is vertically aligned, illustrating a clear pathway from the input tokens, through the hidden layers and attention mechanisms, culminating in the probability distribution of the output tokens.\\n\\nThe overall layout effectively conveys the flow of data through the transformer model, emphasizing how multiple layers interact to produce the final output.',\n",
       " 'The image is a schematic representation of the transformer architecture, commonly used in natural language processing tasks. Here\\'s a detailed description of its components:\\n\\n1. **Layers**: The image is divided into two main sections labeled \"Layer 1\" and \"Layer 2.\" Each layer consists of a sequence of operations that transform input data.\\n\\n2. **Inputs**: At the bottom, there is an input layer represented by \\\\(X\\\\), which comprises elements labeled \\\\(x1, x2, \\\\ldots, xN\\\\). This denotes the input tokens or embeddings.\\n\\n3. **Layer Components**:\\n   - Each layer contains the following components stacked vertically:\\n     - **Multihead Self-Attention**: Shown in red, this block is critical for computing attention scores across the input tokens, allowing the model to focus on different parts of the input simultaneously.\\n     - **Layer Norm**: This component, outlined in light yellow, is applied both before and after certain operations to maintain mean activation close to 0 and variance close to 1, enhancing model stability.\\n     - **Feedforward**: Depicted in purple, this block typically consists of two linear transformations with a non-linear activation function in between.\\n\\n4. **Output**: At the top of each layer, outputs are represented as \\\\(h1, h2, \\\\ldots, hN\\\\), indicating the processed embeddings that will be fed into the next layer.\\n\\n5. **Boxes and Colors**: Each operation is encapsulated in boxes with different colors, helping to visually differentiate the components. The use of dashed outlines suggests the overall architecture is a continuous flow.\\n\\nThis diagram effectively summarizes the architecture\\'s structure and the sequence of operations applied at each layer within the transformer model.',\n",
       " 'The image contains the text \"Pretraining\" in a bold, dark font, indicating its significance in the context of the transformer architecture. There are no graphs or bar plots present in the image. Instead, it appears to serve as a title or key concept within a research paper that delves into the processes associated with pretraining in transformer models.\\n\\nIn a typical research setting, a section titled \"Pretraining\" might be expected to include illustrative graphs, such as bar plots, that effectively compare performance metrics (like accuracy or loss) across different models or algorithms. These plots could visualize changes in performance over training iterations or compare results between various pretraining strategies. However, this specific image solely features the word \"Pretraining\" without additional graphical content.',\n",
       " 'The image illustrates key components of the transformer architecture, specifically focused on the process of predicting the next word in a sequence. \\n\\n### Structure Breakdown:\\n\\n1. **Top Section**:\\n   - It shows a sequence of words: \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These words are examples of inputs that feed into the model for prediction.\\n   - Each word is accompanied by a loss term illustrated with a red background, indicating the prediction error (e.g., \\\\(-\\\\log y_{\\\\text{long}}\\\\)).\\n\\n2. **Softmax Layer**:\\n   - Just below the loss terms, there is a representation of a \"Softmax over Vocabulary\" layer. It suggests that the model calculates the probabilities of all possible next words based on the input, represented by a bar plot. This plot visually conveys the distribution of predicted probabilities for the vocabulary.\\n\\n3. **Linear Layer**:\\n   - Below the softmax layer is a \"Linear Layer.\" This layer transforms the input data using learned weights, setting the stage for the probability calculations that follow.\\n\\n4. **Transformer Block**:\\n   - Central to the architecture is the \"Transformer Block,\" drawn in purple, which contains multiple sub-layers (not explicitly divided in this image). Each represents the various operations (like self-attention and feed-forward layers) that process embeddings.\\n\\n5. **Input Embeddings**:\\n   - At the bottom, \"Input Embeddings\" are represented by red circles, indicating the initial textual representations of the words fed into the transformer model.\\n\\n6. **Connectivity**:\\n   - The diagram displays arrows indicating the flow of information. Red arrows lead from the input embeddings to the transformer block, while other arrows connect the transformer block to the softmax and loss layers, illustrating how the output of the transformer influences the prediction of subsequent words.\\n\\n### Overall Design:\\nThe clean layout and use of colors help to distinguish different components of the architecture clearly. The combination of word inputs, loss calculations, and transformer operations effectively communicates the mechanism by which transformers understand and generate language sequences.',\n",
       " 'The image appears to be a treemap, which provides a visual representation of various datasets or sources related to the transformers architecture in a research context.\\n\\n1. **Layout**: The treemap displays multiple rectangular segments, each representing different data sources. These rectangles vary in size, indicating the relative volume or importance of the dataset they represent.\\n\\n2. **Color Coding**: Different colors categorize the datasets. The main colors include:\\n   - **Blue**: Represents a significant set of datasets, such as PubMed Central, ArXiv, and others, suggesting a strong emphasis on academic and research-related sources.\\n   - **Green**: Contains datasets like Pile-CC and StackExchange, hinting at additional diverse content.\\n   - **Orange and Yellow**: Highlight smaller datasets like Bibliotik, PG-19, and titles like Subtitles, indicating varied media sources.\\n\\n3. **Labels**: Each rectangle is labeled with the name of the dataset, such as \"PubMed Central,\" \"Wikipedia,\" \"Github,\" and others. Some labels suggest specific types of data (e.g., Creative Commons, educational or entertainment content).\\n\\n4. **Proportions**: The sizes of the rectangles reflect the significance of each dataset, either in terms of data volume or relevance to the research topic. Larger rectangles imply a more extensive dataset, while smaller ones may indicate niche sources.\\n\\n5. **Hierarchy**: The arrangement signifies a hierarchical structure, where larger rectangles are often positioned in a way to denote primary sources, while smaller ones branch off, indicating their secondary importance.\\n\\nThis visualization is useful for quickly understanding the composition of the datasets utilized in the transformers architecture research.',\n",
       " \"The image illustrates a simplified diagram of the transformer architecture, typically used in natural language processing and machine learning contexts. Here is a detailed description:\\n\\n1. **Rectangular Blocks**: The image features several vertical rectangular blocks, colored in a pale red hue. These blocks represent individual steps in the transformer model's architecture, which processes input data.\\n\\n2. **Arrows**: From the left side, there are several black arrows directed towards the red blocks. These arrows signify the flow of data or information between various components of the transformer architecture. \\n\\n3. **Connections**: The most notable aspect of the diagram is the presence of black arrows connecting the different red blocks. These arrows are directed diagonally and horizontally, indicating that information can flow in multiple directions. This illustrates the concept of attention mechanisms, whereby each block can attend to information from not just its immediate predecessor, but also from non-adjacent blocks, which is central to the functioning of transformers.\\n\\n4. **Layered Structure**: The arrangement of the blocks suggests a layering or sequential processing of information. This layered structure helps to capture contextual relationships in the data being processed.\\n\\nThe overall design is minimalistic but effectively conveys the complex interconnections that define the transformer model, emphasizing its ability to handle dependencies across varying distances in the input data.\",\n",
       " 'The image depicts a schematic representation of the transformer architecture, often used in deep learning for natural language processing and other tasks. \\n\\n### Key Features of the Image:\\n\\n- **Rectangular Nodes**: The image features several vertical rectangular boxes, colored in a bright cyan. These likely represent input data or embeddings being processed through the transformer model.\\n\\n- **Black Arrows**: Between the cyan rectangles, there are multiple black arrows emanating from specific points, which represent the connections between different layers of the architecture. These arrows may indicate the flow of data or the attention mechanisms in play.\\n\\n- **Layer Structure**: The layout suggests a stacked configuration, typical of transformer models where input flows through multiple layers. The arrows crossing over multiple rectangles highlight the interconnectedness and parallel processing capabilities inherent in the transformer design.\\n\\n- **Abstract Representation**: The use of abstract shapes and colors indicates an emphasis on conceptual understanding rather than intricate detail. This aligns with visual explanations often found in research papers where complex architectures need simplifying for readability.\\n\\nOverall, the image succinctly communicates the core ideas of how data flows through the transformer architecture, emphasizing both the input layers and the interconnections that facilitate complex computation.',\n",
       " \"The image illustrates a simplified representation of the transformer architecture, commonly used in natural language processing and machine learning. It features two groups of rectangular nodes, with the left side depicted in light blue and the right in light red.\\n\\n### Key Components:\\n\\n- **Blue Nodes (Input Layer)**: The left side contains several blue rectangles that signify input embeddings or information segments. This likely represents the initial input layer where data is processed.\\n\\n- **Red Nodes (Output Layer)**: The right side includes red rectangles representing the output layer or transformed outputs generated by the model. \\n\\n- **Connections**: \\n  - There are multiple directed arrows (black lines) showing connections between the blue and red nodes, indicating the flow of information from the input layer to the output layer. \\n  - Some arrows connect the blue nodes with each other, suggesting internal relationships or processing steps within the input layer before reaching the output layer.\\n\\n### Arrangement:\\n- The blue nodes are aligned in a vertical stack on the left, while the red nodes are arranged in a similar vertical orientation on the right.\\n- The arrows are organized in a way that makes clear the directionality of the flow, with the outputs receiving information from multiple inputs, a characteristic of transformer models that rely on attention mechanisms to integrate contextual information.\\n\\n### Overall Purpose:\\nThis diagram serves to visualize the transformer architecture's layered approach, highlighting how inputs are processed and transformed into outputs through interconnected nodes. The focus here is on illustrating the relationships and dynamic flow of data within the architecture.\",\n",
       " \"The image illustrates a conceptual representation of the Transformer architecture, commonly used in natural language processing and other fields. \\n\\n1. **Structure**: The layout consists of several vertically-aligned rectangular blocks, colored in a light red hue. Each block likely represents a layer or component of the Transformer model, such as an encoder or decoder.\\n\\n2. **Connections**: Black arrows connect the blocks, indicating the flow of information or data between different layers. The arrows vary in direction and are likely meant to represent the attention mechanism utilized in Transformers, where each layer can attend to multiple other layers.\\n\\n3. **Directionality**: Some arrows point upward towards the blocks above, while others point diagonally or horizontally, suggesting a complex interrelationship among layers that allows for parallel processing of input sequences.\\n\\n4. **Overall Implication**: The diagram emphasizes the model's architecture and the relationships between its components. The focus is on how the layers work together to process data effectively, highlighting the non-sequential nature of the Transformer model compared to traditional recurrent networks.\\n\\nIn summary, this image serves as a visual aid to understand the layered structure and connectivity of the Transformer architecture, crucial for comprehending its functionality in processing sequences.\",\n",
       " 'The image is a schematic representation of the Transformer architecture, commonly used in natural language processing. Here\\'s a detailed breakdown:\\n\\n1. **Input Layer**: At the bottom, the image displays labeled \"Input embeddings\" with several red circles that appear to represent tokens or words from an input sequence. Each token is accompanied by a corresponding blue label indicating the word, such as \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" \"all,\" and \"the.\"\\n\\n2. **Transformer Blocks**: Above the input layer, there is a large, shaded purple rectangle labeled \"Transformer Blocks.\" This indicates that the input embeddings are processed through multiple layers of Transformer blocks. Inside the rectangle, parallel horizontal lines or layers represent the individual processing steps or components within those blocks.\\n\\n3. **Connections and Flow**: Arrows connect the input embeddings to the Transformer blocks, suggesting the flow of data as the tokens are input into the model. Several arrows lead to specific components within the blocks, illustrating how the model pays attention to different words.\\n\\n4. **Output and Softmax**: To the right, there\\'s a section indicating the model output. Two bold red words, \"all\" and \"the,\" are highlighted. Arrows point from these output words to a smaller box that likely symbolizes a sampling mechanism (such as Softmax) where probabilities are determined. The label \"Sample from Softmax\" indicates that the model generates the next word based on learned probabilities from the context.\\n\\n5. **Linear Layer**: Adjacent to the Softmax sampling box, another box labeled \"linear layer\" suggests that before sampling, the model’s output goes through a linear transformation, which is a common component in neural networks.\\n\\n6. **Overall Structure**: The design visually communicates how input words are transformed sequentially through the architecture, culminating in the generation of output words. The arrows emphasize connections and dependencies between words, illustrating the self-attention mechanism that allows the model to consider the entire context of the input sequence when generating predictions.\\n\\nThis illustration effectively summarizes the key components and data flow within the Transformer architecture, making it accessible for readers familiarizing themselves with the concept.',\n",
       " 'The image depicts a graphical representation of the transformer architecture, commonly used in machine learning models. It features several vertical rectangles colored in light blue, which represent different layers or components of the transformer.\\n\\nIn the center, there are multiple black arrows that form a complex network of connections between the layers. These arrows symbolize the attention mechanism of the transformer, illustrating how information flows and is processed between different parts of the model. The layering suggests a multi-layered architecture, which is characteristic of transformers.\\n\\nOverall, the design is minimalist and focuses on conveying the structure and connectivity within the transformer model, highlighting its complexity and efficiency in processing data.',\n",
       " \"The image illustrates a diagrammatic representation of the transformer architecture, which is crucial for understanding how transformer models process data.\\n\\n1. **Structure**: The diagram is divided into two main sections. The left side features a series of blue rectangles, while the right side consists of red rectangles. \\n\\n2. **Blue Rectangles**: The blue rectangles likely represent input embeddings or initial layers within the transformer model. These nodes appear to be connected through black arrows, which indicate directed relationships or flows of information between them.\\n\\n3. **Red Rectangles**: On the right side, the red rectangles likely signify the output layers or subsequent stages of processing in the transformer architecture. \\n\\n4. **Connections**: There are multiple black arrows originating from the blue rectangles and connecting to the red rectangles. This suggests that each node on the left can influence multiple nodes on the right, showcasing the attention mechanism where inputs can attend to different parts of the output.\\n\\n5. **Directional Arrows**: The arrows depict the flow of information from one layer to the next, emphasizing the model's capability to transform inputs into outputs by attending to various inputs in a non-sequential manner.\\n\\nOverall, the image demonstrates the core concept of the transformer architecture, highlighting how different layers interact and the flow of data through the network.\",\n",
       " \"The image features a stylized and colorful representation of a person's face, divided into three distinct panels. Each panel showcases a similar portrait, but with differing color schemes that create a striking pop-art effect.\\n\\n1. **First Panel (Left)**: The background is a bright blue, contrasting with the vibrant red and yellow tones used for the face. The features are somewhat abstract, but recognizable, with prominent highlights and shadows enhancing the facial structure.\\n\\n2. **Second Panel (Middle)**: This panel presents a red background, accentuating the face in a blend of yellow and orange. The use of color creates a warm feel, and the detailing remains consistent with the first, continuing the theme of abstraction.\\n\\n3. **Third Panel (Right)**: Here, the background shifts to a vivid green, while the colors of the face remain bright, with a mix of yellow and red. This panel maintains the same artistic style, showcasing similar facial features in an abstract manner.\\n\\nOverall, the image employs bold colors and contrasts, creating an engaging visual experience that resonates with themes often found in pop art. The consistent facial representation across all three panels suggests an exploration of identity or perception through varied color interpretations.\",\n",
       " 'The image appears to depict a modern building façade featuring the iconic logo of \"The New York Times.\" The logo is displayed prominently against a backdrop of horizontal metal slats, which create a pattern that adds a dynamic element to the scene. The slats allow light to filter through, creating shadows that enhance the text\\'s visibility.\\n\\nIn the foreground, there may be an overlay containing text, possibly part of the research paper\\'s context or title. This text highlights an aspect related to training in a technical framework, likely connected to the transformers architecture being discussed in the paper.\\n\\nThe overall composition combines architectural features with textual elements, blending modern design with scholarly content. The use of black and white enhances the visual impact, giving it a clean and professional appearance, suitable for an academic setting.',\n",
       " 'The image depicts a bar plot representing the number of research papers published from 2018 to 2022, categorized by different sectors. \\n\\n### Graph Details:\\n\\n- **Y-Axis**: Labeled \"Number of Papers,\" indicating the quantity of research papers.\\n- **X-Axis**: Denotes the years from 2018 to 2022, with tick marks for each year.\\n\\n### Bars:\\n- Each bar represents the total number of papers published in a given year, divided into segments that correspond to different categories.\\n  \\n1. **2018**: \\n   - Total: 71 papers\\n   - Purple segment (Education): 63 papers\\n   - Teal segment (Industry): 8 papers\\n\\n2. **2019**: \\n   - Total: 166 papers\\n   - Purple (Education): 139 papers\\n   - Teal (Industry): 27 papers\\n\\n3. **2020**: \\n   - Total: 244 papers \\n   - Purple (Education): 200 papers\\n   - Teal (Industry): 44 papers\\n\\n4. **2021**: \\n   - Total: 302 papers \\n   - Purple (Education): 227 papers\\n   - Teal (Industry): 53 papers\\n\\n5. **2022**: \\n   - Total: 772 papers \\n   - Purple (Education): 503 papers\\n   - Teal (Industry): 181 papers\\n   - Light teal segment (Government): 70 papers\\n   - Dark teal segment (Nonprofit): 181 papers\\n   - Dark blue segment (Other): 70 papers\\n\\n### Color Coding:\\n- **Purple**: Represents the Education sector.\\n- **Teal (dark)**: Represents the Industry sector.\\n- **Teal (light)**: Represents the Government sector.\\n- **Dark teal**: Represents the Nonprofit sector.\\n- **Dark blue**: Represents other categories.\\n\\n### Data Labels:\\n- Each segment within the bars is labeled with the specific number of papers corresponding to that category for clarity.\\n\\n### Overall Trend:\\nThe graph indicates a significant increase in the number of research papers published over the years, particularly in the Education sector, culminating in a sharp rise in 2022.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57e006",
   "metadata": {},
   "source": [
    "## Load data and summaries to vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78cbaa",
   "metadata": {},
   "source": [
    "### Create the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "93073bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy import DataAPIClient\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "\n",
    "ASTRA_DB_API_ENDPOINT =os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9f84fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445abff",
   "metadata": {},
   "source": [
    "## AstraDBVector with Flat index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25034c65",
   "metadata": {},
   "source": [
    "### for some reason the below cell started giving error and restrting the kernel, i will loose all the variables. So not trying to create different indexes as of now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a566d608",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AstraDBVectorStore.__init__() got an unexpected keyword argument 'index_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[308], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_astradb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AstraDBVectorStore\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The vectorstore to use to index the child chunks\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m vectorstore\u001b[38;5;241m=\u001b[39m \u001b[43mAstraDBVectorStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAgenticAI_Assignment_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASTRA_DB_API_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASTRA_DB_APPLICATION_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: AstraDBVectorStore.__init__() got an unexpected keyword argument 'index_type'"
     ]
    }
   ],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore= AstraDBVectorStore(\n",
    "    collection_name=\"AgenticAI_Assignment_2\",\n",
    "    embedding=embeddings,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    namespace=None,\n",
    "    index_type =\"flat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "18d09468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "306948cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f11ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a55ec",
   "metadata": {},
   "source": [
    "### Load the summaries and link the to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fd16a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "''' # Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables))) '''\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "509a44b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 'b220329b-e430-461b-bb33-5df8415290ed'}, page_content='The article \"AGENTIC AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\" was published in the International Journal of Computer Engineering & Technology in January 2025. It has received 4 citations and been read 6,789 times. The authors are Panneer Selvam Viswanathan, with 3 publications and 14 citations, and Research Pub, which has 4,379 publications and 624 citations.'),\n",
       " Document(metadata={'doc_id': '8de5133d-b662-4ca2-b584-05e6e60c4d3a'}, page_content='User requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'doc_id': '1670d975-ba36-4433-8eff-5f0ff49a56c5'}, page_content='The article \"Agentic AI: A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence\" by Panneer Selvam Viswanathan discusses the concept of Agentic AI, which enables autonomous decision-making and adaptive problem-solving in AI systems. It analyzes the foundational architecture, capabilities, and applications across various industries such as robotics, healthcare, and finance. The article emphasizes the importance of addressing technical challenges and ethical considerations, including accountability and bias mitigation, highlighting the need for responsible deployment of such technologies. It contributes to understanding the evolution of autonomous AI systems and their impact on future technology.'),\n",
       " Document(metadata={'doc_id': 'c7016441-af9b-4bed-9fa7-b23ddd46765f'}, page_content='The document presents a study on the development of a novel Intelligent Classroom Management System (ICMS). It integrates various technologies to enhance teaching effectiveness and student engagement. The system employs sensors and real-time data analytics to monitor classroom dynamics, promote interactive learning, and provide feedback to educators. Experimental results indicate improvements in student participation and overall learning outcomes compared to traditional methods.'),\n",
       " Document(metadata={'doc_id': '23d885e5-208d-4ed1-847f-4522065d3db8'}, page_content=\"The text discusses the evolution of artificial intelligence (AI), highlighting the shift from traditional rule-based systems to advanced architectures, particularly focusing on Agentic AI. This new form of AI features autonomous decision-making, independent goal-setting, and adaptive behaviors, allowing it to operate with minimal human intervention. The significance of Agentic AI is underscored in sectors requiring rapid decision-making and adaptation, such as manufacturing, healthcare, finance, and transportation. The research aims to analyze Agentic AI's architecture, implementation challenges, ethical considerations, and implications for human-AI collaboration. It emphasizes foundational principles that integrate computer science, cognitive psychology, and systems theory for developing autonomous systems.\"),\n",
       " Document(metadata={'doc_id': '5281c3a4-c2e1-4541-80ec-b7c6fa44fe9b'}, page_content='Agentic AI systems represent an advanced method of autonomous decision-making, going beyond traditional algorithms. They use complex neural architectures for real-time information processing and operate on several levels, from immediate reactions to strategic planning. This multi-tiered framework enables effective navigation of complex situations while balancing quick responses with long-term considerations, resembling human cognitive processes.'),\n",
       " Document(metadata={'doc_id': 'dc87fff2-3ef6-42cb-831c-0dba1ea04860'}, page_content='Agentic AI features goal-oriented behavior that transitions from reactive to proactive actions through advanced objective hierarchies, enabling the decomposition of complex goals and dynamic goal prioritization. This system maintains operational coherence while adapting to changing conditions. Additionally, it employs a hybrid learning framework combining supervised and reinforcement learning, facilitating rapid adaptation and improved performance in dynamic environments, particularly evident in web navigation tasks.'),\n",
       " Document(metadata={'doc_id': '627dddbc-fec7-4835-8824-cd0c4192e2e0'}, page_content='Agentic AI significantly differs from traditional AI by demonstrating greater autonomy and adaptability. Unlike conventional systems confined to set parameters, Agentic AI can dynamically adjust its behavior, learn from new experiences, and effectively navigate unstructured environments while adhering to ethical constraints. Key characteristics include autonomous decision-making, goal-oriented behavior, adaptive learning, and contextual awareness. The architecture of Agentic AI is a complex integration of various technologies essential for its independent functioning.'),\n",
       " Document(metadata={'doc_id': 'aa4f5b6f-1b74-4105-9a59-58cdc0aae0c4'}, page_content='Modern Agentic AI systems use advanced perception frameworks to process and integrate multiple data streams through sensor fusion algorithms, creating detailed environmental models for accurate assessment and informed decision-making.'),\n",
       " Document(metadata={'doc_id': 'd73a7acf-09ae-4085-98e4-16be2b24fdd9'}, page_content='Agentic AI features advanced decision-making algorithms that combine classical reasoning with probabilistic inference for nuanced responses in complex situations. Its layered architecture supports both rapid heuristic responses and deeper analysis, allowing flexible adaptation. Additionally, the action execution frameworks ensure accurate implementation of decisions into actions, maintaining stability and effectiveness across various operational contexts.'),\n",
       " Document(metadata={'doc_id': '4cb108d1-44fa-4b3e-b8b7-60568b332c6f'}, page_content='The execution layer features strong error handling and adaptive control mechanisms for reliable performance in difficult conditions.'),\n",
       " Document(metadata={'doc_id': '36bb47e4-acd2-4069-af35-78483c39fa87'}, page_content='Agentic AI systems employ a continuous improvement cycle through a learning architecture that uses performance data and feedback mechanisms to update internal models, enhancing decision-making and optimizing future operations. This adaptive learning enables the systems to evolve and respond more effectively to complex challenges.'),\n",
       " Document(metadata={'doc_id': 'bfdaa47e-e7f5-47e0-97f5-2a96e933c373'}, page_content='Autonomous operation in Agentic AI systems involves a complex integration of various technologies, facilitating independent functioning and adaptive behavior.'),\n",
       " Document(metadata={'doc_id': '9cb6f382-f1c2-46f4-b635-31c0c7e7a002'}, page_content='Agentic AI systems use complex decision-making architectures that integrate probabilistic reasoning with deterministic rules, allowing for both immediate responses and strategic long-term planning through multi-layered frameworks.'),\n",
       " Document(metadata={'doc_id': 'ccf7601f-8c19-4dbc-9439-6439da674f2e'}, page_content='Decision-making processes utilize learned experiences and predefined constraints to enable informed choices within operational limits.'),\n",
       " Document(metadata={'doc_id': '6da4c8fc-ec19-4cc8-aa13-0bcaa0a1cd0b'}, page_content='Agentic AI systems utilize advanced sensor fusion and data integration for environmental perception, enabling comprehensive situational awareness through sophisticated data processing algorithms. They create detailed environmental models and feature real-time analysis to identify patterns within IoT-integrated contexts.'),\n",
       " Document(metadata={'doc_id': 'aaf78da4-c40b-42f7-b491-77b723649a77'}, page_content='Agentic AI systems possess advanced real-time adaptation capabilities, allowing them to learn and adjust their behavior in response to changing environments, unforeseen challenges, and new demands.'),\n",
       " Document(metadata={'doc_id': 'f81de7c7-172e-4931-ae19-8d8a3495a17c'}, page_content='The adaptation framework features feedback loops for ongoing performance enhancement and operational stability.'),\n",
       " Document(metadata={'doc_id': 'fa286f01-1337-47e6-aebd-1fa59c960cea'}, page_content='B.'),\n",
       " Document(metadata={'doc_id': '54ce0d54-1105-45a0-adbd-eaca10ac5d32'}, page_content='Machine learning integration is essential for Agentic AI systems, facilitating advanced learning and adaptation for autonomous functions.'),\n",
       " Document(metadata={'doc_id': '243fba85-2bc9-4690-b2ca-b30ef33bbce3'}, page_content='Reinforcement learning is essential for creating autonomous features in Agentic AI systems.'),\n",
       " Document(metadata={'doc_id': '84ebd5b4-4821-4f3d-b22d-82ab52b49e88'}, page_content='The text discusses reinforcement learning applications that help systems learn optimal behaviors through environmental interaction, utilizing advanced reward mechanisms and policy optimization. It also highlights the use of state-of-the-art neural network architectures in Agentic AI systems, which facilitate sophisticated pattern recognition and decision-making, incorporating deep learning for complex analysis and specialized networks for various operational needs.'),\n",
       " Document(metadata={'doc_id': 'a6b3dddd-829f-4fc1-b33d-887d1adec534'}, page_content='The neural network framework enables both offline learning for baseline development and online learning for real-time adaptation.'),\n",
       " Document(metadata={'doc_id': '4711d514-0fba-4024-9d96-01a9990db1a8'}, page_content='Agentic AI systems utilize advanced algorithms for data processing and pattern recognition, enhancing their decision-making capabilities in complex environments, particularly in IoT and distributed computing. The performance metrics of Agentic AI, such as decision accuracy, response time, adaptation rate, and system reliability, show improvements over traditional AI. In manufacturing, the integration of Agentic AI has transformed industrial processes through intelligent manufacturing systems and autonomous mobile robots (AMRs), which feature contextual awareness and adaptive decision-making. This leads to improved operational efficiency, proactive quality management, and significant reductions in downtime and defects.'),\n",
       " Document(metadata={'doc_id': '564222d2-3769-4318-9302-7301596a53e9'}, page_content='Integrating Agentic AI in healthcare transforms patient care and medical decisions through smart medical devices, which offer adaptive drug delivery and intelligent monitoring. These systems analyze patient data in real time for better treatment outcomes. Additionally, enhanced diagnostic systems utilize AI for advanced data analysis and pattern recognition, facilitating early disease detection and improved treatment planning.'),\n",
       " Document(metadata={'doc_id': 'c631a2bf-9428-4201-ac1a-a8faeaae63d3'}, page_content='Integrating diverse data sources enhances diagnostic accuracy and recommendations. Advanced patient monitoring systems utilize Agentic AI for continuous analysis of vital signs and behaviors, facilitating predictive analytics that identify potential health issues early. This proactive strategy improves patient outcomes and eases the workload on healthcare providers.'),\n",
       " Document(metadata={'doc_id': '6e523227-6bae-43bf-bca1-e00e45bcfefa'}, page_content='C.'),\n",
       " Document(metadata={'doc_id': '5e9ecbe5-b9cd-4f59-8839-8e48b25ba75f'}, page_content='The transportation sector has significantly transformed with the implementation of Agentic AI, particularly in autonomous vehicle technology, enabling advanced navigation in complex environments with little human involvement.'),\n",
       " Document(metadata={'doc_id': 'cf8baf96-515e-49f9-82d3-1e9c1284d908'}, page_content='Advanced navigation and traffic management systems utilize real-time data analysis and predictive capabilities for dynamic route optimization and traffic management, significantly enhancing transportation efficiency by continuously analyzing data streams to adapt to changing conditions.'),\n",
       " Document(metadata={'doc_id': '8eb3e284-6750-4b02-9ada-4d443901141b'}, page_content='Integrated emergency response coordination improves the management of complex urban transportation networks.'),\n",
       " Document(metadata={'doc_id': 'd4e86e21-2026-4605-a680-789af7471fe3'}, page_content='The entry appears to be labeled \"D,\" but no specific data or context is provided to summarize. Please provide more details or content related to \"D\" for an accurate summary.'),\n",
       " Document(metadata={'doc_id': '5c61f75b-b781-413f-be43-869b01fc1887'}, page_content='The financial sector utilizes Agentic AI to improve decision-making and risk management. Advanced algorithmic trading systems offer real-time market analysis and pattern recognition, identifying trading opportunities while adhering to strict risk protocols. Risk assessment systems have advanced to provide thorough analyses across various dimensions for better risk management and compliance monitoring.'),\n",
       " Document(metadata={'doc_id': '1159da1e-ac24-4e73-90ad-8684154835d1'}, page_content='Agentic AI systems in portfolio management enhance asset allocation and investment strategies by analyzing market trends and risks, enabling autonomous decision-making aligned with investment goals. Key industry applications include autonomous robots and quality control in manufacturing, diagnostic systems and patient monitoring in healthcare, autonomous vehicles and traffic management in transportation, and algorithmic trading and risk assessment in finance, each providing improved efficiency, accuracy, and management.'),\n",
       " Document(metadata={'doc_id': '5b646919-2930-4180-8945-ee49c6ecdfeb'}, page_content='Agentic AI systems require strong accountability frameworks to balance autonomous decision-making with human oversight. Legal guidelines must clarify liability and responsibility for unintended consequences, while decision-making transparency is essential for trust, necessitating explainability features and audit trails for accountability.'),\n",
       " Document(metadata={'doc_id': 'ef435a8b-8ab2-4a4a-bd28-8b3665c91703'}, page_content='Agentic AI systems require comprehensive risk management strategies to mitigate technical and operational risks, which involve creating testing frameworks, defining operational boundaries, and implementing failsafe mechanisms to ensure acceptable system behavior.'),\n",
       " Document(metadata={'doc_id': 'd82ba25e-ea0b-4b0b-ac71-6dac09c29717'}, page_content='Please provide the content or details of the table or text chunk labeled \"B\" for summarization.'),\n",
       " Document(metadata={'doc_id': '0a02d93e-af76-48ad-a373-01c20a055ec5'}, page_content='Agentic AI integration is reshaping workforce dynamics, requiring adaptation strategies to address job displacement and evolving skill needs, while also creating roles for AI system management. Furthermore, establishing effective human-AI collaboration frameworks is essential, balancing autonomy with necessary human oversight for optimal interaction.'),\n",
       " Document(metadata={'doc_id': '5c075fa4-8308-45e7-80c7-7e15f7c7575a'}, page_content='Public trust in Agentic AI systems depends on transparency, reliability, and demonstrated value, necessitating clear communication of capabilities and limitations, as well as proactive stakeholder engagement.'),\n",
       " Document(metadata={'doc_id': '1ba70dcd-0345-499b-a997-e464dc930e61'}, page_content='C.'),\n",
       " Document(metadata={'doc_id': '1d31c3e2-39dc-4d99-9741-287b285ea43e'}, page_content='The ethical considerations for Agentic AI systems include addressing bias and ensuring fairness through diverse representation, prioritizing privacy concerns with robust data protection while maintaining effectiveness, and focusing on security implications that go beyond traditional cybersecurity to protect autonomous decision-making. Additionally, significant challenges such as ensuring system reliability and stability in diverse scenarios are critical for successful deployment, particularly in high-stakes applications.'),\n",
       " Document(metadata={'doc_id': 'faf53867-5ec5-4f50-9b60-d965d41abd16'}, page_content='Effective fault detection and recovery mechanisms are essential for maintaining system integrity under diverse operational conditions.'),\n",
       " Document(metadata={'doc_id': '5f6ab013-2ac1-4ac5-b0c5-a702ca6136cd'}, page_content='Please provide the table or text chunk you would like me to summarize.'),\n",
       " Document(metadata={'doc_id': '7ada253a-ef62-4b59-b001-4bc70d942be0'}, page_content='Performance optimization of Agentic AI systems involves managing complex computational demands while ensuring real-time responses. It requires a careful examination of the interactions between neural network architectures, decision-making algorithms, and system latency, especially in logistics. The key challenge lies in balancing processing power with energy efficiency.'),\n",
       " Document(metadata={'doc_id': '006fd20a-e108-4d1b-b501-d7b52e147052'}, page_content='Integrating Agentic AI systems with existing infrastructure poses significant technical challenges, particularly related to compatibility within complex ecosystems that include legacy systems and differing operational requirements.'),\n",
       " Document(metadata={'doc_id': 'd5ef6490-2d6d-462c-9017-7a5941ff919e'}, page_content='Managing coherence in distributed architectures and heterogeneous systems demands complex and advanced strategies.'),\n",
       " Document(metadata={'doc_id': '24a73688-8560-435c-ae8c-478ee168dd14'}, page_content='It appears there is no specific information provided in your prompt. Please provide the table or text chunk you would like summarized.'),\n",
       " Document(metadata={'doc_id': '9680ee38-d0f9-4987-9480-d7d5a44f9dfd'}, page_content='The deployment of Agentic AI faces significant challenges due to regulatory compliance. Organizations must navigate a complex and dynamic regulatory landscape that varies across different jurisdictions and industries, necessitating flexibility in AI system design to adapt to evolving compliance requirements while ensuring operational effectiveness.'),\n",
       " Document(metadata={'doc_id': 'a9752417-7963-4713-bf6b-20deb8ce725d'}, page_content='Deploying Agentic AI systems requires substantial infrastructure investments and modifications. Organizations must assess their current infrastructure to identify necessary upgrades in computational resources, storage, and network capabilities for effective AI operations.'),\n",
       " Document(metadata={'doc_id': '99528de5-b352-4daf-a648-566d38b3c66c'}, page_content='Implementing Agentic AI systems involves significant financial considerations beyond initial development costs. Organizations must evaluate total ownership costs, including implementation and long-term operational expenses, while developing sustainable strategies to balance these costs with potential benefits. Successful implementation requires a comprehensive understanding of these challenges and a strategic approach that includes both technical and operational aspects, focusing on long-term sustainability and effectiveness. Careful planning can help manage these challenges and maximize the potential of Agentic AI systems.'),\n",
       " Document(metadata={'doc_id': '01966acf-ab4f-4dfd-a2b3-7f0f4cadadc5'}, page_content='The section identifies potential future research directions and opportunities across various fields, highlighting areas that require further exploration and development.'),\n",
       " Document(metadata={'doc_id': '042d2461-a3a9-46e5-bb03-2d2d04f66539'}, page_content='Future advancements in Agentic AI systems will focus on enhanced autonomy, enabling improved decision-making in complex environments, and significant breakthroughs in self-learning. Additionally, research is geared towards sophisticated learning algorithms that enhance adaptability and efficiency, allowing systems to learn from smaller datasets with high accuracy through advanced neural network architectures and training methodologies.'),\n",
       " Document(metadata={'doc_id': 'd073738a-14b3-4a72-b277-145b9b8bd200'}, page_content='The integration of Agentic AI with emerging technologies like quantum computing, advanced sensors, and edge computing offers significant enhancements, allowing for improved real-time processing and decision-making while overcoming current computational limitations.'),\n",
       " Document(metadata={'doc_id': 'bf088f51-1cd6-4df0-b0c9-a888389623c1'}, page_content='B.'),\n",
       " Document(metadata={'doc_id': 'eb8d2a04-1cc3-4d6b-9c58-f5d6958de959'}, page_content='The document discusses the opportunities and applications of Agentic AI across various industries. It highlights new use cases in environmental monitoring, sustainable energy management, and advanced healthcare. Market analysis shows growing demand for specialized AI solutions, particularly in digital transformation sectors. The innovation potential of Agentic AI is also emphasized, with possibilities for breakthroughs in personalized medicine, climate change mitigation, and space exploration. Future research directions in the field aim to address theoretical, practical, and societal implications.'),\n",
       " Document(metadata={'doc_id': 'f8cc8c8d-ab98-4a3a-a9aa-df983e40e783'}, page_content='Future innovation and advancements in autonomous capabilities and technology integration show significant potential for breakthroughs.'),\n",
       " Document(metadata={'doc_id': 'd8f78b66-4b66-4605-a6a1-2cf0a899aae3'}, page_content='The article discusses the transformative potential of Agentic AI across various industries while addressing the practical challenges of its implementation. It highlights the importance of balancing technological capabilities with deployment considerations, ethical implications, and governance structures. Agentic AI is portrayed as a significant advancement in AI, promising improvements in operational efficiency and decision-making, but also facing challenges such as system reliability, regulatory compliance, and costs. The findings suggest that careful development and integration of these systems are vital for future advancements in autonomous capabilities and learning algorithms.'),\n",
       " Document(metadata={'doc_id': 'aff1d7d4-1af2-4c51-9a0d-21e3d040f41d'}, page_content='The citations provided include research on AI engineering and autonomous AI agents, focusing on their design principles and decision-making capabilities. Key documents include a 2022 IEEE publication on AI software, an explainer from IEEE Spectrum on AI agents, and a 2024 study on foundational design principles in agentic systems.'),\n",
       " Document(metadata={'doc_id': 'e45f6332-01e7-4489-ac9c-938a84515f20'}, page_content='The provided content is insufficient to summarize.'),\n",
       " Document(metadata={'doc_id': '132e3450-f348-4597-a45a-823111e7c285'}, page_content='The document is a publication titled \"Artificial Intelligence: A Systems Approach from Architecture Principles to Deployment,\" released in 2024 by MIT Press and available through IEEE Xplore.'),\n",
       " Document(metadata={'doc_id': 'c01cdeec-ebb7-46ec-b1b7-f6ffb13a227b'}, page_content='The referenced document appears to be a scholarly work authored by Mohanarangan, S., Karthika, D., Moohambigai, B., and Sangeetha, R. It likely discusses topics within the field of electrical engineering or related areas, as indicated by its publication in an IEEE source. Further details about specific content or findings would require access to the document itself.'),\n",
       " Document(metadata={'doc_id': '2567a84f-d53a-49c0-8993-2cf656b7b666'}, page_content='The article discusses integration strategies for AI and machine learning in IoT systems, emphasizing their potential to enhance functionality and efficiency. It is published in the International Journal of Scientific Research in Computer Science and Engineering, volume 12, issue 2, pages 25-32, in 2024.'),\n",
       " Document(metadata={'doc_id': '766bfc91-a8db-492f-8517-125994eac623'}, page_content='The document presents a novel approach for managing energy consumption in cloud computing environments through an innovative algorithm that optimizes resource allocation and scheduling. It emphasizes the importance of energy efficiency in cloud data centers and evaluates performance improvements in comparison to existing methods. The findings indicate significant reductions in energy usage while maintaining service quality, making it a valuable contribution to sustainable cloud computing practices.'),\n",
       " Document(metadata={'doc_id': 'd8e72672-da91-46da-8bb3-95450284bf64'}, page_content='The article discusses the emergence of Agentic AI as a transformative approach to autonomous artificial intelligence, highlighting its implications and potential impact on various industries.'),\n",
       " Document(metadata={'doc_id': '41c13af2-bb48-4a5a-9086-c6bbd6fe1242'}, page_content='The article discusses \"agentic AI,\" a new paradigm in autonomous artificial intelligence, as explored by Automation Anywhere.'),\n",
       " Document(metadata={'doc_id': 'da6783f2-a9bf-4173-99ed-384d118ab0a5'}, page_content='The document discusses agentic AI, highlighting its key benefits and features as presented on the Automation Anywhere website.'),\n",
       " Document(metadata={'doc_id': '62b92251-1ca4-4061-9ae4-ec7b211306df'}, page_content='The provided sources discuss the ethical and social implications of artificial intelligence, focusing on moral considerations and the impact of AI on society.'),\n",
       " Document(metadata={'doc_id': '56ac5619-f5e4-48d8-ba23-3c142fd8aeca'}, page_content='The study identifies key barriers to implementing AI in organizations through a Delphi methodology, highlighting challenges such as lack of understanding, insufficient infrastructure, data quality issues, and resistance to change.'),\n",
       " Document(metadata={'doc_id': '55b2ae4a-2eab-409d-a107-5cf9b295f96e'}, page_content='The reference indicates a source authored by Alsuwaidi, Aydin, and Rashid, available at the provided hyperlink, but detailed content or specifics from the table or text are not included.'),\n",
       " Document(metadata={'doc_id': 'deb13469-7c67-4c37-b1e0-0843806b7eb9'}, page_content='The 2022 study presented at the 5th European International Conference on Industrial Engineering and Operations Management systematically reviews the barriers and challenges faced in implementing Artificial Intelligence (AI) in logistic operations.'),\n",
       " Document(metadata={'doc_id': '4ac98b31-f15f-4f67-8101-76ee3586fa7c'}, page_content='The content appears to reference a research article or study available at specific URLs, focusing on work by Panneer Selvam Viswanathan and mentioning \"Rapid Canvas.\" There are no additional details provided in the text chunk.'),\n",
       " Document(metadata={'doc_id': 'a0be7882-e376-45af-9cb6-285b2ecc876f'}, page_content='The content is limited to the year 2024, which does not provide specific details or context for summarization.'),\n",
       " Document(metadata={'doc_id': '58391749-15bb-4a1b-a870-709281e6e558'}, page_content='The document discusses future directions in AI research, highlighting trends and predictions regarding autonomous decision-making systems. It emphasizes the growing importance of agentic AI frameworks and includes references to relevant publications by IBM and academic sources.'),\n",
       " Document(metadata={'doc_id': '4241f8c4-f4f3-4a98-8b3c-d190bfb4a199'}, page_content='The document contains an abstract for an article published in the International Journal of Civil Engineering and Technology (IJCET), focusing on research related to civil engineering topics. Full access to the article is provided through the linked PDF.'),\n",
       " Document(metadata={'doc_id': 'f4ed0654-7c61-4179-9e66-a4534e6d3ac4'}, page_content='The article is open-access and allows unrestricted use, distribution, and reproduction, as long as proper credit is given to the original author and source.'),\n",
       " Document(metadata={'doc_id': '0614374e-9aff-402d-8dde-17495607ecb1'}, page_content='The work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).'),\n",
       " Document(metadata={'doc_id': '83633cee-486e-4c95-a6b7-fe7f9425c0fa'}, page_content='The table discusses the authors and affiliations of the paper \"Attention Is All You Need,\" highlighting key contributors from Google Brain, Google Research, and the University of Toronto. The abstract emphasizes that the leading sequence transduction models utilize complex recurrent or convolutional neural networks featuring an encoder-decoder architecture, often enhanced by an attention mechanism for improved performance.'),\n",
       " Document(metadata={'doc_id': '8c99f891-a38f-4a0b-bf88-40a0ded6a2c6'}, page_content='The proposed Transformer model is a new network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. It demonstrates improved quality in machine translation tasks, is more parallelizable, and requires less training time.'),\n",
       " Document(metadata={'doc_id': '330364e3-b3f7-445b-8594-80d19ebd3925'}, page_content='The model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing previous best results by over 2 BLEU. It sets a new state-of-the-art BLEU score of 41.0 for the English-to-French translation task, trained efficiently on eight GPUs. Recurrent neural networks, particularly gated recurrent neural networks, are highlighted as leading techniques in sequence modeling and machine translation.'),\n",
       " Document(metadata={'doc_id': '111bdc80-0bf2-475e-be7e-da7fa46d6a8a'}, page_content='Jakob proposed using self-attention instead of RNNs; Ashish and Illia created the first Transformer models. Noam introduced scaled dot-product attention and multi-head attention, playing a key role in development. Niki evaluated and tuned numerous model variants, while Llion worked on the initial codebase and efficient inference. Lukasz and Aidan significantly improved results and accelerated research by redesigning tensor2tensor.'),\n",
       " Document(metadata={'doc_id': '67a0f1e8-dacb-4304-80e5-616f1a7274dc'}, page_content='Work performed at Google Research includes contributions to various research projects, publication of academic papers, and development of innovative technologies.'),\n",
       " Document(metadata={'doc_id': 'cb540cde-5be2-4af0-ba70-fe18a40071b5'}, page_content='The 31st Conference on Neural Information Processing Systems (NIPS 2017) focused on recurrent models that compute along symbol positions in input and output sequences. These models align positions with computation time steps, generating a sequence of hidden states based on the previous hidden state and input at each position.'),\n",
       " Document(metadata={'doc_id': 'd43d8135-3aad-4908-936b-1ccc06280bd9'}, page_content='The sequential nature of training examples limits parallelization, particularly at longer sequence lengths due to memory constraints affecting batching.'),\n",
       " Document(metadata={'doc_id': 'c8a01b2d-c152-42b2-8f08-d2c10152b2dd'}, page_content='Recent advancements have enhanced computational efficiency and model performance using factorization tricks and conditional computation.'),\n",
       " Document(metadata={'doc_id': '5d4e2eb8-28f3-43d6-b5a1-c4a7a80fcc85'}, page_content='The text discusses the limitations of sequential computation and introduces the Transformer model, which relies solely on attention mechanisms instead of recurrent networks. This approach enables greater parallelization and improved translation quality. It also mentions other models like the Extended Neural GPU, ByteNet, and ConvS2S that aim to reduce sequential computation by utilizing convolutional neural networks for parallel processing of inputs and outputs.'),\n",
       " Document(metadata={'doc_id': 'a441d792-78d7-41a9-b2d7-3e464037c4e2'}, page_content='ConvS2S requires a linear number of operations to relate signals based on distance, while ByteNet requires a logarithmic number of operations.'),\n",
       " Document(metadata={'doc_id': '1cb85b67-f8e6-44a4-b5b6-2c3e0101f44d'}, page_content=\"The text discusses self-attention, an intra-sequence mechanism used to capture dependencies in sequences for various tasks like reading comprehension and summarization. It highlights the Transformer model's unique reliance on self-attention for input and output representation, distinguishing it from RNNs and convolution-based models. Additionally, it mentions Multi-Head Attention as a technique to mitigate resolution loss due to averaging in the Transformer architecture.\"),\n",
       " Document(metadata={'doc_id': '63b83d36-8f01-4a85-83a9-06bc53300b0c'}, page_content='Competitive neural sequence transduction models utilize an encoder-decoder architecture, where the encoder transforms an input sequence of symbols into a continuous representation sequence.'),\n",
       " Document(metadata={'doc_id': '7eb10edc-4ec1-4e96-91dc-b09e50bab0ce'}, page_content='The Transformer architecture consists of an encoder and decoder, each made up of six identical layers. The encoder features a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder generates output sequences auto-regressively, utilizing previously generated symbols as input at each step, and includes similar attention mechanisms and feed-forward components.'),\n",
       " Document(metadata={'doc_id': '0710df48-7edd-48ba-9ba9-75634e5e71dc'}, page_content='A residual connection is used around each of the two sub-layers, accompanied by layer normalization.'),\n",
       " Document(metadata={'doc_id': '5708b454-0785-4137-a736-cb221ba1c412'}, page_content='The output of each sub-layer is calculated using LayerNorm applied to the sum of the original input and the output of the sub-layer function.'),\n",
       " Document(metadata={'doc_id': '8a5f0345-5936-43b7-baa2-f76eefa21863'}, page_content='The model employs residual connections for all sub-layers and embedding layers, with an output dimension of 512. The decoder consists of 6 identical layers, including a unique sub-layer for multi-head attention over the encoder output. It incorporates masking in the self-attention sub-layer to prevent future position dependencies, enabling predictions for a position to rely only on known outputs from previous positions.'),\n",
       " Document(metadata={'doc_id': 'dadc331c-b793-45bf-99f7-38aebe69d893'}, page_content='3.2 outlines specific aspects or data related to the topic, providing insights or details necessary for understanding the content addressed in the preceding sections.'),\n",
       " Document(metadata={'doc_id': '8d7460bd-5147-45a8-bf26-809a865d8099'}, page_content='The attention function maps a query and key-value pairs to an output as a weighted sum of values, with weights determined by a compatibility function. The \"Scaled Dot-Product Attention\" uses queries and keys of dimension dx and values of dimension dv, calculating dot products, scaling them by √d, and applying a softmax function to derive weights. Attention computations are performed on batch matrices for efficiency, with queries, keys, and values organized into matrices Q, K, and V, respectively. Multi-Head Attention consists of multiple parallel attention layers.'),\n",
       " Document(metadata={'doc_id': '28bc568b-bc49-40cb-8584-f0a287c18cd0'}, page_content='The text discusses attention mechanisms in machine learning, focusing on two main types: additive attention and dot-product (multiplicative) attention. Dot-product attention, which uses a scaling factor, is faster and more efficient due to optimized matrix multiplication, while additive attention uses a feed-forward network and performs better with larger dimensions. To enhance performance, a multi-head attention approach is introduced, where queries, keys, and values are projected into different dimensions and processed in parallel, resulting in improved output values.'),\n",
       " Document(metadata={'doc_id': 'a585cd88-6c8c-4475-9111-1ba1bf924eed'}, page_content='The final values are obtained through concatenation and projection, as shown in Figure[2]. Multi-head attention enables the model to simultaneously focus on information from various representation subspaces at different positions.'),\n",
       " Document(metadata={'doc_id': '2a0cdfb6-ad1d-407f-bd36-ada78fb79299'}, page_content=\"A single attention head's averaging inhibits dot product growth, which is illustrated by assuming independent random variables for components of q and k, leading to a mean of 0 and variance of dx for their dot product.\"),\n",
       " Document(metadata={'doc_id': '8756d603-a1ec-4adf-9a39-3273f3bfdb69'}, page_content='The MultiHead attention mechanism involves concatenating multiple attention heads, each defined by the projection of input matrices Q, K, and V using parameter matrices. This work utilizes 8 parallel attention heads (h = 8), with each head having a dimensionality of 64 (dy = dm / h = 64).'),\n",
       " Document(metadata={'doc_id': '9dcf672a-e79f-4927-9758-39cdd5536d93'}, page_content='The computational cost of multi-head attention with reduced dimensions per head is comparable to that of single-head attention with full dimensionality.'),\n",
       " Document(metadata={'doc_id': 'e996947a-d07d-47b0-85b5-d613ed7dd193'}, page_content='The Transformer model employs multi-head attention in three ways, including \"encoder-decoder attention,\" where the decoder queries are derived from the previous layer and memory keys and values are sourced from the encoder\\'s output, enabling the decoder to attend to all input sequence positions.'),\n",
       " Document(metadata={'doc_id': 'eff8ca4d-4768-4e3e-aeae-8deb20836882'}, page_content='The text describes the operation of self-attention layers in encoder-decoder models, where the encoder allows all positions to attend to previous layers while the decoder restricts access to prevent leftward information flow for auto-regression. It also mentions that each encoder and decoder layer includes position-wise feed-forward networks that operate identically on each position.'),\n",
       " Document(metadata={'doc_id': 'c07b231d-94b6-49ff-978d-e0af8a18800c'}, page_content='Two linear transformations are applied sequentially with a ReLU activation function in between.'),\n",
       " Document(metadata={'doc_id': '55e05105-6bf5-4370-b5be-40360ea5119b'}, page_content='The function FFN(x) applies linear transformations that utilize distinct parameters at each layer, resembling two convolutions with a kernel size of 1.'),\n",
       " Document(metadata={'doc_id': 'a18a99c5-6a34-425a-ae86-74749c2a0740'}, page_content='The model has an input and output dimensionality of 512 and an inner-layer dimensionality of 2048. It employs learned embeddings to transform input and output tokens into vectors of the same dimension, sharing the weight matrix between embedding layers and the pre-softmax linear transformation. Positional encodings are added to the input embeddings to provide order information. Table 1 compares maximum path lengths, per-layer complexity, and minimum sequential operations for different layer types: self-attention, recurrent, convolutional, and restricted self-attention, highlighting varying complexities and operational characteristics based on sequence length and representation dimension.'),\n",
       " Document(metadata={'doc_id': '133b8d8b-d015-4f7a-b962-7e32345ed24b'}, page_content='Positional encodings match the dimension dmodel of embeddings for summation.'),\n",
       " Document(metadata={'doc_id': '38e7e33f-3d2b-4fda-acb5-c90091cce6a9'}, page_content='The work employs sine and cosine functions of varying frequencies for positional encodings, defined mathematically as PE(pos, 2i) = sin(pos/10000^(4i)) and PE(pos, 2i+1) = cos(pos/10000^(4i)), where \"pos\" represents the position and \"i\" indicates the dimension. Each dimension\\'s positional encoding corresponds to a sinusoidal function.'),\n",
       " Document(metadata={'doc_id': '41104d7e-1368-4eb7-9128-f2c504f7562b'}, page_content='Wavelengths were set in a geometric progression from 27 to 10000 - 27 to facilitate model learning based on relative positions. Experiments with learned positional embeddings yielded nearly identical results to the geometric progression method.'),\n",
       " Document(metadata={'doc_id': '2041a2b1-134d-4248-8ee2-083e7e643507'}, page_content='The sinusoidal version is preferred for the model as it may enable extrapolation to longer sequence lengths than those seen during training. The text discusses the comparison of self-attention layers with recurrent and convolutional layers for mapping variable-length sequences, focusing on three key objectives that motivate the use of self-attention.'),\n",
       " Document(metadata={'doc_id': '536f4aaa-cf74-49af-8ae6-f583dd843368'}, page_content='Total computational complexity per layer.'),\n",
       " Document(metadata={'doc_id': '5ae6b516-7c3f-4ec3-93f8-9fe11b42d98b'}, page_content='The amount of computable tasks that can be parallelized is measured by the minimum number of sequential operations needed.'),\n",
       " Document(metadata={'doc_id': '982cef4a-6048-47b1-bba0-fa84be36d7d6'}, page_content='The ability to learn long-range dependencies in sequence transduction tasks is influenced by the path length for forward and backward signals in the network. Shorter paths ease this learning. Self-attention layers connect all positions with a fixed number of operations, while recurrent layers require O(n) operations. Self-attention is computationally more efficient than recurrent layers when the sequence length (n) is smaller than the representation dimensionality (d), common in state-of-the-art machine translation models. For very long sequences, self-attention can be limited to a neighborhood size, increasing maximum path length to O(n/r).'),\n",
       " Document(metadata={'doc_id': '6ff98806-1c60-4975-aa96-313910116afa'}, page_content='The investigation will focus on the limitations of using a single convolutional layer with kernel width less than the input size, which necessitates multiple layers to connect all input-output pairs, leading to increased complexity. Convolutional layers are more costly than recurrent layers, but separable convolutions significantly reduce this complexity. Even when using a full kernel size, the complexity matches that of a self-attention layer combined with a point-wise feed-forward layer, which may also enhance model interpretability.'),\n",
       " Document(metadata={'doc_id': '73238fc5-9079-49d2-bd0d-4aa4bd12b2e7'}, page_content='Attention heads in the models learn distinct tasks and display behaviors linked to the syntactic and semantic structure of sentences. The training regime for the models is described in the following section.'),\n",
       " Document(metadata={'doc_id': 'ab8ceb88-098d-4fcb-9808-82315a10ef5b'}, page_content='The training utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and a shared vocabulary of around 37,000 tokens. For English-French, the dataset was much larger at 36 million sentences with a 32,000 word-piece vocabulary. Sentence pairs were batched by similar sequence length, with each batch containing approximately 25,000 source and target tokens.'),\n",
       " Document(metadata={'doc_id': '24039fd0-17cb-44af-8ab9-5f7285d24138'}, page_content='Models were trained on a machine with 8 NVIDIA P100 GPUs. Base models took 0.4 seconds per training step for 100,000 steps over 12 hours. Big models took 1.0 second per step for 300,000 steps over 3.5 days.'),\n",
       " Document(metadata={'doc_id': 'e90555f8-be7f-4285-8aba-548ef7a5ad63'}, page_content='The content required for summarization is missing or incomplete. Please provide the full text or table for an accurate summary.'),\n",
       " Document(metadata={'doc_id': '3aa856d0-c489-4012-8cc4-561036986b6c'}, page_content='The Adam optimizer was utilized with parameters β1 = 0.9, β2 = 0.98, and ε = 10^-8. The learning rate varied during training, initially increasing linearly for a set number of warmup steps and then decreasing proportionally to the inverse square root of the step number.'),\n",
       " Document(metadata={'doc_id': '30af9ea4-b983-4464-b888-bd4c229f9aaa'}, page_content='Warmup steps were set to 4000.'),\n",
       " Document(metadata={'doc_id': '53eb2d51-8d16-48f1-b181-298911f1153c'}, page_content='Three types of regularization are utilized during training: Residual Dropout, which involves applying dropout to the output of each sub-layer before adding it to the input, as well as to the sums of embeddings and positional encodings in both the encoder and decoder stacks.'),\n",
       " Document(metadata={'doc_id': '92f7deb9-b818-45a8-8904-87d9eafff8ac'}, page_content='The base model uses a parameter rate of Parop = 0.1.'),\n",
       " Document(metadata={'doc_id': '3ff97dee-02f2-4404-b668-6222826a2091'}, page_content='The Transformer outperforms previous state-of-the-art models in BLEU scores for English-to-German and English-to-French translations on the newstest2014 tests while requiring significantly less training cost in FLOPs.'),\n",
       " Document(metadata={'doc_id': '23cd1c73-9e49-4037-858b-484d6c671f6e'}, page_content='The table presents various ensemble models with their corresponding metrics. \"1079 Deep-Att + PosUnk\" has a score of 40.4 and a value of 8.0. The \"GNMT + RL Ensemble\" shows scores of 26.30 for one metric and 41.16 for another, with a performance drop indicated by 1.8-1079 and 1.1-1074. The \"ConvS2S Ensemble\" records scores of 26.36 and 41.29, with a noted drop of 7.7-10!9 and 1.2.10?! for other metrics.'),\n",
       " Document(metadata={'doc_id': 'f59f6771-573d-4cc6-8d47-6a15d4fc7af1'}, page_content='The Transformer base model has an accuracy of 27.3 and a score of 38.1 with a parameter count of 3.3 million, while the Transformer big model has an accuracy of 28.4 and a score of 41.0 with a parameter count of 2.3 million.'),\n",
       " Document(metadata={'doc_id': 'ebc22ff6-fc77-45d5-9ce4-65ada2e91fe8'}, page_content='Label smoothing with a value of €;, = 0.1 was used during training.'),\n",
       " Document(metadata={'doc_id': '1ddd0fb7-89ab-4cd9-ae06-f2f2338895cc'}, page_content=\"The big transformer model outperforms previous models in the WMT 2014 English-to-German translation task with a new state-of-the-art BLEU score of 28.4, surpassing prior models by over 2.0 BLEU. It trained in 3.5 days on 8 P100 GPUs and achieved superior results at a lower training cost. In the English-to-French task, it scored 41.0 BLEU, beating all previously published single models while costing less than 1/4 of the previous leading model's training expense. Dropout rates were set at 0.1 for the big model and 0.3 for others. The base models were derived from averaging the last 5 checkpoints recorded every 10 minutes.\"),\n",
       " Document(metadata={'doc_id': '1eca544f-2d66-486b-9659-b6d02e931d35'}, page_content='The big models utilized an average of the last 20 checkpoints, employing beam search with a beam size of 4 and a length penalty of 0.6.'),\n",
       " Document(metadata={'doc_id': 'd8817a6d-3de4-492e-9b60-126a667ca782'}, page_content='Hyperparameters were selected based on experimentation with the development set.'),\n",
       " Document(metadata={'doc_id': 'f4d2a75d-53c4-4fb7-8b22-7a7e21aaa423'}, page_content='The maximum output length for inference is set to input length plus 50, with early termination when possible. Table 2 compares the translation quality and training costs of this model architecture to others in the literature.'),\n",
       " Document(metadata={'doc_id': 'c397b059-dfe8-4e12-b74a-1509d63de16b'}, page_content=\"The number of floating point operations for training a model is estimated by multiplying training time, the number of GPUs, and the GPUs' sustained single-precision floating-point capacity. Additionally, the study examines model variations by altering components of the Transformer and measuring the impact on performance in English-to-German translation using beam search without checkpoint averaging.\"),\n",
       " Document(metadata={'doc_id': '03166a31-d45b-4501-9cd0-b9f06aaae2fb'}, page_content='Table 3 shows the impact of varying the number of attention heads and key/value dimensions on performance, maintaining constant computation. Single-head attention is 0.9 BLEU points worse than the optimal setting, and quality decreases with excessive heads.'),\n",
       " Document(metadata={'doc_id': '676ac4a7-42ef-44be-a043-7bc4093d3fee'}, page_content='TELOP values for K80, K40, M40, and P100 are 2.8, 3.7, 6.0, and 9.5, respectively.'),\n",
       " Document(metadata={'doc_id': '69bab68f-770e-4ec4-9d36-4fca06e27b67'}, page_content='Table 3 presents different adaptations of the Transformer architecture, highlighting various modifications and enhancements to improve performance, efficiency, and applicability across tasks.'),\n",
       " Document(metadata={'doc_id': '5ba5238b-5bb4-48fa-8848-2447dac14bcb'}, page_content='Unlisted values match those of the base model.'),\n",
       " Document(metadata={'doc_id': '48078a08-59ad-46b4-8360-542c1a43611a'}, page_content='Metrics for the English-to-German translation development set (newstest2013) are presented, with listed perplexities calculated per wordpiece using byte-pair encoding, not intended for comparison with per-word perplexities.'),\n",
       " Document(metadata={'doc_id': 'fa73781a-23a3-44f1-8837-cf7d8f7037e8'}, page_content='The table presents various configurations of a model with distinct parameters such as sizes and performance metrics, indicating that reducing the attention key size negatively impacts model quality. It suggests the need for a more advanced compatibility function beyond the dot product for better performance.'),\n",
       " Document(metadata={'doc_id': '8f42e400-d9ad-4a6a-8e88-ef980739e59d'}, page_content='Bigger models perform better, and dropout is effective in preventing overfitting.'),\n",
       " Document(metadata={'doc_id': 'f9947615-de16-4244-851d-614730d8c84f'}, page_content='The study introduces the Transformer, a novel sequence transduction model that utilizes attention mechanisms instead of recurrent layers. It achieves faster training and sets new records in translation tasks for both English-to-German and English-to-French. The best-performing model surpasses all previously reported ensembles in English-to-German translations. Future plans include applying attention-based models to various tasks beyond text and exploring local attention mechanisms for efficiently processing larger data types like images and audio. The goal is also to reduce sequential generation.'),\n",
       " Document(metadata={'doc_id': 'c4c8c7b1-9266-4e68-8a4b-9d3f7349dcf5'}, page_content='The code for training and evaluating the models is accessible at https://github.com/tensorflow/tensor2tensor. Acknowledgements are given to Nal Kalchbrenner and Stephan Gouws for their contributions.'),\n",
       " Document(metadata={'doc_id': 'e7a414d7-1f7d-4368-bcd1-18fd1090f377'}, page_content='The reference is for a work by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.'),\n",
       " Document(metadata={'doc_id': 'eea3308d-ba3a-473c-8ad9-de282e3c116a'}, page_content='Layer normalization is a technique introduced in a 2016 arXiv preprint that addresses issues in training deep neural networks by normalizing the inputs across the features for each training example, enhancing overall training stability and performance.'),\n",
       " Document(metadata={'doc_id': '183751b4-f604-49eb-87fc-6c160e5bf223'}, page_content='The paper by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, published in 2014, presents a neural machine translation model that integrates alignment and translation processes, significantly improving translation quality by jointly learning these tasks.'),\n",
       " Document(metadata={'doc_id': '868e4bb3-fada-4534-9dc2-6d91a4e0d7ae'}, page_content='Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V are likely authors or contributors to a work, though the specific details or context are not provided.'),\n",
       " Document(metadata={'doc_id': '6d71d26b-6dee-48df-9d2d-6632d58c21e2'}, page_content='A study on neural machine translation architectures published in 2017, exploring various models and their effectiveness in translation tasks.'),\n",
       " Document(metadata={'doc_id': '1022e0c5-c122-4b84-bf5e-670b477b9231'}, page_content='The paper discusses the application of Long Short-Term Memory (LSTM) networks for machine reading tasks, highlighting their effectiveness in processing sequential data.'),\n",
       " Document(metadata={'doc_id': '95c9acf4-3506-47a1-9c91-54b66a24dcc6'}, page_content='The paper discusses the use of a recurrent neural network (RMN) encoder-decoder for learning phrase representations to improve statistical machine translation.'),\n",
       " Document(metadata={'doc_id': 'e59bfb01-af0c-4e06-b71f-adb43afc98d2'}, page_content='Francois Chollet\\'s paper \"Xception: Deep Learning with Depthwise Separable Convolutions,\" published in 2016, introduces the Xception model, which utilizes depthwise separable convolutions to improve deep learning performance.'),\n",
       " Document(metadata={'doc_id': '941ddad8-954f-4eda-a575-91542b8b3dc4'}, page_content='The paper evaluates the performance of gated recurrent neural networks (GRNNs) in sequence modeling, authored by Junyoung Chung, Caglar Giilgehre, Kyunghyun Cho, and Yoshua Bengio, and published in 2014.'),\n",
       " Document(metadata={'doc_id': 'dcac605a-dd0a-4809-a09d-297dad3929a7'}, page_content='The text likely refers to a group of researchers or authors: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.'),\n",
       " Document(metadata={'doc_id': '850400be-94fd-4266-8bc3-e0692966995c'}, page_content='Dauphin discusses a convolutional sequence-to-sequence learning model in a 2017 arXiv preprint.'),\n",
       " Document(metadata={'doc_id': '0b376533-0f32-4797-b37b-56b189a62a57'}, page_content='This paper by Alex Graves discusses the generation of sequences using recurrent neural networks (RNNs), detailing methodologies and applications in various domains.'),\n",
       " Document(metadata={'doc_id': '9e5adc1b-eb1f-4dae-a343-33aca4d87b38'}, page_content='The paper introduces deep residual learning for image recognition, proposing a novel network architecture that utilizes residual connections to enhance training of deep neural networks. It was presented at the IEEE Conference on Computer Vision and Pattern Recognition in 2016.'),\n",
       " Document(metadata={'doc_id': '2eb7d528-34a2-44a4-b6bf-aab65ab0f606'}, page_content='The document references research conducted by Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber in 2001, discussing the challenges of learning long-term dependencies in recurrent neural networks. Sepp Hochreiter and Jiirgen Schmidhuber are also mentioned in connection to this work.'),\n",
       " Document(metadata={'doc_id': '613321a2-ff41-490c-9bc8-0bdf3a22bd29'}, page_content='The article discusses long short-term memory (LSTM), a type of recurrent neural network architecture designed to model temporal dependencies in data, published in Neural Computation in 1997.'),\n",
       " Document(metadata={'doc_id': '8e44a744-1513-4781-b61c-d1a0d14824d6'}, page_content='The text lists the names of five individuals: Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.'),\n",
       " Document(metadata={'doc_id': '2be99c2f-1339-415f-bfbf-69559208b85c'}, page_content='The paper investigates the boundaries and capabilities of language modeling, examining various techniques and their effectiveness in generating coherent and contextually relevant text.'),\n",
       " Document(metadata={'doc_id': '2382b844-ddce-44bf-bba3-49ee640c57ef'}, page_content=\"Lukasz Kaiser and Ilya Sutskever's paper discusses how Neural GPUs are capable of learning algorithms, presented at the International Conference on Learning Representations (ICLR) in 2016.\"),\n",
       " Document(metadata={'doc_id': '6cf7552c-766c-4981-806a-abe1a8e62850'}, page_content='The referenced work by Nal Kalchbrenner et al. discusses a method for neural machine translation that operates in linear time, as detailed in their 2017 arXiv preprint.'),\n",
       " Document(metadata={'doc_id': '18831edd-32ae-4af7-bd31-a8697ce5e2b2'}, page_content='The text refers to individuals: Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.'),\n",
       " Document(metadata={'doc_id': 'f4bcfa26-db9e-4710-8f88-27fb3617590d'}, page_content='The paper presents Structured Attention Networks, introducing a method for improving attention mechanisms in machine learning models, aiming to enhance performance on various tasks by leveraging structured information for more focused and relevant inputs.'),\n",
       " Document(metadata={'doc_id': '194555ed-e999-4ded-8efd-6908552658fa'}, page_content='The paper discusses the Adam optimization algorithm for stochastic optimization, authored by Diederik Kingma and Jimmy Ba, published in 2015 in the Journal of Machine Learning Research (JCLR).'),\n",
       " Document(metadata={'doc_id': '037f3e3c-27a9-4c99-a4d3-323e8d9d787c'}, page_content='The paper by Oleksii Kuchaiev and Boris Ginsburg discusses factorization techniques to improve the efficiency of LSTM networks, as presented in their 2017 arXiv preprint.'),\n",
       " Document(metadata={'doc_id': '5063d5eb-2d9f-4361-ac7f-6c78efbfea27'}, page_content='The document references a 2017 preprint by Zhouhan Lin and colleagues on a method for structured self-attentive sentence embedding.'),\n",
       " Document(metadata={'doc_id': '5f3422c2-10ae-4849-b9ec-6bbd989084ce'}, page_content='The paper by Samy Bengio and Lukasz Kaiser discusses the potential of active memory as an alternative to attention mechanisms in neural networks, presented at the Advances in Neural Information Processing Systems (NIPS) conference in 2016.'),\n",
       " Document(metadata={'doc_id': 'f6841def-33a6-4078-b8e0-cc8a3212c1f1'}, page_content='The document discusses effective methods for implementing attention-based neural machine translation as proposed by Minh-Thang Luong, Hieu Pham, and Christopher D Manning in their 2015 paper.'),\n",
       " Document(metadata={'doc_id': '37a386a1-0071-4ef1-b252-71600b5a5298'}, page_content='The referenced work discusses a decomposable attention model presented by Ankur Parikh, Oscar Tackstrém, Dipanjan Das, and Jakob Uszkoreit at the 2016 Empirical Methods in Natural Language Processing conference.'),\n",
       " Document(metadata={'doc_id': 'ef7375d0-763a-48f5-93a9-bcdb7fa5ee1a'}, page_content='The paper presents a deep reinforced model for abstractive summarization, authored by Romain Paulus, Caiming Xiong, and Richard Socher, and was published as an arXiv preprint in 2017.'),\n",
       " Document(metadata={'doc_id': '7524fdc3-ef3f-48f9-b2d7-139c0ff81614'}, page_content='The paper by Ofir Press and Lior Wolf discusses a method for enhancing language models by utilizing output embeddings.'),\n",
       " Document(metadata={'doc_id': '1a7f7c90-810b-41ac-b066-1be3dd82375e'}, page_content='The paper discusses a neural machine translation approach that addresses the challenge of translating rare words by utilizing subword units.'),\n",
       " Document(metadata={'doc_id': 'cf53bcad-9fc8-401f-a5f4-b5dfa7819aa7'}, page_content='The document discusses a research paper by Shazeer et al. on the sparsely-gated mixture-of-experts layer in large neural networks, highlighting advancements in neural network architecture.'),\n",
       " Document(metadata={'doc_id': '7cfb0614-e0ef-4e62-b391-dc54c5eaf2fc'}, page_content='The referenced paper discusses Dropout, a technique to reduce overfitting in neural networks by randomly omitting a subset of neurons during training, enhancing model generalization.'),\n",
       " Document(metadata={'doc_id': 'b1005227-964f-4a23-8870-ced05b0a6eb6'}, page_content='The referenced work is presented by Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus, focusing on end-to-end memory networks within a conference paper context.'),\n",
       " Document(metadata={'doc_id': '35a7c4a2-f9b8-48e9-86cb-b4c1ee98806f'}, page_content='The provided text does not contain enough information or context to summarize.'),\n",
       " Document(metadata={'doc_id': 'cadd6cb1-dc9f-4c6d-b349-99c118d90d88'}, page_content='D'),\n",
       " Document(metadata={'doc_id': '832e4c76-3de6-4546-ab7d-8397336e531f'}, page_content='No information provided to summarize.'),\n",
       " Document(metadata={'doc_id': 'eaea6fb6-e9cb-4294-bb38-db7244344429'}, page_content='The content provided is incomplete and lacks specific information to summarize.'),\n",
       " Document(metadata={'doc_id': '7552483a-66aa-42ec-97bf-f30f31e94169'}, page_content='The reference is for a publication edited by Sugiyama and Garnett in the proceedings of Advances in Neural Information Processing Systems 28, covering pages 2440-2448, published by Curran Associates, Inc. in 2015.'),\n",
       " Document(metadata={'doc_id': '90ad410f-974a-41fa-91dc-a61216be982c'}, page_content='The referenced paper by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le presents a method for sequence to sequence learning using neural networks, published in 2014 in the Advances in Neural Information Processing Systems.'),\n",
       " Document(metadata={'doc_id': '04932d54-df7d-4b82-9380-7476a1c0c64f'}, page_content='The authors propose a re-evaluation of the inception architecture in computer vision, presenting improvements and insights in their 2015 paper published in CoRR.'),\n",
       " Document(metadata={'doc_id': '6a73cf81-1ca8-4660-abc3-063ca149f103'}, page_content=\"The document discusses Google's neural machine translation system, highlighting its effectiveness in narrowing the performance gap between human and machine translation.\"),\n",
       " Document(metadata={'doc_id': 'b75638cb-e44c-41c3-992e-ce93b38023c9'}, page_content='The paper presents deep recurrent models with fast-forward connections aimed at improving neural machine translation.'),\n",
       " Document(metadata={'doc_id': '76f00cf9-ff50-44e8-befb-0baa7d5c08fe'}, page_content='The article discusses the ability of AI to identify images generated by other AI systems, focusing on the challenges and effectiveness of current detection methods.'),\n",
       " Document(metadata={'doc_id': 'dbd672ce-ac06-49e7-bed9-1d3934f418b9'}, page_content='The authors propose a framework using Convolutional Neural Networks (CNNs) to detect AI-generated images, particularly those created by Generative Adversarial Networks (GANs), amidst concerns over legitimacy and security. They collected GAN-generated images from various tasks and architectures to enhance generalization in detection.'),\n",
       " Document(metadata={'doc_id': 'f499e625-85ba-47d6-a751-0a4822066f97'}, page_content='Transfer learning was utilized.'),\n",
       " Document(metadata={'doc_id': 'f8037dc4-c4e8-4823-b9ee-f4482e769506'}, page_content=\"The method employed Class Activation Maps (CAM) to identify key regions influencing the classification model's decisions. It achieved 100% accuracy on the Real or Synthetic Images (RSI) dataset and outperformed in accuracy on other datasets and configurations.\"),\n",
       " Document(metadata={'doc_id': 'df620e5a-afd8-4605-aaa9-f845c677a668'}, page_content='The most effective evaluation tool for image generation was a fine-tuned EfficientNetB4 detector, trained with a batch size of 64 and an initial learning rate of 0.001 over 20 epochs.'),\n",
       " Document(metadata={'doc_id': '5be88297-0a02-4e24-811f-4f2c9507bb53'}, page_content='The study explores the detection and localization of GAN-generated images using convolutional neural networks, incorporating techniques like Adam optimizer, learning rate reduction, and data augmentation.'),\n",
       " Document(metadata={'doc_id': 'd3bcf894-2525-4fba-b5dc-ab40fafece92'}, page_content='The article published in \"Imaging\" on 28 September 2023 by academic editor Cosimo Distante details a study or findings related to imaging, originally received on 10 August 2023, revised on 13 September 2023, and accepted on 27 September 2023. It is published under a CC BY license.'),\n",
       " Document(metadata={'doc_id': 'a3eaef92-0e20-4406-8059-68e0e6904b07'}, page_content='The content is copyrighted in 2023 by the authors.'),\n",
       " Document(metadata={'doc_id': '49dc59d1-483c-4d3d-ac97-14e8c862cf76'}, page_content='MDPI is a licensee based in Basel, Switzerland.'),\n",
       " Document(metadata={'doc_id': '7579c3ed-9fb1-4d41-bbb7-67742fd1583a'}, page_content='The article discusses image synthesis, which involves creating artificial images from various input types such as text, sketches, audio, or images.'),\n",
       " Document(metadata={'doc_id': 'db9d9828-7962-41b5-bfa9-e54fb58f54f9'}, page_content='Image synthesis, widely used in art generation, photo editing, inpainting, and design, has garnered significant research attention, particularly following the development of Generative Adversarial Networks (GAN), which consist of a generator and a discriminator.'),\n",
       " Document(metadata={'doc_id': '7dd362ff-6df3-454a-8f3b-5893d7aa0a37'}, page_content='The two components are in competition.'),\n",
       " Document(metadata={'doc_id': 'f4a59616-64cf-4071-9b3f-83db7f43a399'}, page_content=\"The text describes the dynamic between a generator and a discriminator in Generative Adversarial Networks (GANs), where the generator creates images to deceive the discriminator, which distinguishes between real and synthetic images. Initially, the generator produces easily identifiable fake data, but over time it improves, creating more realistic images that challenge the discriminator's ability to differentiate. This training process results in the generation of high-quality, photorealistic images, significantly advancing the field of image synthesis and synthetic media over the past decade.\"),\n",
       " Document(metadata={'doc_id': '8fb7a479-6ea5-4ea9-a6a8-b098608fd707'}, page_content='The text is incomplete and does not provide sufficient information for summarization.'),\n",
       " Document(metadata={'doc_id': '41e35822-8e8e-408d-82ed-e78c5bd144f2'}, page_content='The document titled \"Imaging 2023, 9, 199\" likely contains information related to a study, review, or findings in the field of imaging, published in 2023. Specific details about the content such as methods, results, or conclusions are not provided here.'),\n",
       " Document(metadata={'doc_id': '2cb1efa4-7897-4c20-9490-d3c81bb2feca'}, page_content='The provided links lead to a journal article in the journal \"Journal of Imaging,\" specifically discussing a topic related to imaging research. Please provide the relevant text or a description of the table for summarization.'),\n",
       " Document(metadata={'doc_id': '94dbabd9-7ef4-465b-9737-0c3d2a8078ab'}, page_content='Imaging 2023 presents opportunities for artists and creative professionals to enhance their work, but also raises concerns about legitimacy and security risks associated with potential misuse by malicious users.'),\n",
       " Document(metadata={'doc_id': '31f3e96f-8df8-47e4-b1e6-d9f5f3c0e80c'}, page_content='Malicious users exploit the ability to create AI-generated images, posing challenges for the media forensic community in detecting such images and maintaining image integrity in investigations. This can lead to the planting of synthetic images at crime scenes or as false leads, and the creation of fake websites or social media profiles to spread misinformation and commit fraud.'),\n",
       " Document(metadata={'doc_id': '5138cc51-e275-4755-888e-0408f46a887b'}, page_content=\"The rapid advancement in image synthesis makes it difficult to differentiate between synthetic and real images, necessitating automated tools to detect GAN-generated content. Although high-quality, these synthetic images often contain explicit artifacts (pixel anomalies) and implicit artifacts (fingerprints from the generative model's architecture). Different GAN architectures leave unique signatures that are not easily transferable between models, complicating the creation of generalized detection tools. Previous research has utilized the Fourier domain to identify distinct features that differentiate GAN-generated images from real ones, as these features manifest as specific peaks in the Fourier spectrum, a result of up-sampling operations specific to each GAN architecture.\"),\n",
       " Document(metadata={'doc_id': '87eff1af-40db-45b7-9d53-c1d750298a46'}, page_content=\"The paper presents an extensive study on differentiating real images from GAN-generated images using a fine-tuned pre-trained CNN model. A newly collected dataset incorporates various image synthesis modalities and GAN architectures to enhance the model's generalization capability. The approach includes visualization of the model's focus during classification through Class Activation Maps (CAMs).\"),\n",
       " Document(metadata={'doc_id': '6c3ceb67-0f19-4d01-bf5a-a54067aadc12'}, page_content='The text or table segment is not provided, so the summary cannot be generated. Please provide the specific content for summarization.'),\n",
       " Document(metadata={'doc_id': '118cf565-3909-4d30-96a2-49b70dbd4059'}, page_content='The paper is organized into sections that follow the introduction.'),\n",
       " Document(metadata={'doc_id': '24ca3eeb-9b4f-4e56-a7ed-8994ab0e2537'}, page_content='Section 2 covers a brief introduction to related works.'),\n",
       " Document(metadata={'doc_id': '8e4963c8-1e01-402c-aba1-dbd5f7f99626'}, page_content='Data collection and methods are outlined in Section 3, experiments are conducted in Section 4, and results along with limitations are discussed in Section 5.'),\n",
       " Document(metadata={'doc_id': '0654503b-5e47-42e9-90bd-52189a6d42b6'}, page_content='The paper concludes with future work discussed in Section 6, showcasing examples of GAN-generated images that display various artifacts, including pixel anomalies, artificial fingerprints, and spectral artifacts.'),\n",
       " Document(metadata={'doc_id': '2c8fdb90-ffdc-4512-b275-7c7dd88ed544'}, page_content='Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': '98c9b66a-0eb3-4a38-8a79-39c2d28e4317'}, page_content='Image generation involves creating realistic high-quality images from sources such as other images, sketches, or text.'),\n",
       " Document(metadata={'doc_id': 'f331eb9d-f6ce-4545-9d52-02321ca1cbf5'}, page_content='Image synthesis in computer vision has seen significant research advancements over the past decade, particularly with the development of GANs, which produce highly realistic artificial images that can be difficult to differentiate from real ones.'),\n",
       " Document(metadata={'doc_id': 'e3489178-0790-4aaf-b73b-78c9d25bcd11'}, page_content='The digital media and content landscape can be enhanced, but it also carries risks related to authenticity, genuineness, and security.'),\n",
       " Document(metadata={'doc_id': 'f24f8f73-3431-414b-8119-9b2ee0892223'}, page_content='Image-to-image synthesis involves converting images from one domain to another by altering specific features while preserving content, such as transforming segmentation masks into color images.'),\n",
       " Document(metadata={'doc_id': '0d510273-988e-4f51-9752-a24c9f23ee57'}, page_content='Conditional GAN (cGAN) extends traditional GAN by introducing an additional input, such as class labels, for both the generator and discriminator, enabling the generation of synthetic data based on specific conditions.'),\n",
       " Document(metadata={'doc_id': '04c4548a-2720-4248-90d2-aa95c831f54b'}, page_content='OASIS utilizes a modified GAN architecture with a discriminator designed as a U-Net based semantic segmentation network, leveraging label maps as ground truth to produce images that align well with their semantic labels.'),\n",
       " Document(metadata={'doc_id': '041caf3a-e19e-40a3-b61f-aae77b76aa01'}, page_content=\"LabelMix regularization enhances the discriminator's focus on semantic differences in images, prompting a re-design of both the discriminator and the generator, which now synthesizes multi-modal data via 3D noise sampling.\"),\n",
       " Document(metadata={'doc_id': '644f1489-d0d3-4bbf-b7a4-45c326571a4a'}, page_content='CC-FPSE utilizes a conditional GAN for semantic image synthesis, allowing for partial or complete changes to images, thereby increasing the diversity of generated images through the use of semantic layouts.'),\n",
       " Document(metadata={'doc_id': '46d9fdd5-653c-4333-9251-77587afabdd0'}, page_content='The generator uses convolutional kernels based on semantic label maps to create feature maps.'),\n",
       " Document(metadata={'doc_id': 'd7837c99-cbe7-44d3-913a-d1c6a238d8f4'}, page_content='The feature pyramid semantic-embedding discriminator predicts real/fake scores and semantic alignment scores to improve the alignment of synthesized images with their label maps and enhance fine details.'),\n",
       " Document(metadata={'doc_id': 'd49c216b-2973-4314-b9a1-3b1158ec508e'}, page_content='SPADE is based on the pix2pix model and incorporates a spatially adaptive normalization layer with learned parameters to preserve semantic information, resulting in the production of photo-realistic images.'),\n",
       " Document(metadata={'doc_id': 'a22ed45c-8391-4d80-8c01-2ebdf867c47b'}, page_content='SPADE normalizes activations from the prior layer, preserving input information, unlike InstanceNorm which often loses semantic details in the generation process.'),\n",
       " Document(metadata={'doc_id': 'f1d297e3-7573-4e77-9022-cba48bbe1ea7'}, page_content='Taming-transformers merges transformers with CNNs in an encoder-decoder architecture to create a discrete spatial codebook. It utilizes VQ-GAN and perceptual loss for local realism while the transformer focuses on global image composition, allowing for high-resolution image generation.'),\n",
       " Document(metadata={'doc_id': '6fc5f546-4b3f-4391-88ec-a1b37aaad6b3'}, page_content='Sketch-to-Image Synthesis transforms simple sketches into photo-realistic images, enhancing basic structural details into visually rich, colored representations.'),\n",
       " Document(metadata={'doc_id': '65f663f4-603e-46de-a4b6-8f4a3dbdc970'}, page_content='S2I-DetectoRS, S2I-HTC, S2I-QueryInst, and S2I-MaskRCNN utilize a common framework with four main stages. The first stage employs a pre-trained instance segmentation model from the MS-COCO dataset, specifically fine-tuning DetectoRS, HTC, QueryInst, or Mask R-CNN using various edge maps to segment 92 classes from the dataset, followed by a semantic segmentation stage.'),\n",
       " Document(metadata={'doc_id': '170fac2a-4c89-4f61-a304-c34e62f6bddb'}, page_content='The process involves using a semantic mask map with an image-to-image translation model called SPADE to create a photo-realistic image, followed by a post-processing stage to improve the background and refine human faces.'),\n",
       " Document(metadata={'doc_id': '7fb25d90-9e2f-4c95-a438-7853497a1e6f'}, page_content='Text-to-image synthesis transforms natural language descriptions into realistic images, effectively condensing long sentences into single images. AttnGAN enhances this process by incorporating an attention module to generate detailed images, using word vectors and a sentence vector derived from the input text.'),\n",
       " Document(metadata={'doc_id': 'b95ee2e9-15a1-4a30-893f-b7a3ad27ffca'}, page_content='The generation process consists of two stages: first, a low-quality image is produced from a sentence vector, followed by multi-stage refinement focusing on specific areas based on related word vectors. ControlGAN employs an attention module for fine-tuning, utilizing word-level spatial and channel-wise attention to target subregions corresponding to relevant words. A word-level discriminator is also used to provide feedback, allowing the generator to modify specific visual attributes of subregions without affecting the overall image.'),\n",
       " Document(metadata={'doc_id': '9cd002b4-ea53-44b3-91c2-9180ffba33f4'}, page_content='DM-GAN+CL is a proposed method for text-to-image synthesis using contrastive learning to address discrepancies in image captions assigned by different annotators, aiming to generate images that are more consistent with the corresponding ground truth.'),\n",
       " Document(metadata={'doc_id': 'fd207971-1eff-4e05-a23a-a7b2d8e9ce03'}, page_content='Improves semantic consistency.'),\n",
       " Document(metadata={'doc_id': '4d319e51-101f-4612-b36e-8484b35e9852'}, page_content='The provided content is insufficient to summarize any specific table or text. Please provide more details or additional context.'),\n",
       " Document(metadata={'doc_id': '5e65d5b5-2fe6-4227-afa7-3eef7a31372b'}, page_content='DF-GAN is a one-stage model that generates high-quality images directly from text, using a Deep text-image Fusion Block (DFBlock) to improve semantic consistency. It incorporates a Matching-Aware Gradient Penalty (MA-GP) in the discriminator to promote the generation of realistic images.'),\n",
       " Document(metadata={'doc_id': 'c5a21ac0-2d4c-4652-a24f-d09acebd7b84'}, page_content='3.'),\n",
       " Document(metadata={'doc_id': 'ecbef491-a175-45e0-ac16-c414bcd16a04'}, page_content='The section introduces the Real or Synthetic Images (RSI) dataset used for training and testing, followed by a detailed discussion of the proposed framework.'),\n",
       " Document(metadata={'doc_id': 'fbbcb11a-e5f7-4a76-9de8-9950c25448b6'}, page_content='The proposed method for GAN-generated image recognition involves a framework illustrated in Figure 3, utilizing a CNN model with transfer learning from ImageNet and a new head for recognizing real and synthetic images across two classes. Data collection included images synthesized from 12 different models across three tasks: image-to-image, sketch-to-image, and text-to-image, leveraging the COCO-Stuff dataset for improved generalization and accuracy.'),\n",
       " Document(metadata={'doc_id': 'f7960682-6415-4a00-859a-81a91b388087'}, page_content='The COCO dataset has 164k images with instance-level annotations, augmented for pixel-level stuff annotations and split into 118k training, 5k validation, and 40k testing images. The COCO-Stuff dataset includes 172 classes: 80 thing classes, 91 stuff classes, and 1 unlabeled class, supporting various tasks like classification, detection, segmentation, scene understanding, captioning, and image generation.'),\n",
       " Document(metadata={'doc_id': '01fff2f7-6de6-486f-a049-f425bf106033'}, page_content='The document presents examples of synthesized images generated through various tasks, utilizing different input types and GAN models. It specifies a dataset comprising 24,000 images for training, 12,000 for validation, and 12,000 for testing, categorized by tasks such as image-to-image, sketch-to-image, and text-to-image synthesis.'),\n",
       " Document(metadata={'doc_id': '57f2d4b7-7b59-4e83-abe0-4cad3692290d'}, page_content='The image sets contained an equal number of real and synthetic images.'),\n",
       " Document(metadata={'doc_id': '8f9cf3fe-743a-48a4-a85d-51c3cca0f989'}, page_content='Table 1 contains additional details regarding the data collection process.'),\n",
       " Document(metadata={'doc_id': 'c6ac5502-2d2e-4189-a213-512c988ed24b'}, page_content='No information provided for summarization.'),\n",
       " Document(metadata={'doc_id': '98971134-0944-49af-bd5f-0eafe7ce8164'}, page_content='The dataset includes 48,000 total images divided into training, validation, and testing sets (24,000 for training and 12,000 each for validation and testing) across various tasks: image-to-image synthesis (OASIS, CC-FPSE, SPADE, Taming-transformers, $2I-DetectoRS, S2I-HTC, $2I-QueryInst, S2I-MaskRCNN, AttnGAN), and text-to-image synthesis (DM-GAN+CL, DF-GAN, ControlGAN). Image collection relied on the COCO-Stuff dataset, with diverse inputs generated, including text, sketches, and semantic segmentation masks for respective synthesis tasks. Sketches were created using a dodging and burning algorithm to ensure structural fidelity.'),\n",
       " Document(metadata={'doc_id': '14688959-e882-4660-9e2f-a9f1a6150146'}, page_content='The study involved 12 image synthesis models that were tested using a pre-trained model on generated inputs to create synthetic images. Both real and synthetic images were resized to a resolution of 256 x 256, irrespective of the original resolutions of the synthesis models.'),\n",
       " Document(metadata={'doc_id': '3020d889-412a-471b-8e4c-193844facf8c'}, page_content='The methodology involves developing an automated tool to reliably and promptly identify GAN-generated images by fine-tuning a pre-trained classifier on the newly collected RSI dataset.'),\n",
       " Document(metadata={'doc_id': '2b6a92ca-9cd4-4195-bce8-1831be3ab453'}, page_content='Training a classifier from scratch is time-consuming, costly, and less efficient in classification performance.'),\n",
       " Document(metadata={'doc_id': 'c9c21513-0b3e-478b-805e-3b2a348aa692'}, page_content='Transfer learning is considered a more effective approach.'),\n",
       " Document(metadata={'doc_id': 'afd521c2-48a7-44fc-bda0-0feca1a317af'}, page_content='The training process involved binary classification to differentiate between real and synthetic images using various classifiers, aiming to find the most effective model for detecting and localizing GAN-generated images.'),\n",
       " Document(metadata={'doc_id': 'cef02fc3-2291-4b54-8157-54a44286bd21'}, page_content='The dataset was fine-tuned on various models including VGG19, ResNet versions (50, 101, 152), InceptionV3, Xception, DenseNet121, InceptionResNetV2, MixConv, MaxViT, and EfficientNetB4. The original head of classifiers pre-trained on ImageNet was replaced with a new head that includes global average pooling, a ReLU dense layer, batch normalization, a dropout layer, and a final dense layer with sigmoid activation. Training utilized a batch size of 64 and an initial learning rate of 0.001.'),\n",
       " Document(metadata={'doc_id': '62392195-fe5f-405d-84fb-644d7dc510d6'}, page_content='The training involved 20 epochs with checkpoints saved for improvements in validation loss, allowing the best model to be loaded during testing. The learning rate was automatically reduced when validation loss plateaued, and data augmentation via horizontal flipping was applied to the training set. Adam was used as the optimizer for all classifiers except ResNet101, which utilized RMSprop.'),\n",
       " Document(metadata={'doc_id': '6989a159-6cdb-4790-a130-61d75420fc78'}, page_content='The input does not contain sufficient information to provide a summary. Please provide the relevant table or text for summarization.'),\n",
       " Document(metadata={'doc_id': '49c61d07-d320-4d3a-951d-e5fa2d06fece'}, page_content='The section presents evaluation metrics used to assess the performance of methods for detecting and localizing GAN-generated images among real ones. Eight metrics were employed: Precision, Recall, F1 score, Accuracy, Average Precision (AP), Area Under Curve of Receiver Operating Characteristic (ROC-AUC), False Positive Rate (FPR), and False Negative Rate (FNR). A performance evaluation was conducted on a dataset (RSI) of 12,000 images, equally divided between GAN-generated and real images, to validate the methods proposed. An ablation study further examined the effectiveness of the approach across different image synthesis modalities and GAN models.'),\n",
       " Document(metadata={'doc_id': 'ac0d47d4-295e-4304-9b10-e2cff99c4840'}, page_content='EfficientNetB4 demonstrated the highest performance in recognizing and localizing GAN-generated images on the RSI dataset, achieving 100% accuracy, while InceptionV3 followed with 98% accuracy.'),\n",
       " Document(metadata={'doc_id': '4e7060b5-3e01-496c-8b58-5f7f68b7db38'}, page_content='The table provides performance metrics for various classifiers on a testing set, including Precision, Recall, Accuracy, Average Precision (AP), ROC-AUC, False Positive Rate (FPR), and False Negative Rate (FNR). Notably, EfficientNetB4 achieved the highest scores across all metrics with perfect 1.00 values, while InceptionV3 and Xception also demonstrated strong performance with values close to 1. Other models like ResNet101 and DenseNet121 also performed well, but had slightly lower metrics. Additionally, the text mentions the use of Class Activation Maps (CAM) for visualizing model focus during the classification of real and GAN-generated images, confirming 100% accuracy with the best model.'),\n",
       " Document(metadata={'doc_id': 'ff16d149-44ce-49ea-8535-101c77612944'}, page_content=\"GradCAM, AblationCAM, LayerCAM, and Faster ScoreCAM were used to analyze images and determine their contributions to the model's classification decisions, as illustrated in Figure 6.\"),\n",
       " Document(metadata={'doc_id': '50e23cfa-5c6f-422e-99f8-0c1335459a15'}, page_content='The content references various image analysis techniques, specifically GradCAM, AblationCAM, LayerCAM, and Faster ScoreCAM, as outlined in a study in J. Imaging 2023.'),\n",
       " Document(metadata={'doc_id': '35e66453-3d14-45e9-aecd-f9d90020066c'}, page_content='The EfficientNetB4 model primarily focused on the background of GAN-generated images during the visualization of CAM variants on the testing set.'),\n",
       " Document(metadata={'doc_id': '4317d53f-4545-4697-a793-7e169176e36f'}, page_content='The analysis identified regions in the synthetic images that exhibited distortion and anomalies.'),\n",
       " Document(metadata={'doc_id': 'fe5a885c-91aa-40d2-b2ba-02d196c9c5c5'}, page_content=\"The method's predictions were explained using the LIME explanation technique, illustrated in Figure 7.\"),\n",
       " Document(metadata={'doc_id': 'f56b02c2-751d-40f1-8eaa-44f640798acb'}, page_content='The table illustrates the visualization of the LIME interpretability technique for distinguishing between GAN-generated synthetic images and real images, showcasing super-pixel and region-based explanations for both image types.'),\n",
       " Document(metadata={'doc_id': 'a7046837-5663-4905-9c85-6625a0aca523'}, page_content='LIME explanations are provided for GAN-generated and real images, illustrating significant super-pixels contributing to classification. Green super-pixels increase the probability of a label, while red super-pixels decrease it.'),\n",
       " Document(metadata={'doc_id': '457b1477-39e1-41ae-878c-548953c31776'}, page_content='The effectiveness of the model in recognizing GAN-generated images was tested through an experiment where three models were trained separately based on different input modalities.'),\n",
       " Document(metadata={'doc_id': 'ca98f48f-038e-4dd4-b407-a573ae664c46'}, page_content='Three experiments were conducted using different model training and testing combinations: (1) trained on sketch-to-image and text-to-image (S2I_T2I) and tested on image-to-image; (2) trained on image-to-image and text-to-image (I21_T2I) and tested on sketch-to-image; (3) trained on image-to-image and sketch-to-image (I2I_S2I) and tested on text-to-image. Results are detailed in Table 3.'),\n",
       " Document(metadata={'doc_id': '2e13c086-cf09-4f32-a76d-24800152ed0c'}, page_content='The S2I_T2I model achieved the highest accuracy of 0.99, followed by I2I_S2I at 0.95, while the I2I_T2I model had lower accuracy at 0.83 due to the image enhancement step used.'),\n",
       " Document(metadata={'doc_id': '4e440abc-1384-4823-9f18-63dfe0428114'}, page_content=\"The GAN detection model achieved a high accuracy of 0.83 in identifying GAN-generated images. Its performance metrics included precision of 0.99, recall of 0.99, and F1 score of 0.99 for the S21_T2I modality. The I21_T2I modality showed slightly lower metrics with a precision of 0.87 and recall of 0.83. The I21_S21 modality demonstrated strong performance with a precision of 0.96 and recall of 0.95. The model effectively detected GAN-generated images even when not specifically trained on the evaluated modality. An ablation study was conducted to further validate the method's effectiveness.\"),\n",
       " Document(metadata={'doc_id': 'b1b35637-8acf-4694-bb58-d457beafb898'}, page_content='The second-best model, InceptionV3, was evaluated on the testing set of image synthesis models individually because the best model, EfficientNetB4, achieved 100% accuracy, leaving no performance differences when part of the dataset was assessed.'),\n",
       " Document(metadata={'doc_id': '467067a8-83b9-41e4-8d7e-b8982dfba328'}, page_content=\"The second-best model achieved the highest accuracy of 98.35% with GAN-generated images from text inputs, while the lowest accuracy of 96.90% came from images based on semantic segmentation maps. Performance evaluations in Table 5 indicate that detecting GAN-generated images from semantic segmentation is more challenging compared to those produced from text descriptions. The confusion matrices in Figure 8 illustrate the model's accuracy across image-to-image, sketch-to-image, and text-to-image synthesis inputs. Overall, image-to-image synthesis yields more photo-realistic results than text-based generation.\"),\n",
       " Document(metadata={'doc_id': '4a8cae5f-5950-4a99-9c38-c1bd7195ce76'}, page_content='The table presents the performance of the InceptionV3 model on GAN-generated images, evaluating various models based on metrics such as Precision, Recall, F1 Score, Accuracy, Average Precision (AP), Area Under Curve (AUC), False Positive Rate (FPR), and False Negative Rate (FNR). Most models achieve high scores, with notable performances including OASIS (Accuracy: 0.964), CC-FPSE and SPADE (Accuracy: 0.978), and several models achieving an Accuracy of 0.980 or higher, indicating strong overall effectiveness in classifying GAN-generated images.'),\n",
       " Document(metadata={'doc_id': 'dea5a7c8-558f-467b-b115-98f1fad6f6cd'}, page_content='The model was tested on additional datasets to evaluate its effectiveness in distinguishing generated images from real ones.'),\n",
       " Document(metadata={'doc_id': 'c9b61106-1cfb-466b-b70d-0c785e72d0eb'}, page_content='The first model from each modality listed in Table 1 was chosen for the experiment.'),\n",
       " Document(metadata={'doc_id': '705040b7-d8f6-4ddb-b70b-a93f59305251'}, page_content='OASIS, S2I-DetectoRS, and AttnGAN were utilized as models for image-to-image, sketch-to-image, and text-to-image synthesis, respectively, using unrelated datasets to generate synthetic images.'),\n",
       " Document(metadata={'doc_id': '31d6547d-0061-4b7c-8064-6c0e4f04b71e'}, page_content=\"The study utilized the ADE20K, Sketchy, and Caltech-UCSD Birds-200-2011 datasets and employed EfficientNetB4 to differentiate between synthetic and real images, evaluating the model's performance with eight common metrics.\"),\n",
       " Document(metadata={'doc_id': '9607ca76-4041-49f7-9b11-6dfa59543461'}, page_content='Key performance metrics adopted include Precision, Recall, F score, Accuracy, Average Precision (AP), ROC-AUC, False Positive Rate (FPR), and False Negative Rate (FNR).'),\n",
       " Document(metadata={'doc_id': 'e955d107-3f05-481f-b3ab-402f02a46e2f'}, page_content=\"The OASIS image-to-image synthesis model was tested using the ADE20K dataset's 2000 semantic segmentation images, producing synthesized images for evaluation. The S2I-DetectoRS model utilized a subset of 2470 sketches from the Sketchy dataset, aligned with 35 classes from the MS COCO dataset, for sketch-to-image synthesis.\"),\n",
       " Document(metadata={'doc_id': 'de51fc47-187e-47bd-9316-f7f68315a618'}, page_content='Sketches were transformed into colored images during the generation phase.'),\n",
       " Document(metadata={'doc_id': '0ba471f9-f893-49a2-aa13-cab01bd16b27'}, page_content='Both generated and real images were evaluated using eight metrics with the best model, EfficientNetB4. The procedure mirrored that of the AttnGAN model, utilizing the CUB-200-2011 dataset, which comprises 5,794 captions/descriptions for testing.'),\n",
       " Document(metadata={'doc_id': '8a94b779-b8d2-4f2e-8a91-f906d50335c2'}, page_content='Performance was evaluated using adopted metrics in the final step.'),\n",
       " Document(metadata={'doc_id': '574cdc26-2eed-465e-a7cc-f93a52622f84'}, page_content='The experimental results indicate that the model successfully detected and recognized GAN-generated images from real ones, achieving a maximum accuracy of 98% with the CUB-200-2011 dataset and the AttnGAN, while reaching 89% accuracy with the ADE20K dataset and the OASIS model.'),\n",
       " Document(metadata={'doc_id': '98fc3f5c-51db-4f7d-beb1-0c10cede1c70'}, page_content=\"The dataset's complexity and the generator's capability may contribute to the observed outcomes.\"),\n",
       " Document(metadata={'doc_id': 'd0c471c6-b45d-4606-a1e0-b8b53a0b9f4c'}, page_content='The model serves as an evaluation tool applicable to various input modalities.'),\n",
       " Document(metadata={'doc_id': 'ff0dfb92-4003-41d8-9728-6133c95c9068'}, page_content='The table presents the performance metrics of the best model across various datasets that were not included in the training phase, highlighting its generalization capabilities.'),\n",
       " Document(metadata={'doc_id': '16c23ca7-e652-4f9b-8f35-4ddd304e413f'}, page_content='The table presents performance metrics of different models used for detecting GAN-generated images across various datasets. Performance measures include Precision, Recall, F1 Score, Accuracy, Average Precision (AP), Area Under Curve (AUC), and False Positive Rate (FPR). The models evaluated are OASIS on ADE20K, S21 on Sketchy, and AttnGAN on CUB-200-2011, showing high performance with accuracy scores ranging from 0.889 to 0.978. The discussion notes limitations and emphasizes the evaluation of fine-tuned classifiers on a new dataset.'),\n",
       " Document(metadata={'doc_id': 'fa0a6489-3949-4d2f-a64f-6943dd3d9c73'}, page_content='Imaging 2023 discusses synthetic images (RSI), focusing on their relevance and applications.'),\n",
       " Document(metadata={'doc_id': '948384e7-2c8a-4994-8ece-bdf615fc28b9'}, page_content='The evaluation process utilized a testing set of 12,000 RSI images.'),\n",
       " Document(metadata={'doc_id': 'a837e0a0-ba27-4746-b156-ab94d185fbee'}, page_content='EfficientNetB4 recorded the highest accuracy of 100% on the evaluated dataset, demonstrating its effectiveness in detecting GAN-generated images. A subsequent experiment involved fine-tuning and re-training EfficientNetB4 using two input modalities from the RSI training set.'),\n",
       " Document(metadata={'doc_id': '7d281b82-de48-42ad-80fe-9d06cda2c356'}, page_content='The modality-based trained model was evaluated using the third excluded input modality from the RSI testing set.'),\n",
       " Document(metadata={'doc_id': '22509512-1bb7-484f-908d-0e16ec86e2e1'}, page_content='The model achieved accuracy rates of 99%, 83%, and 95% when excluding I2I, S2I, and T2I modalities, respectively. The dip to 83% accuracy upon excluding the S2I modality is linked to background improvement and face refinement processes in sketch-to-image models, which enhance image quality by aligning synthetic backgrounds with real, high-quality ones and reconstructing faces.'),\n",
       " Document(metadata={'doc_id': '014ccb9b-db38-4398-8e7c-e98d509eaeb7'}, page_content='Post-processing steps enhance the quality of synthetic images.'),\n",
       " Document(metadata={'doc_id': 'e87cb12f-9f61-4978-b332-f27adf5fba24'}, page_content='The model demonstrated an 83% accuracy in recognizing GAN-generated images even without prior training on the S2I modality and successfully identified generated images from different datasets, indicating high detection capability.'),\n",
       " Document(metadata={'doc_id': 'f7a4c4a1-61bf-4ee7-9f9d-a54d84a762a4'}, page_content='The model is applicable for detecting synthetic images, aiding forensic investigations, and combating AI misuse and cyberattacks. It utilizes Class Activation Maps (CAM) to identify influential image regions for classification, primarily focusing on the background of generated images.'),\n",
       " Document(metadata={'doc_id': 'b18f9727-436b-4894-b35c-abb714945ded'}, page_content='Generators tend to focus more on the foreground than the background, resulting in artifacts during background generation. Occasionally, the model incorporates aspects of the foreground due to anomalies and distortions, leading to classification failures despite high accuracy overall.'),\n",
       " Document(metadata={'doc_id': 'd94cb43b-d92f-41ea-8f8b-ccef294a50b7'}, page_content='The model struggles to classify images accurately in cases of blurriness, low quality, vintage appearance, or motion. It also misclassifies GAN-generated images as fake when fine details and sharper textures are present. To improve accuracy, these misclassification factors should be incorporated into the training dataset.'),\n",
       " Document(metadata={'doc_id': 'f9db8adb-e356-4942-ab90-e4ad34b851e1'}, page_content='The classifier improves its ability to classify images by learning about various aspects.'),\n",
       " Document(metadata={'doc_id': '96f279d5-d129-4093-875f-38432a7057a6'}, page_content='The content provided is insufficient for summarization.'),\n",
       " Document(metadata={'doc_id': '0e923eb1-848d-45c0-b695-205b56b050da'}, page_content='The table presents probabilities of \"Real\" and \"Fake\" classifications across different conditions (GT, P Real, P Fake, T2I). Key values include P(Real) for various categories, with notable percentages for Real as follows: GT (63.91), P Real (73.69), and T2I (99.98), while P(Fake) values are 36.09 for GT and 10.72 for P Fake. Lower probabilities are also noted for auxiliary real classifications, with figures like 0.03 and 0.82.'),\n",
       " Document(metadata={'doc_id': 'f0e62169-4dc4-4e94-8ad6-1c0767b212e4'}, page_content='The table presents misclassification probabilities from a model on different datasets (ADE20K, Sketchy, CUB-200-2011), with a high proportion of predictions classified as fake (ranging from 76.92% to 99.97%) and a low probability of real predictions (4.11%). Ground truth is denoted as GT, while misclassified predictions are highlighted in red as P.'),\n",
       " Document(metadata={'doc_id': 'bb249605-a3b3-4e9b-ada5-c7f5db6287a1'}, page_content='The rapid advancement of AI-based image synthesis is making generated images increasingly photo-realistic, which can enhance media but also raises security and authenticity concerns. Consequently, there is a need for automated tools to detect and localize these AI-generated images.'),\n",
       " Document(metadata={'doc_id': '5e797259-d3be-4d9b-b688-21fea5c9f9d2'}, page_content='The paper presents a machine model for distinguishing AI-generated images from real ones, utilizing a large dataset (RSI) derived from various GAN-based tasks. A CNN model was effectively trained on this dataset, achieving high accuracy in detecting synthetic images, even when tested on unfamiliar datasets. The study highlights that GAN-generated images exhibit specific flaws and artifacts, undetectable by humans but identifiable by the trained model. This capability aids the multimedia forensics community in countering potential threats arising from advancements in AI image synthesis.'),\n",
       " Document(metadata={'doc_id': '192f9010-2b32-4823-8880-8b384f6cb1e2'}, page_content='Researchers should focus on minimizing traces during image generation to produce high-quality, photo-realistic images. Future plans include integrating various image synthesis tasks across different architectures and datasets, such as facial and biological images like Western blot and microscopic images.'),\n",
       " Document(metadata={'doc_id': 'f59878f6-1f42-4056-8838-5232963c3fb9'}, page_content='The project aims to combine the frequency and spatial domains of a dataset and improve a GAN-generated image detector with techniques to counter adversarial attacks that could deceive the detector. Contributions include conceptualization and methodology by S.S.B.'),\n",
       " Document(metadata={'doc_id': 'cfdaca4a-6e94-4d1f-9f33-27b6eaee66ce'}, page_content='Roles in the project include software development (S.S.B.), data curation (S.S.B.), validation (S.S.B.), formal analysis (S.S.B.), investigation (S.S.B. and T.V.N.), and original draft writing (S.S.B.).'),\n",
       " Document(metadata={'doc_id': '5365ecb0-14f6-43cf-ac97-4e37b139783c'}, page_content='T.V.N. is responsible for writing, reviewing, and editing, while S.S.B. is also involved in the process.'),\n",
       " Document(metadata={'doc_id': 'f10ef3d6-3c61-4a35-8b3e-6eb095689422'}, page_content='All authors contributed to the visualization and supervision of the manuscript and have agreed to the published version.'),\n",
       " Document(metadata={'doc_id': '28015b02-f830-47c8-9205-908c30d9646a'}, page_content='The article from J. Imaging, 2023, indicates that Open Access funding is provided by the University of Dayton Libraries, with the second author receiving support from NSF grant # 2025234.'),\n",
       " Document(metadata={'doc_id': '5cc02e3d-1d3d-495c-a672-504b7dd249e4'}, page_content='No Institutional Review Board statement is applicable.'),\n",
       " Document(metadata={'doc_id': '6a7814c7-91a1-4560-a3d8-21257a573d79'}, page_content='Informed consent statement is not applicable.'),\n",
       " Document(metadata={'doc_id': '55cb2b48-97b7-4fa9-8ccc-6030cabc346c'}, page_content='Datasets from the study are available from the corresponding author upon reasonable request. The first author acknowledges support from Umm Al-Qura University in Saudi Arabia.'),\n",
       " Document(metadata={'doc_id': 'ef5ef7dc-6383-4738-ac0d-db40e918f69c'}, page_content=\"The University of Dayton's Office for Graduate Academic Affairs partially funded this work through a Graduate Student Summer Fellowship.\"),\n",
       " Document(metadata={'doc_id': '6618bffe-8eff-41f3-b79e-83fedc9d19ba'}, page_content='The authors report no conflicts of interest.'),\n",
       " Document(metadata={'doc_id': '34c2c41c-d6df-48fe-bec1-01c571ce9bf0'}, page_content='No content provided for summarization.'),\n",
       " Document(metadata={'doc_id': '73928722-9d33-467d-9b62-d5c0545ff412'}, page_content='The article by Baraheem, Le, and Nguyen reviews various methods for image synthesis, discusses relevant datasets and evaluation metrics, and provides a future outlook on the field.'),\n",
       " Document(metadata={'doc_id': '34b4c544-8fd3-42d9-afc3-1904fa2ee775'}, page_content='The text or table regarding \"Intell\" is not provided. Please provide the relevant content for summarization.'),\n",
       " Document(metadata={'doc_id': '6b378545-4e2f-4075-91bf-4c0c02ffcfb9'}, page_content='The input does not provide sufficient information to summarize.'),\n",
       " Document(metadata={'doc_id': 'fca33b4f-7e83-4f75-b981-17463fa23338'}, page_content='The data pertains to the year 2023 and includes a numerical range from 56 to 10865.'),\n",
       " Document(metadata={'doc_id': '6a44f643-191e-41c4-aa67-b5d1f5d2d9fa'}, page_content='The provided text does not contain a table or specific content to summarize.'),\n",
       " Document(metadata={'doc_id': 'e3f70f76-545d-4159-ae8f-dea08ffa6c80'}, page_content='The referenced paper discusses CAN (Creative Adversarial Networks), which generate art by understanding various artistic styles and deliberately deviating from established style norms.'),\n",
       " Document(metadata={'doc_id': '191019a8-7440-432f-9075-249d4338b3bf'}, page_content='3.'),\n",
       " Document(metadata={'doc_id': '7ed01fe8-e17e-4e18-94a3-e936eb936e39'}, page_content='The referenced work presents a method for image editing that utilizes recurrent attentive models to enhance language-based interactions with images.'),\n",
       " Document(metadata={'doc_id': 'a71a704c-8d61-4b57-8051-f2ad20398a12'}, page_content='The paper by Yan et al. (2014) discusses a method for automatic photo adjustment utilizing deep neural networks, as detailed in their arXiv publication.'),\n",
       " Document(metadata={'doc_id': '533c4b0a-d170-44bc-9bce-bd1c2148ce9a'}, page_content='The request lacks the necessary information to provide a summary. Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': 'd4b5664e-eb54-43e5-8f64-9d28a4538339'}, page_content='The paper by Yu et al. (2018) presents a method for generative image inpainting using contextual attention, which improves the quality of image restoration by focusing on relevant contextual information during the filling process.'),\n",
       " Document(metadata={'doc_id': '6cbbabeb-426a-4bb4-b8c6-dbe79cbddcac'}, page_content='The provided information is insufficient to summarize. Please provide more context or details.'),\n",
       " Document(metadata={'doc_id': '93f4eb32-eeeb-45c7-ba00-3885048455f9'}, page_content='6.'),\n",
       " Document(metadata={'doc_id': '557eec05-33ac-4e2f-803f-f6e8b4faaf47'}, page_content='Nazeri et al. introduced EdgeConnect, a method for generative image inpainting that utilizes adversarial edge learning, presented in their 2019 paper on arXiv.'),\n",
       " Document(metadata={'doc_id': '9e603e88-a4b0-42a1-84e0-40e1823a2f9a'}, page_content='Insufficient information provided to summarize.'),\n",
       " Document(metadata={'doc_id': '0dc91c0f-4834-4a2d-b333-f977393637a9'}, page_content='The dissertation by Thaung (2020) explores advanced data augmentation techniques using Generative Adversarial Networks (GANs) and Computer-Aided Design (CAD).'),\n",
       " Document(metadata={'doc_id': 'eac4f3b8-27b3-44ad-9217-ce4967318a80'}, page_content='The link provided leads to a record on the DiVA portal, accessed on April 1, 2022.'),\n",
       " Document(metadata={'doc_id': 'b1f19d94-8ecd-4560-a859-f9116b542bbc'}, page_content='The cited work is a conference paper titled \"Generative adversarial nets\" by Goodfellow et al., presented at the 27th International Conference on Neural Information Processing Systems in Montreal, Canada, from December 8-13, 2014.'),\n",
       " Document(metadata={'doc_id': '23b60861-abdd-4673-8924-6cb7e32996cd'}, page_content='The data from lines 2672 to 2680 appears to focus on specific cases or instances, likely detailing outcomes or findings related to various subjects. Each entry summarizes particular events, characteristics, or outcomes, providing a snapshot of the larger context in which these cases exist.'),\n",
       " Document(metadata={'doc_id': 'abd503b2-3b9e-4b38-9581-ef79c1d55e10'}, page_content='The content provided does not include sufficient information to summarize.'),\n",
       " Document(metadata={'doc_id': '22b63b54-a975-4c15-b515-02a6cd0f0942'}, page_content='The text appears to list the authors: Marra, F., Gragnaniello, D., Verdoliva, L., and Poggi, G.'),\n",
       " Document(metadata={'doc_id': '4d567729-6380-4058-a09c-a44ba3134d5f'}, page_content='The paper discusses whether Generative Adversarial Networks (GANs) produce identifiable patterns or \"fingerprints\" in generated content, presented at the 2019 IEEE Conference on Multimedia Information Processing and Retrieval.'),\n",
       " Document(metadata={'doc_id': '6eb5d52e-da6c-4954-b0dd-380b8a2f94ef'}, page_content=\"The provided text or table from 506-511 is missing, so I'm unable to summarize it. Please provide the relevant information for summarization.\"),\n",
       " Document(metadata={'doc_id': 'b79deaa9-f391-4c68-864b-4eab02312a78'}, page_content='The provided content is insufficient to summarize. Please provide a complete table or text for summarization.'),\n",
       " Document(metadata={'doc_id': '4cb57c1d-72de-4e8a-bdef-a1b93953774b'}, page_content='The work by Yu, Davis, and Fritz focuses on identifying and analyzing fingerprints of Generative Adversarial Networks (GANs) to attribute fake images to specific GAN models.'),\n",
       " Document(metadata={'doc_id': '47820ddc-8ad5-4e11-9eb6-95aabd7e40da'}, page_content='International Conference on Computer Vision occurred in Seoul, South Korea, from 27 October to 2 November 2019.'),\n",
       " Document(metadata={'doc_id': '5a447ad0-d54a-4a91-b660-eb1530248393'}, page_content='The document discusses a study presented at the 2019 IEEE International Workshop on Information Forensics and Security, focusing on detecting and simulating artifacts in images generated by Generative Adversarial Networks (GANs).'),\n",
       " Document(metadata={'doc_id': 'f1e013b0-b234-486d-a95d-9287841ef507'}, page_content='Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': '61d94bc8-7647-446c-aa64-ca4f7adee060'}, page_content='The paper by Dzanic et al. discusses inconsistencies in the Fourier spectrum of images generated by deep networks, highlighting potential issues in the fidelity of these generated images.'),\n",
       " Document(metadata={'doc_id': '470c3148-0eae-41db-a44a-a7c2b702a18b'}, page_content='12.'),\n",
       " Document(metadata={'doc_id': 'a9963d4e-6250-41d5-8683-b977dc19713b'}, page_content='The paper discusses utilizing frequency analysis techniques to improve the recognition of deep fake images, presented at the 37th International Conference on Machine Learning in July 2020.'),\n",
       " Document(metadata={'doc_id': 'c79164d0-eff9-45f8-8e81-817485d03ad0'}, page_content='13.'),\n",
       " Document(metadata={'doc_id': 'de38c0ff-1052-4f2c-8327-e0523c58c2d7'}, page_content='The paper by Sushko et al. discusses the necessity of adversarial supervision in the context of semantic image synthesis, presented in an arXiv paper published in 2020.'),\n",
       " Document(metadata={'doc_id': 'fdddccda-e31a-4b55-bd94-76e91e2a3c20'}, page_content='14 is a significant number that often represents various concepts in mathematics and culture, including its role in counting, its representation as an integer, and its appearances in historical and cultural contexts.'),\n",
       " Document(metadata={'doc_id': '1c0b4140-7bff-4a1a-86e6-11789f63276e'}, page_content='The paper by Liu et al. (2019) focuses on developing a method for semantic image synthesis by predicting layout-to-image conditional convolutions.'),\n",
       " Document(metadata={'doc_id': 'ca233a5f-b3fb-4c4c-8c5f-d7c705b3a937'}, page_content='The content provided is insufficient for summarization.'),\n",
       " Document(metadata={'doc_id': 'ac12ef97-f813-4754-8cc6-53f08bec746b'}, page_content=\"The provided chunk, labeled '16,' does not include any contextual information or data to summarize.\"),\n",
       " Document(metadata={'doc_id': '49a6d51f-2c0d-48f3-ad56-06cd9afc5b7c'}, page_content='The paper by Park et al. discusses semantic image synthesis using a method called spatially-adaptive normalization, presented at the 2019 CVPR conference held in Long Beach, CA.'),\n",
       " Document(metadata={'doc_id': '8c1262d7-cba1-454c-b33b-0a77fcdc5d83'}, page_content='The work by Esser, Rombach, and Ommer presents techniques for utilizing transformers in high-resolution image synthesis, showcased at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) held in Nashville, TN.'),\n",
       " Document(metadata={'doc_id': '9a480b75-612d-42e9-9a69-798e6485edd2'}, page_content='Please provide the content you would like summarized.'),\n",
       " Document(metadata={'doc_id': 'f5cb1cc4-0e23-4b4a-b393-965ac0710ee8'}, page_content='The work by Baraheem and Nguyen discusses sketch-to-image synthesis using semantic masks, as presented in a publication by Springer in 2023.'),\n",
       " Document(metadata={'doc_id': 'a9856de4-d6dc-40cd-8999-6a23838fd1d6'}, page_content='1-20 consists of a set of data points or information that needs to be summarized into key insights. The focus would be on extracting essential details without added commentary or context for clarity.'),\n",
       " Document(metadata={'doc_id': '2bb7c3f3-4448-451e-b7c6-cb083e52b6bc'}, page_content='No content provided to summarize.'),\n",
       " Document(metadata={'doc_id': 'f3c0e95d-5770-4740-b884-ce1851b1528b'}, page_content='The paper presents AttnGAN, a novel approach for fine-grained text to image generation using attentional generative adversarial networks, introduced by Xu et al. in 2017.'),\n",
       " Document(metadata={'doc_id': '82309ef3-c675-47e0-b97e-db844ab6d15b'}, page_content='19 is a number that typically represents a quantity, age, or label, depending on context. It can signify various concepts in different fields, such as mathematics, legal age, or historical events.'),\n",
       " Document(metadata={'doc_id': 'da039450-5dcb-4d2a-bf1a-fbab8abc1171'}, page_content='20'),\n",
       " Document(metadata={'doc_id': 'd0efad41-247f-4921-8a5f-85d1f2aadfd8'}, page_content='The work by Ye et al. (2021) focuses on enhancing text-to-image synthesis through the application of contrastive learning techniques.'),\n",
       " Document(metadata={'doc_id': 'f9808e80-4214-48c3-ba44-3e2b307529c0'}, page_content='The content refers to a specific item labeled \"21,\" but no additional details or context are provided to summarize.'),\n",
       " Document(metadata={'doc_id': 'fada5d42-bd6f-49d9-879d-79a874f09b78'}, page_content='The paper presented by Tao et al. introduces DF-GAN, a framework for text-to-image synthesis that utilizes deep fusion generative adversarial networks. It was discussed at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in June 2022.'),\n",
       " Document(metadata={'doc_id': '6c0289fb-688c-4180-b3d2-dff925f5e325'}, page_content='The table spans entries numbered 16515 to 16525, detailing various data points such as identifiers, values, and associated metadata, relevant to a specific dataset or study. Each entry contains columns for distinct variables, displaying a range of information in a structured format.'),\n",
       " Document(metadata={'doc_id': '337a8c50-9623-4be1-b031-747f85382edd'}, page_content='The provided text references \"CrossRef\" and includes an incomplete citation, indicating partial information with no discernible table or data to summarize.'),\n",
       " Document(metadata={'doc_id': 'b8b361c6-6384-434c-b116-61abc53cf2ca'}, page_content='The provided text references a publication or article from \"Imaging 2023\" with the identification number 9,199, indicating a specific study or topic within that journal.'),\n",
       " Document(metadata={'doc_id': 'a3b17ae2-e72d-486f-b8c4-7453537b3d40'}, page_content='The paper discusses a method for controllable text-to-image generation, presenting an approach that allows for enhanced manipulation and fine-tuning of generated images based on textual descriptions.'),\n",
       " Document(metadata={'doc_id': 'e5a5b41a-8042-41ae-85e7-5bd104ccbc5e'}, page_content='The information is insufficient to provide a summary. Please provide additional context or details.'),\n",
       " Document(metadata={'doc_id': '357c17b0-7e92-4664-8cb7-cb946daeabb7'}, page_content='The cited work presents Conditional Generative Adversarial Nets (CGANs), which extend traditional GANs by conditioning the generation process on additional information, allowing for more controlled and meaningful image synthesis.'),\n",
       " Document(metadata={'doc_id': 'fba4e819-a64f-4d87-8a91-d54d69dd4a77'}, page_content='The paper by Ronneberger, Fischer, and Brox presents U-Net, a convolutional neural network architecture specifically designed for biomedical image segmentation, and was published on arXiv in 2015.'),\n",
       " Document(metadata={'doc_id': 'cb831cb5-035d-4b88-940c-b7223c28f5d2'}, page_content='The number 24 is a whole number that follows 23 and precedes 25. It is an even number and can be expressed as a product of prime factors: 2^3 × 3. In various contexts, 24 may represent a quantity, a time duration, or an identifier.'),\n",
       " Document(metadata={'doc_id': '90accffe-69cf-4d1b-9e4d-cf89d1cff3a8'}, page_content='The paper discusses the use of conditional adversarial networks for image-to-image translation, presenting a method that pairs input images with desired output images to achieve versatile transformations across various applications.'),\n",
       " Document(metadata={'doc_id': 'b3759256-2ff9-46bb-a4f0-24a248baacbf'}, page_content='25.'),\n",
       " Document(metadata={'doc_id': '7813c5f6-1230-40a3-bb37-a50a33d5c919'}, page_content='The paper by Ulyanov, Vedaldi, and Lempitsky discusses instance normalization as a key component for enhancing the speed of image stylization processes, highlighting its significance in achieving fast and effective stylization techniques.'),\n",
       " Document(metadata={'doc_id': '7efc38d4-9b92-437c-9a9e-05d747d78335'}, page_content='The information provided is insufficient to generate a summary, as \"26\" does not offer context or details. Please provide a specific table or text for summarization.'),\n",
       " Document(metadata={'doc_id': '5d1168b4-2649-458c-b8f7-023acb33c217'}, page_content='Please provide the specific table or text chunk you would like summarized.'),\n",
       " Document(metadata={'doc_id': 'a3c17c3b-53e6-49f9-acbd-7d152cebba3c'}, page_content='Lin et al. (2014) presented the Microsoft COCO dataset, which focuses on common objects in context, during the 13th European Conference on Computer Vision (ECCV) held in Zurich, Switzerland.'),\n",
       " Document(metadata={'doc_id': 'bc8dd851-5e61-42af-b735-f4b5da27d2c2'}, page_content='The data or text from this range (740-755) has not been provided for summarization. Please provide the specific content to be summarized.'),\n",
       " Document(metadata={'doc_id': 'a6f0cafd-6e40-466c-bf01-264e07615a2c'}, page_content='The content provided is insufficient to summarize, as no specific table or text was included.'),\n",
       " Document(metadata={'doc_id': '5a73fe9c-3e1e-4542-a41e-b8d25a0eb8b9'}, page_content='The paper by Qiao, Chen, and Yuille presents DetectoRS, a method for object detection that utilizes a recursive feature pyramid and switchable atrous convolution, and was discussed at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) in Nashville, TN.'),\n",
       " Document(metadata={'doc_id': '4dda351b-b1c1-4924-bc0b-66a0ae4233dc'}, page_content='The information provided is insufficient to summarize. Please provide more context or details.'),\n",
       " Document(metadata={'doc_id': '0469f963-52c1-4b8f-ac04-7a24c879fe90'}, page_content='The paper presents the Hybrid Task Cascade (HTC) method for instance segmentation, enhancing performance by integrating multiple tasks in a unified framework. It was published in 2019 on arXiv.'),\n",
       " Document(metadata={'doc_id': '2252766d-4a49-4730-a71c-abbc132fa792'}, page_content='30.'),\n",
       " Document(metadata={'doc_id': '3207d2cc-6044-485d-ac10-d43dc44fef0f'}, page_content='Fang et al. (2021) propose a method titled \"Instances as Queries\" in their paper published on arXiv, which focuses on leveraging instances to formulate queries for improved information retrieval.'),\n",
       " Document(metadata={'doc_id': 'ead16a45-0622-4801-91d6-11dcedb3cf07'}, page_content='Mask R-CNN was presented by He, K., Gkioxari, G., Dollar, P., and Girshick, R. at the 2017 IEEE International Conference on Computer Vision (ICCV) held in Venice, Italy, from October 22-29, 2017.'),\n",
       " Document(metadata={'doc_id': '99ad6f2d-7323-44bb-977d-34bd0c6c6a6f'}, page_content='The content provided for summarization is incomplete or missing. Please provide a complete table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': '2ec7f3dc-2e48-4b4f-9acc-2479982385a0'}, page_content='The content appears to be a single numerical value, 32.'),\n",
       " Document(metadata={'doc_id': '01eb74d7-767e-413c-b84f-2170c3895501'}, page_content='Beyeler, M. authored a book titled \"OpenCV with Python Blueprints,\" published by Packt Publishing in London, UK, in 2015. The book focuses on designing and developing advanced computer vision projects using OpenCV with Python, and has the ISBN 978-178528269-0.'),\n",
       " Document(metadata={'doc_id': '96562206-537c-4294-b368-8ebc2febe027'}, page_content='The paper by Xie and Tu presents a method for edge detection using a holistically-nested approach, and was presented at the IEEE International Conference on Computer Vision (ICCV) in December 2015 in Santiago, Chile.'),\n",
       " Document(metadata={'doc_id': '82f32c6f-455c-41b9-96c2-f3eba405856c'}, page_content='The content provided does not include any specific table or text to summarize. Please provide the relevant information for summarization.'),\n",
       " Document(metadata={'doc_id': 'd0cdf6b5-4273-48b4-bdd4-deec61d37b24'}, page_content='Ding and Goshtasby discuss the Canny edge detector in their work published in Pattern Recognition, highlighting its significance in image processing for edge detection.'),\n",
       " Document(metadata={'doc_id': 'db72cb2d-947c-4afd-a2c6-912f7d71af7c'}, page_content='The information seems to reference a publication from 2001, appearing on pages 721 to 725 of a document, likely an article or research paper.'),\n",
       " Document(metadata={'doc_id': 'a981d3e1-8439-411b-9a70-b4a4938b46b3'}, page_content='The referenced work discusses the design of an image edge detection filter utilizing the Sobel operator, published in the IEEE Journal by authors anopoulos, N., Vasanthavada, N., and Baker, R.L.'),\n",
       " Document(metadata={'doc_id': '5963d65b-3620-4060-a52e-fe4459e43a85'}, page_content='The document references a publication from 1988 in the journal \"Solid-State Circuits,\" detailing research or findings related to solid-state electronics within the specified page range of 358-367.'),\n",
       " Document(metadata={'doc_id': 'f349aae8-2176-4907-89cf-e1b22b891e16'}, page_content='The reference lists authors Caesar, H., Uijlings, J., and Ferrari, V.'),\n",
       " Document(metadata={'doc_id': '08e638eb-b0c9-45db-b7cf-cfda4df45056'}, page_content='The COCO-stuff paper presented at the 2018 IEEE/CVF Conference focuses on the classification of both \"thing\" and \"stuff\" categories within visual contexts, detailing the context-aware categorization in computer vision.'),\n",
       " Document(metadata={'doc_id': 'a322d04e-f6fe-4ad8-99f2-ee8827f2bee1'}, page_content='The range 1209-1218 refers to numerical values or records, but no specific content is provided to summarize. More context is needed for an accurate summary.'),\n",
       " Document(metadata={'doc_id': 'a9be7818-e64e-408d-8ee6-d797b6c0c364'}, page_content='36.'),\n",
       " Document(metadata={'doc_id': '0a5e2f85-8b5b-44ed-b922-557c372b4374'}, page_content='Deng et al. (2009) presented a large-scale hierarchical image database called ImageNet at the IEEE Conference on Computer Vision and Pattern Recognition, held in Miami, FL.'),\n",
       " Document(metadata={'doc_id': '1e803f51-1708-496a-aeb8-b09ac8b72e71'}, page_content='The text discusses various topics from pages 248 to 255, including key findings, significant events, or trends relevant to the subject at hand. It likely covers specific details, case studies, or analyses that provide insights or conclusions drawn from the data or narrative presented in those pages.'),\n",
       " Document(metadata={'doc_id': '342cb580-ab76-4e5c-9d41-036a02a7e62c'}, page_content='The content consists of the number 37.'),\n",
       " Document(metadata={'doc_id': '50941cb9-2e2a-4ef2-9317-7dad6f41093d'}, page_content='The paper by Simonyan and Zisserman presents a novel architecture for very deep convolutional networks aimed at improving large-scale image recognition tasks, as detailed in their research published in 2014 on arXiv.'),\n",
       " Document(metadata={'doc_id': '4729b536-f3d6-4acb-95db-042bff97ee70'}, page_content='The article discusses deep residual learning techniques for image recognition, presented by He, Zhang, Ren, and Sun in 2015.'),\n",
       " Document(metadata={'doc_id': 'a6c9aec2-7b1a-4560-b452-26165d118ead'}, page_content=\"The work by Szegedy et al. (2015) presents a re-evaluation of the Inception architecture, focusing on its application within computer vision tasks. The authors discuss improvements and innovations that enhance the network's performance and efficiency.\"),\n",
       " Document(metadata={'doc_id': '5b2e68aa-a24a-4909-bcb3-85d6e9e87e8d'}, page_content='Chollet, F. presented Xception, a deep learning model utilizing depthwise separable convolutions at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) held in Honolulu, HI, from July 21-26, 2017.'),\n",
       " Document(metadata={'doc_id': '7fe56768-ad3a-4e84-87a5-60a405a24412'}, page_content='The data ranges from 1251 to 1258, likely detailing specific entries or metrics in a structured format. Without the specific content of the table or text, it is unclear what information is being presented.'),\n",
       " Document(metadata={'doc_id': 'f2fb3505-1244-4cf9-a960-fbcca2c3dcaf'}, page_content='The paper discusses densely connected convolutional networks (DenseNets), which enhance feature propagation and reuse through dense connections between layers, resulting in improved performance in various tasks, particularly in computer vision.'),\n",
       " Document(metadata={'doc_id': 'd52e2e1d-5136-4421-8e47-2e6912e0a5fb'}, page_content='The paper by Szegedy et al. discusses Inception-v4 and Inception-ResNet architectures, analyzing the effects of residual connections on learning in deep neural networks, presented in a 2016 arXiv publication.'),\n",
       " Document(metadata={'doc_id': '33a71981-bb2d-4b32-a836-adfaabe9fed0'}, page_content='Tan and Le introduced MixConv, a method involving mixed depthwise convolutional kernels, in a 2019 arXiv paper.'),\n",
       " Document(metadata={'doc_id': '58c6fa9e-ae7c-454b-8bcf-293e97f2f558'}, page_content='The referenced work is on \"Maxvit: Multi-axis vision transformer,\" presented at the European Conference on Computer Vision in 2022, authored by Tu et al. and published by Springer Nature in Cham, Switzerland.'),\n",
       " Document(metadata={'doc_id': 'b154c0fe-9d47-45fb-af48-74f486323b70'}, page_content='The text covers pages 459-479, detailing specific data or information relevant to the topic at hand, but without the actual content, a precise summary cannot be generated. Please provide the necessary data or context for a complete summary.'),\n",
       " Document(metadata={'doc_id': '124a3f2c-fea5-42b4-88f0-d45d363883a5'}, page_content='EfficientNet is a paper by Tan and Le that proposes a new approach to scaling convolutional neural network models, emphasizing efficiency and performance improvements through a systematic model scaling strategy.'),\n",
       " Document(metadata={'doc_id': '36b96dbf-9518-45b4-9b7a-60a539361d64'}, page_content=\"Agarap's 2018 paper discusses the use of Rectified Linear Units (ReLU) in deep learning, as presented in a preprint on arXiv.\"),\n",
       " Document(metadata={'doc_id': '6fef3265-a862-4e96-980e-33d21cde398e'}, page_content='The paper by Ioffe and Szegedy introduces Batch Normalization, a technique that accelerates the training of deep neural networks by minimizing internal covariate shift.'),\n",
       " Document(metadata={'doc_id': '3bb18480-789d-4f92-baf5-785be070f927'}, page_content='The text references a paper by Srivastava et al. discussing \"Dropout,\" a technique designed to reduce overfitting in neural networks.'),\n",
       " Document(metadata={'doc_id': 'ee8d182c-2170-4028-ab8c-e94c0758e29c'}, page_content='\"Mach\" refers to the Mach number, a dimensionless unit measuring the speed of an object relative to the speed of sound in the surrounding medium. It is commonly used in aerodynamics to describe supersonic and subsonic speeds.'),\n",
       " Document(metadata={'doc_id': '26ade809-d366-4334-8df8-f7b7436381ad'}, page_content='The content emphasizes the importance of learning, suggesting it is a fundamental process that leads to personal growth and development.'),\n",
       " Document(metadata={'doc_id': '69aa883d-4b4f-4245-bb55-df635258f4ad'}, page_content='The reference pertains to a publication from 2014, specifically volume 15, pages 1929 to 1958.'),\n",
       " Document(metadata={'doc_id': 'bd70d547-9814-468b-943a-60d04d306aaf'}, page_content='The document discusses a stochastic optimization method proposed by Kingma and Ba in 2014, detailing the Adam algorithm designed for improving optimization in machine learning.'),\n",
       " Document(metadata={'doc_id': '3faa6838-90c4-4ead-b64c-2ebc860df48f'}, page_content='The content is missing or unspecified, so a summary cannot be provided.'),\n",
       " Document(metadata={'doc_id': '8904c12f-cd4f-4d65-9525-c4dee2aa1eac'}, page_content='The paper by Graves (2013) discusses the use of recurrent neural networks (RNNs) for generating sequences, outlining techniques for training RNNs and their application in tasks like language modeling and text generation. It highlights the architecture of RNNs and presents experimental results that demonstrate their effectiveness in generating coherent sequences.'),\n",
       " Document(metadata={'doc_id': '72e0405f-87d9-40c0-bcf2-288f13de8ca0'}, page_content='The paper by Selvaraju et al. presents Grad-CAM, a method for generating visual explanations from deep neural networks using gradient-based localization techniques.'),\n",
       " Document(metadata={'doc_id': '21bdebe7-762c-46e3-8461-dc499369a2dd'}, page_content='The input provided is a single letter, \"J.\" There is no additional context or information to summarize.'),\n",
       " Document(metadata={'doc_id': '887d3a91-13e3-4bd5-9340-1128cfeb1215'}, page_content='The text does not provide enough context or information to generate a summary.'),\n",
       " Document(metadata={'doc_id': 'bb94c60a-dbad-48de-a345-70b0609c7581'}, page_content='The text or table related to \"Vis\" is not provided; therefore, a summary cannot be generated.'),\n",
       " Document(metadata={'doc_id': 'b5757548-1a27-4037-ac90-d572df5c5980'}, page_content='In 2020, the values range from 128 to 336-359.'),\n",
       " Document(metadata={'doc_id': '143b56c5-0a79-4b91-ad9c-939ad9965707'}, page_content='The entry titled \"CrossRef\" does not contain enough contextual information or data to provide a summary.'),\n",
       " Document(metadata={'doc_id': '8e68f26f-9a1c-4dba-8e05-d24a97ea264a'}, page_content='The provided text or table chunk is insufficient for summarization. Please provide additional details or context.'),\n",
       " Document(metadata={'doc_id': '174594cf-27cd-46bf-8269-6fb35fba7aed'}, page_content='Desai and Ramaswamy presented Ablation-CAM, a method for providing visual explanations for deep convolutional networks using gradient-free localization, at the 2020 IEEE Winter Conference on Applications of Computer Vision.'),\n",
       " Document(metadata={'doc_id': 'c6be6120-0188-4ca5-9e90-a64f2ad4e56b'}, page_content='The content spans pages 972 to 980, likely containing a series of entries, data points, or discussions across various topics, but specific details or context are not provided for summarization.'),\n",
       " Document(metadata={'doc_id': '28129db2-44a8-4807-893e-32e2f534745f'}, page_content='The paper by Jiang et al. introduces LayerCAM, a method that explores hierarchical class activation maps to enhance localization in deep learning models, as published in the IEEE Transactions.'),\n",
       " Document(metadata={'doc_id': 'd57b10b1-d171-452f-af85-5c924a9e19fa'}, page_content='The text outlines various techniques and methods used for image processing, focusing on enhancing, analyzing, and manipulating images to improve quality and extract useful information. It covers algorithms, tools, and applications relevant to fields such as computer vision and photography.'),\n",
       " Document(metadata={'doc_id': '67590f08-f251-4808-92b2-b86a341737b8'}, page_content='The data refers to a publication from 2021, spanning pages 5875-5888, with a total of 30 entries or sections.'),\n",
       " Document(metadata={'doc_id': '2c95c26b-d3f3-4763-8ddd-d3f545c1a82e'}, page_content='The provided content does not contain enough information to summarize.'),\n",
       " Document(metadata={'doc_id': '8fd810aa-49c4-417c-a639-1ee6e7b1f33e'}, page_content='The reference appears to be for a publication in the journal \"Imaging\" from the year 2023, specifically volume 9, page 199.'),\n",
       " Document(metadata={'doc_id': '27ff2de5-449e-4b00-b272-c09b6aacb939'}, page_content='Score-CAM is a method for generating score-weighted visual explanations for convolutional neural networks, presented by Wang et al. at the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.'),\n",
       " Document(metadata={'doc_id': '555da0a8-0e5f-4eea-836d-cb0a0a6ee608'}, page_content='56.'),\n",
       " Document(metadata={'doc_id': 'd888d6a8-28b0-4111-addc-3a0fcd701ffb'}, page_content='The entry lists the authors Ribeiro, M.T., Singh, S., and Guestrin, C.'),\n",
       " Document(metadata={'doc_id': '21779231-c52b-48c0-b732-36ca3a82dd9e'}, page_content='The paper discusses the importance of explaining the predictions made by classifiers in machine learning, emphasizing trust in automated systems. It was presented at the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining in August 2016.'),\n",
       " Document(metadata={'doc_id': '4a49a56e-0739-48ef-a5ba-0db722f443cf'}, page_content='There is no data provided for summarization.'),\n",
       " Document(metadata={'doc_id': '2bbece78-6d2a-44f7-bf1c-c4dec364be5c'}, page_content='The paper presents research on scene parsing using the ADE20K dataset, discussed at the 2017 IEEE CVPR conference in Honolulu, Hawaii.'),\n",
       " Document(metadata={'doc_id': 'f440146c-d975-4328-9935-54b4cd6355e7'}, page_content='The numbers 5122-5130 represent a sequence of integers from 5122 to 5130, inclusive.'),\n",
       " Document(metadata={'doc_id': 'e79eeda4-8bdc-41a0-a794-e04abf6d5207'}, page_content='The input appears to be incomplete or does not provide sufficient context to summarize effectively. Please provide the complete table or text for a proper summary.'),\n",
       " Document(metadata={'doc_id': '75d8f6ad-5b9a-4b41-997e-89dd07eba952'}, page_content='The paper discusses the \"sketchy database,\" a resource designed to enhance the ability to retrieve poorly drawn depictions of bunnies, contributing to advancements in image retrieval systems.'),\n",
       " Document(metadata={'doc_id': '200c4913-2af8-4549-9c26-89f09300506d'}, page_content='The document is a journal article published in 2016 in volume 35, pages 1-12.'),\n",
       " Document(metadata={'doc_id': '6866dbc3-20c2-4bfe-bf33-cc27374c6d3b'}, page_content='No information provided to summarize.'),\n",
       " Document(metadata={'doc_id': '79305d6d-db8f-4e2d-afcb-4026af7b34e9'}, page_content='The Caltech-UCSD Birds-200-2011 Dataset, created by Wah et al. in 2011, is published by the California Institute of Technology. The disclaimer states that the views and data in all publications are the responsibility of the authors and not of MDPI or the editors.'),\n",
       " Document(metadata={'doc_id': 'c03eca01-e3fb-4ddb-9f81-d087821e2bf7'}, page_content='The document outlines a practical guide titled \"Mastering Generative AI and Prompt Engineering\" aimed at data scientists, emphasizing the growing importance of prompt engineering in the AI-driven economy. It includes chapters on understanding generative AI, prompt engineering basics, designing effective prompts, advanced techniques, ethical considerations, and real-world applications. Key topics cover different types of prompts, AI model capabilities, and ethical challenges such as bias and data privacy.'),\n",
       " Document(metadata={'doc_id': '0824552b-acff-4602-b42d-c3ad5d1500f9'}, page_content='The section focuses on content generation and creative writing, likely discussing methods, strategies, and tools for producing written material and enhancing creativity in writing.'),\n",
       " Document(metadata={'doc_id': 'b556357d-1620-4825-b321-da9175228648'}, page_content='The document covers topics related to data analysis and visualization, including chatbots and conversational AI, anomaly detection, and pattern recognition. It concludes with recommended resources and online communities for further exploration in the field of artificial intelligence, which has significantly progressed due to generative AI and prompt engineering.'),\n",
       " Document(metadata={'doc_id': '81047313-a62f-42b8-947b-c49f3454472f'}, page_content='The ebook serves as a comprehensive guide for data scientists on generative AI and prompt engineering, highlighting the importance of staying current with trends and techniques. It focuses on models that generate new data from existing inputs, showcasing their applications in natural language processing and image generation.'),\n",
       " Document(metadata={'doc_id': '8a0786f3-5229-4063-a338-f37f4fe44f2f'}, page_content='Generative AI enables data scientists to create innovative solutions by understanding its complexities, while prompt engineering focuses on crafting effective prompts to guide AI models for desired outputs. The need for precise prompt engineering has increased with the advancement of AI models. An accompanying ebook covers key concepts, best practices, and real-world applications of generative AI and prompt engineering, including case studies and ethical considerations. The evolution of AI, from rule-based systems to generative models, highlights significant technological advancements over the years.'),\n",
       " Document(metadata={'doc_id': '9c1e69ff-79c7-4cbc-a06e-8f6ec00eb7ed'}, page_content='Rule-based AI systems, while effective for simple tasks, faced limitations in scaling and adapting to complex problems due to their rigidity. Machine learning emerged as a more flexible approach, enabling AI to learn patterns from data and generalize to new situations. Generative AI further advances this by generating new data patterns, excelling in applications like natural language processing and image synthesis. Key generative AI models include Restricted Boltzmann Machines (RBMs), Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers, each contributing unique capabilities to the field.'),\n",
       " Document(metadata={'doc_id': 'f47151d4-df02-44cd-b458-28af124d55a8'}, page_content='RBMs, VAEs, GANs, RNNs, and LSTMs are various generative models utilized for different tasks. RBMs are employed in image synthesis and feature learning. VAEs merge deep learning with Bayesian inference for tasks like image synthesis and text generation through encoding and decoding processes. GANs use a generator-discriminator framework to create high-quality synthetic data across various mediums. RNNs process sequential data with feedback loops, applicable in text and speech generation, while LSTMs enhance RNNs to manage long-range dependencies, addressing the vanishing gradient issue.'),\n",
       " Document(metadata={'doc_id': 'ad68249b-f946-4f3e-9b3f-e4e05113d487'}, page_content='Transformers are neural network architectures that use self-attention mechanisms, allowing for parallel processing of sequences, which enhances efficiency for large-scale tasks. GPT, developed by OpenAI, is a notable transformer model recognized for its strong performance in natural language processing and generation, providing valuable tools for data scientists in various applications.'),\n",
       " Document(metadata={'doc_id': 'bdeb4bcf-6279-4cc2-a07d-f86fe0f3c5d0'}, page_content='Generative AI is valuable in data science by creating new data from existing patterns. It significantly impacts natural language processing (NLP), enabling coherent text generation for applications like chatbots, summarization, translation, and content creation.'),\n",
       " Document(metadata={'doc_id': 'c0e42f15-b665-4df0-be35-1f876a79418e'}, page_content='Generative models, such as GPT, GANs, and VAEs, have advanced significantly across various domains, including NLP, image synthesis, music generation, drug discovery, anomaly detection, and data augmentation. They facilitate the creation of realistic text, images, music, and molecular structures, while also aiding in identifying anomalies and improving machine learning performance by generating additional training data.'),\n",
       " Document(metadata={'doc_id': '1b2eade8-da14-444a-875c-4e550c6b7ccb'}, page_content=\"Generative AI enables the creation of synthetic data for simulating scenarios, aiding businesses and researchers in testing hypotheses and strategies across various fields such as finance and urban planning. The ongoing advancement of this technology is expected to yield more innovative applications. Prompt engineering, which involves crafting effective prompts for AI models, is critical for guiding generative models like GPT to produce high-quality outputs. The quality of a prompt impacts the relevance and accuracy of AI-generated responses, making prompt engineering essential for optimizing AI performance in diverse applications, including content generation and data analysis. As generative AI's complexity grows, mastering prompt engineering becomes increasingly vital for data scientists.\"),\n",
       " Document(metadata={'doc_id': 'a65268ff-034d-40fe-870d-96eb9fa6d8c9'}, page_content='Prompts for generative AI can be classified into three types: explicit, implicit, and creative. Explicit prompts give direct instructions and specify the desired output format, which leads to accurate results but may hinder creativity. Implicit prompts are more open-ended, allowing the AI model to interpret the context and potentially generate diverse outputs, though they can also produce ambiguous responses. Creative prompts aim to inspire novel and imaginative responses through open-ended scenarios, but they may require more iteration for optimal results. Best practices for crafting effective prompts include being clear and concise to ensure the AI comprehends the instructions.'),\n",
       " Document(metadata={'doc_id': '46cfeea5-61e6-4d84-b02c-8552353cd322'}, page_content='Concise prompts enhance model focus and accuracy. Providing context, such as titles and publication details, guides the AI for relevant outputs. Specifying desired formats further aids in achieving structured results.'),\n",
       " Document(metadata={'doc_id': '02ac8f31-8a8f-4553-bca6-6909829bdb76'}, page_content='- Generative AI models may produce unexpected outputs; rephrasing prompts can improve results.\\n- Finding a balance between guidance and creative freedom in prompts is essential for effective outputs.\\n- Regular evaluation and iteration of prompts enhances the quality of results from AI models.\\n- Custom prompts significantly improve the performance of generative AI in natural language processing tasks.\\n- Effective prompts for text summarization should specify details like length, format, and key points.'),\n",
       " Document(metadata={'doc_id': '05c54090-e558-43db-bb4c-38c4b39ba367'}, page_content='Effective prompts for AI summarization should provide clear instructions and context. For summarization, examples include requests for concise overviews of news articles, research papers, or book chapters, focusing on key findings and implications. For sentiment analysis, prompts should guide the model to assess emotions in specific contexts, such as customer reviews of a product.'),\n",
       " Document(metadata={'doc_id': 'ab4cf876-f582-477d-be59-4d1b41e1cbf0'}, page_content=\"The two prompts provide clear contexts for sentiment analysis: one focuses on Twitter users' feelings about a political controversy, while the other assesses the overall sentiment of movie reviews for a new blockbuster film. Both prompts are specific and well-defined.\"),\n",
       " Document(metadata={'doc_id': 'f27c611b-090a-4d02-b8a9-b8eb062beafe'}, page_content='Effective prompt engineering is essential for guiding AI in generating contextually relevant text across various tasks such as text generation, question-answering, text classification, and machine translation. Specific, detailed prompts can enhance the quality of outputs by providing clear themes, context, and instructions. Examples include creating imaginative stories, explaining concepts, classifying text types, and translating languages to improve accuracy and relevance in responses. Mastering prompt crafting enables data scientists to maximize the capabilities of generative AI models in natural language processing tasks.'),\n",
       " Document(metadata={'doc_id': '2a848a79-41a1-4b7c-8d9b-4fc88bc4109f'}, page_content='Generative AI models can enhance creativity and diversity in content through effective prompt engineering. Key strategies include using open-ended prompts to inspire imaginative responses, introducing constraints to foster innovative thinking, combining multiple themes for unique narratives, encouraging variability by generating multiple versions, experimenting with different styles and formats, and employing iterative prompting to refine outputs. These approaches help data scientists unlock the creative potential of AI, improving the quality and scope of applications across various fields such as entertainment, marketing, education, and research.'),\n",
       " Document(metadata={'doc_id': '996f14a5-4388-48d2-bfff-49f79b23b6a7'}, page_content='Generative AI models can perpetuate training data biases, leading to ethical issues. Thoughtful prompt engineering can mitigate these concerns by promoting fairness, avoiding stereotypes, generating positive content, ensuring factual accuracy, and allowing for iterative refinement. Strategies include crafting inclusive prompts, discouraging discrimination, emphasizing responsibility, and incorporating user feedback to improve AI outputs. By implementing these practices, data scientists can enhance the ethical use of AI, resulting in fairer and more balanced content.'),\n",
       " Document(metadata={'doc_id': '8a9bc073-768e-4f7d-a019-b026d8c1099d'}, page_content='Generative AI models can create personalized and adaptable content by utilizing prompt engineering to incorporate user preferences, language, tone, context, and feedback. This includes crafting prompts for tailored content, personalized recommendations, and customized learning experiences. However, challenges such as inherent biases in AI models and understanding their limitations must be acknowledged to improve the effectiveness and responsibility of prompt engineering strategies.'),\n",
       " Document(metadata={'doc_id': '3dbc1654-6c52-4f51-a58c-36a0335b464f'}, page_content='Limitations may cause unexpected, irrelevant, or nonsensical outputs despite effective prompts.'),\n",
       " Document(metadata={'doc_id': '0b6472d0-c2cc-473c-98d1-c973f88e7724'}, page_content='The text discusses challenges and strategies in prompt engineering for generative AI models. It highlights inherent biases, unpredictability, overfitting, evaluation difficulties, and ethical considerations that data scientists must address. Strategies for effective prompt engineering include starting with broad prompts, experimenting with varying detail levels, iterative refinement, encouraging variability, leveraging constraints creatively, and collaborating with users. Striking the right balance between guidance and flexibility is essential for maximizing creativity and ensuring relevance in AI outputs.'),\n",
       " Document(metadata={'doc_id': '1975f3b9-32a4-401c-8c5d-c9dc4e0ed7e2'}, page_content='To ensure quality and reliability in AI-generated content, it is essential to focus on prompt engineering, which can help generate high-quality content while addressing challenges related to consistency, accuracy, and relevance. Key strategies include: \\n\\n1. **Testing and Evaluation**: Regularly assess AI-generated content with quantitative and qualitative measures against clear benchmarks to improve prompts and models.\\n2. **User Feedback and Collaboration**: Involve users in the prompt engineering process to align content with their needs and preferences, addressing areas for improvement.\\n3. **Continuous Model Improvement**: Stay informed about advancements in AI to refine models and techniques based on user input and industry practices.'),\n",
       " Document(metadata={'doc_id': '6e47c366-9483-449a-b585-b434e384d59d'}, page_content='Implement strong error handling and monitoring to identify and rectify issues in AI-generated content, including inaccuracies and harmful material, while ensuring adherence to quality standards through safeguards and filters.'),\n",
       " Document(metadata={'doc_id': 'eee2738a-f9ed-44fe-9b3d-082407b43237'}, page_content='The text discusses the importance of clear communication and transparency regarding AI-generated content, emphasizing management of user expectations and building trust. It highlights the necessity of multimodal validation to enhance content reliability and suggests that advanced AI models and techniques, such as fine-tuning and multi-modal integration, can improve the performance of generative AI systems. The text also addresses the significance of contextual understanding, active learning, and collaborative AI for personalized and efficient content generation. Additionally, it explores the convergence of human and AI creativity, emphasizing ethical practices, the democratization of creativity, and fostering interdisciplinary innovations to drive progress in various fields.'),\n",
       " Document(metadata={'doc_id': '6cf0a003-d8e5-48ef-a543-9accfb35281f'}, page_content='The role of prompt engineering is becoming increasingly significant in the AI-driven economy, enhancing business communications by creating tailored content, accelerating research and development through information synthesis and insight generation, personalizing customer experiences based on user data, and democratizing access to expertise across various fields. This evolution is aimed at improving effectiveness, efficiency, and innovation in diverse sectors.'),\n",
       " Document(metadata={'doc_id': 'b4a4ed00-a968-4534-9d98-d3555ea13076'}, page_content='Prompt engineering enhances policy-making by providing evidence-based insights, fostering creativity and innovation through the synergy of human and AI efforts, and addressing global challenges with scalable solutions. As the AI-driven economy grows, prompt engineering will be pivotal in creating effective, innovative strategies for societal issues. Practical guidelines for implementing prompt engineering are available for practitioners and data scientists aiming to improve AI-generated content.'),\n",
       " Document(metadata={'doc_id': '77579b7f-28c0-46f4-b07d-538e96e92176'}, page_content=\"To effectively start with prompt engineering, first understand the AI model's capabilities and limitations. Establish clear objectives relating to the target audience and intended outcomes. Begin with simple, open-ended prompts to assess the model’s performance and address any biases. Finally, adopt an iterative approach by experimenting and refining prompts based on trial and error to achieve the desired results.\"),\n",
       " Document(metadata={'doc_id': 'c0a31b32-eab7-40a6-98eb-789f075cc546'}, page_content='Regular evaluation of AI-generated content through quantitative and qualitative metrics is essential for maintaining quality. Engaging with the prompt engineering community fosters collaboration and learning. Ethical considerations are crucial, ensuring that content creation is responsible and inclusive. A structured workflow enhances the consistency and quality of AI-generated content, beginning with thorough planning and research on the relevant topics and audience.'),\n",
       " Document(metadata={'doc_id': 'a7fb6ba2-8de9-4246-b0fb-c105d6136865'}, page_content='The research outlines a structured approach to prompt engineering for AI-generated content, emphasizing the importance of setting clear objectives, drafting initial prompts, testing and evaluating content, iterating and refining prompts, monitoring performance, and collaborating for continuous learning. It highlights the need to address common challenges, including AI model limitations, to enhance the quality and relevance of generated content, ultimately aiding in achieving long-term success in the field of generative AI.'),\n",
       " Document(metadata={'doc_id': '1b9ce567-2230-4010-b087-f9a92c1fc5f0'}, page_content='To improve AI content quality and address limitations, experiment with diverse prompt structures, fine-tune models on specific domain data, or combine outputs from multiple models. To mitigate biases in AI, use bias-aware prompt engineering and external tools to identify and correct biased results.'),\n",
       " Document(metadata={'doc_id': '727df2f6-2c0a-4cae-97e1-696cfe14e4d5'}, page_content='Achieving the right balance in prompt guidance is key for effective AI-generated content. This involves adjusting specificity and context based on performance metrics. Quality assurance requires clear benchmarks and mixed evaluations. Creative shortfalls can be addressed through open-ended prompts and multi-modal models. Ethical considerations must be prioritized to promote fairness and inclusivity in content creation. Ongoing refinement in these areas enhances the effectiveness and quality of generative AI output.'),\n",
       " Document(metadata={'doc_id': 'b56142cd-99e2-4937-becb-353d82054f94'}, page_content='Establishing clear metrics and benchmarks is essential for evaluating the success of prompt engineering efforts. Key performance indicators (KPIs) to consider include the quality of AI-generated content, focusing on coherence, consistency, grammatical correctness, and readability.'),\n",
       " Document(metadata={'doc_id': '5db62f19-6790-4ac2-b5f5-f98500b8744c'}, page_content='The text outlines metrics for assessing AI-generated content across several dimensions: \\n\\n1. **Quality**: Utilize readability scores and grammar checkers.\\n2. **Relevance**: Align content with objectives and audience, using qualitative assessments and engagement metrics.\\n3. **Novelty and Creativity**: Measure uniqueness and originality using tools like plagiarism checkers.\\n4. **Accuracy**: Compare content against reliable sources and expert knowledge.\\n5. **User Satisfaction**: Collect feedback through surveys to identify improvement areas.\\n6. **Efficiency**: Assess prompt engineering efficiency by evaluating generation time, iterations, and costs.\\n7. **Adaptability**: Evaluate how well content performs across different channels and formats.'),\n",
       " Document(metadata={'doc_id': 'addbba90-717a-4b65-8673-2a20bf9c9af3'}, page_content=\"Establishing clear KPIs and benchmarks for prompt engineering enhances the measurement of AI-generated content's success, ensuring quality and relevance. The ebook covers generative AI fundamentals, best practices, techniques, challenges, and ethical considerations, while highlighting future trends in prompt engineering. It emphasizes the critical role of prompt engineering in various sectors to drive creativity and innovation. Lastly, it provides a list of recommended books, articles, courses, and online resources for further learning in generative AI and prompt engineering to inspire experimentation and collaboration in the field.\"),\n",
       " Document(metadata={'doc_id': 'a8e20e13-4f91-44d0-ab53-53e9e6cfbd44'}, page_content='Appendix B lists online platforms for discussing and collaborating on generative AI and prompt engineering, highlighting AI Stack Exchange as a key resource for Q&A on AI topics.'),\n",
       " Document(metadata={'doc_id': 'a75a09d6-9a4a-4f6c-a9cb-d27fc7ec2509'}, page_content='The Machine Learning Subreddit is a community platform for discussions on machine learning, deep learning, and generative AI, where users can share projects and ask questions. The AI section on arXiv serves as a repository for recent preprints of AI research papers.'),\n",
       " Document(metadata={'doc_id': '58d784c1-6265-4d68-b8db-cfaf90c45712'}, page_content='The text describes activities such as browsing recent research, giving feedback, and participating in discussions with researchers and practitioners.'),\n",
       " Document(metadata={'doc_id': '6a3ae0ff-f1bc-4f6a-9e12-aa8c10a7f8bc'}, page_content='The Deep Learning Subreddit is a community for sharing resources and discussing deep learning and generative AI. The OpenAI Community forum allows users to engage in conversations about AI research, GPT models, and prompt engineering, fostering connections among AI enthusiasts.'),\n",
       " Document(metadata={'doc_id': '3d7dfed2-c6b3-4df9-9d14-067896f970d9'}, page_content='Data Science Stack Exchange is a Q&A platform for data science professionals and enthusiasts to ask questions, provide answers, and explore discussions on data science, machine learning, and AI topics.'),\n",
       " Document(metadata={'doc_id': 'a0151d2b-f15c-4bd7-8e4f-2aaaaea9aba7'}, page_content=\"Participating in online communities and forums for generative AI and prompt engineering allows individuals to network, share knowledge, collaborate, learn from experts, stay updated on advancements, and contribute to the field's growth.\"),\n",
       " Document(metadata={'doc_id': '67b16758-75c6-4b44-bb07-e9192832ec4c'}, page_content='The table of contents outlines the sections of a document, including a preface, glossary, introduction to Large Language Models (LLMs), and an explanation of what LLMs are, indicating the document is likely focused on the topic of LLMs.'),\n",
       " Document(metadata={'doc_id': '78539905-7185-42be-b471-f9ea1996b233'}, page_content=\"The table outlines various topics related to language models, including comparisons between foundation and fine-tuned models, the evolution of large language models, neural networks, benefits for enterprises, challenges, construction and evaluation of models, notable companies, and popular startup applications. The accompanying text introduces language's historical significance and discusses theories regarding the evolution of speech and language.\"),\n",
       " Document(metadata={'doc_id': '6616f71c-1bf2-4259-9b09-e581de4215f5'}, page_content='Language is the fundamental basis of human communication.'),\n",
       " Document(metadata={'doc_id': '72c55094-de00-47e7-8bdd-018191ca26b3'}, page_content='The digital age has amplified the importance of language, with 347.3 billion emails and over five billion text messages exchanged daily, highlighting its significance for enterprises in extracting insights and trends. Analyzing customer feedback can inform product development, while language production, such as blog posts, enhances brand visibility. However, both tasks are time-consuming and can hinder decision-making. Large language models (LLMs) can automate and improve language tasks, offering versatility and efficiency over traditional methods. This book aims to educate enterprises on the advantages of LLMs and guide them through their development and deployment.'),\n",
       " Document(metadata={'doc_id': '20944a4f-fffd-4ddc-8eba-45b1132b4f30'}, page_content='The book is structured into three parts: Part 1 defines LLMs and explores advancements in the field while advising enterprises on development. Part 2 highlights five use cases of LLMs in businesses with real-life examples. Part 3 serves as a practical guide for enterprises to build, train, and deploy LLMs, detailing requirements and trade-offs to inspire adoption for competitive advantage.'),\n",
       " Document(metadata={'doc_id': '021ad0cb-ec9e-4f52-9ee0-70c0c1685669'}, page_content='Early adopters and visionary innovators will gain the most benefits from advancements in large language models (LLMs) and related technologies. The glossary includes key terms such as deep learning systems, generative AI, natural language processing (NLP), and various types of neural networks.'),\n",
       " Document(metadata={'doc_id': 'e1495637-9c73-4c19-8700-837c4558e04c'}, page_content='AI programs can create original content, such as text, images, and audio, instead of merely analyzing existing material.'),\n",
       " Document(metadata={'doc_id': 'f98f0e5f-be39-400d-89a9-6c5c3f1e62ec'}, page_content='Large language models (LLMs) are AI systems trained on extensive data sets, enabling them to recognize, summarize, translate, predict, and generate text and other content. They have a high number of parameters, often reaching hundreds of billions, allowing for effective understanding and generation of natural language.'),\n",
       " Document(metadata={'doc_id': '1dc5a395-a0a3-44f2-a7d4-fac28a3ef48b'}, page_content='A specialized type of RNN with advanced cell blocks enhances the retention of past inputs and is utilized in NLP for generating human-like text.'),\n",
       " Document(metadata={'doc_id': '87ef144b-f95d-485a-9725-955678c7d9ff'}, page_content='NLP involves computer programs understanding human-like text. Neural networks (NNs) use layered machine learning algorithms inspired by the human brain, allowing for representation learning with minimal human input. Prior to 2020, AI programs mainly focused on processing and analyzing data rather than generating it.'),\n",
       " Document(metadata={'doc_id': '1888182d-6992-47aa-883f-f869a3a50c82'}, page_content='The text discusses a neural network capable of sequential data processing and memory retention. It defines terms related to machine learning, including rule-based systems, traditional machine learning, transformers, structured and unstructured data, fine-tuning, parameter-efficient techniques (PEFT), prompt learning, and adapter tuning.'),\n",
       " Document(metadata={'doc_id': '8d36ae2f-0ecb-4b54-86bd-0ead791239e2'}, page_content='It emphasizes data over rules.'),\n",
       " Document(metadata={'doc_id': '197bc633-846c-4991-b39e-65e7083e2d75'}, page_content='A neural network architecture that processes sequential data in a non-sequential manner.'),\n",
       " Document(metadata={'doc_id': 'c4ce67d8-1a3e-4e37-9cff-7bdce0774146'}, page_content='Quantitative data that is easily standardized and adjustable for efficient processing by machine learning algorithms, such as phone numbers.'),\n",
       " Document(metadata={'doc_id': 'c8a20a2f-7204-45e5-9ed7-2e5a0f05a954'}, page_content='Qualitative data, like customer reviews, is challenging to standardize.'),\n",
       " Document(metadata={'doc_id': '2a913732-2db3-462d-80f9-c51fd5978458'}, page_content='Data is stored in its native formats, such as PDF files, prior to use.'),\n",
       " Document(metadata={'doc_id': 'dbfe8ceb-d5e7-43f5-ba0d-d614a1c14fc4'}, page_content='A transfer learning method enhances model performance on specific downstream tasks or datasets.'),\n",
       " Document(metadata={'doc_id': 'be42dfc2-b9be-496d-a2ae-201b004f8654'}, page_content='The text discusses parameter-efficient fine-tuning (PEFT) techniques for pre-trained language models (PLMs), emphasizing methods like prompt learning, LoRa, and adapter tuning. These approaches enhance model performance by modifying select parameters rather than the whole model, allowing for quicker training and more accurate predictions. Additionally, it mentions open-domain question answering, which covers diverse fields such as legal, medical, and financial domains.'),\n",
       " Document(metadata={'doc_id': 'b2b684f2-9d9e-49c8-9f62-2d2564d619bc'}, page_content='Extractive question answering involves retrieving answers from existing texts or databases. Key terms include throughput, which measures model efficiency and speed, and latency, which relates to the time taken to process requests. Data readiness is also essential for implementing large language models.'),\n",
       " Document(metadata={'doc_id': '1ed34e6a-41d2-4c25-9335-51679ea9f136'}, page_content='The passage discusses the capacity of a model to process or generate data or predictions within a specified time limit.'),\n",
       " Document(metadata={'doc_id': '821575f4-49c7-4155-b0c0-c7cb871091fd'}, page_content='The table/text discusses the processing time required by a model to handle input and produce output.'),\n",
       " Document(metadata={'doc_id': 'f5f57415-8df6-4102-ae87-e57aa8b5cd0f'}, page_content=\"Large language models (LLMs) are advanced AI systems that generate human-like text by learning from extensive datasets using deep learning techniques. Their capabilities have transformed natural language processing (NLP) since the introduction of models like OpenAI's GPT-3 in 2020, which demonstrated proficiency in various tasks such as translation and summarization. LLMs effectively understand context, produce grammatically correct text, and can perform complex NLP tasks with little human oversight.\"),\n",
       " Document(metadata={'doc_id': '83a7be81-3c89-4ba8-8a60-1b7c82f2b8eb'}, page_content='The advancement of LLMs marked a significant improvement over previous rule-based models, which lacked self-learning capabilities and adaptability. Consequently, many enterprises and startups began developing or adopting LLMs to enhance operations, reduce costs, and improve workflows.'),\n",
       " Document(metadata={'doc_id': '86251970-a60b-449d-8c2d-6faa7ca431e9'}, page_content='Part 1 introduces large language models (LLMs), which are deep learning algorithms capable of processing and generating text by learning from extensive datasets, forming a foundation for enterprises considering their development or adoption.'),\n",
       " Document(metadata={'doc_id': 'bbca207a-573a-41bf-99e5-7847488fe24f'}, page_content='All language models can process and generate text resembling natural language.'),\n",
       " Document(metadata={'doc_id': '36a1dcb1-8fc0-4bf2-bfb5-50f92aaac978'}, page_content='Large Language Models (LLMs) are a type of language model distinguished by their size, which allows them to perform a variety of natural language processing (NLP) tasks.'),\n",
       " Document(metadata={'doc_id': '6fba72f9-816f-4ebb-87a1-e5602d22a85b'}, page_content='They are trained with extensive data sets.'),\n",
       " Document(metadata={'doc_id': 'c5b5c535-e307-40ea-978f-ebe8cd5cd435'}, page_content='Please provide the text or table you would like summarized.'),\n",
       " Document(metadata={'doc_id': '3883da53-4acd-43b4-a0e2-3b4e41ba3062'}, page_content='The text compares two large language models, MT-NLG and GPT-3 Davinci, highlighting their learnable parameters and training data sizes. MT-NLG has 530 billion parameters and 270 billion tokens in its training data, whereas GPT-3 Davinci has 175 billion parameters and 499 billion tokens. Larger models tend to produce more accurate responses due to their size and training data quality.'),\n",
       " Document(metadata={'doc_id': '7b045d19-867b-4e96-a213-fb643f901d19'}, page_content='Answer generated by GPT-3.'),\n",
       " Document(metadata={'doc_id': '7bdaaf7a-2ae0-420a-9dcc-c30ead1cc3bc'}, page_content='Large language models improve natural language understanding and generation, aiding in question answering, translation, summarization, and creative text generation. Their performance is influenced by factors beyond just model size and data quantity.'),\n",
       " Document(metadata={'doc_id': '50d1fd03-00d7-4f5a-89e1-066aa6eb2422'}, page_content='Data quality is important.'),\n",
       " Document(metadata={'doc_id': '2107a74d-5b97-4bb8-bb9d-ca8944ba9d70'}, page_content='LLMs trained on high-quality sources like peer-reviewed research and published novels perform better than those trained on low-quality data such as social media posts, which can introduce issues like slang and incorrect spellings.'),\n",
       " Document(metadata={'doc_id': '4fe8efce-aaf8-44e5-8212-a1a3790c6104'}, page_content='Models require diverse data to effectively perform various NLP tasks.'),\n",
       " Document(metadata={'doc_id': 'baf80f20-7f60-46a9-9957-c83ac324d385'}, page_content='Foundation language models like MT-NLG and GPT-3 are trained on extensive datasets to handle diverse NLP tasks. Fine-tuning these models with specific, narrower datasets enhances their performance on specific tasks within a specialized domain.'),\n",
       " Document(metadata={'doc_id': '1dd7df89-dbe6-4571-89f9-1b7fa4b61ea8'}, page_content='Foundation models exhibit strong general performance across various tasks with limited domain-specific data but may not excel in specialized tasks. Fine-tuned language models are derived from these foundation models and are tailored for specific use cases, enhancing their effectiveness in specialized tasks.'),\n",
       " Document(metadata={'doc_id': '252f27e4-6758-4f5b-af62-6d43b4aa0ddc'}, page_content='Fine-tuned models excel in specific tasks compared to foundation models and are generally lighter and easier to train. The main method for fine-tuning involves parameter-efficient techniques like p-tuning, prompt tuning, and adapters.'),\n",
       " Document(metadata={'doc_id': 'c7f1f1e4-4928-487c-9653-91c7a80b72ea'}, page_content='Customization is quicker and cheaper than fine-tuning a whole model, but may result in slightly lower performance.'),\n",
       " Document(metadata={'doc_id': 'e9d6d9e9-93f0-4345-863e-5711149df4a7'}, page_content='Customization methods for large language models are detailed in Part 3, emphasizing the shift from AI systems focusing on data processing and analysis to generating data.'),\n",
       " Document(metadata={'doc_id': '66d165b3-06ac-4195-9660-1fbc27cbe023'}, page_content='The text discusses the distinction between Perceptive AI, which focuses on understanding the world, and Generative AI, which has gained prominence since 2020 alongside the advent of transformer models and large language models (LLMs). It highlights a paradigm shift in natural language processing (NLP) with the introduction of LLMs, contrasting them with earlier NLP models that relied on human-crafted rules. The pre-transformers NLP was limited to simpler tasks, making it unsuitable for complex tasks like machine translation.'),\n",
       " Document(metadata={'doc_id': 'a567d5f3-83ea-4d63-8463-b4aaf2107332'}, page_content='Rule-based models struggled in edge-case scenarios as they could not accurately predict or classify unfamiliar data lacking established rules.'),\n",
       " Document(metadata={'doc_id': 'bd4fe2a7-647a-45b8-b2e6-70975d5fdf55'}, page_content='RNNs and LSTMs, developed during this period, improved context-dependent predictions but struggled with long-span text predictions, limiting their effectiveness.'),\n",
       " Document(metadata={'doc_id': '2fd7b3ba-b7a4-473c-ae11-d6b0a94911b4'}, page_content='The rise of the transformer architecture in 2017 revolutionized NLP by outclassing RNNs and LSTMs in generalization, context capture, and data processing. This allowed for better understanding of longer data sequences and broader task performance, although early models faced limitations due to insufficient datasets and computational resources.'),\n",
       " Document(metadata={'doc_id': '7f915b4b-9cf7-4cc7-bda0-b657ddfde6ed'}, page_content='Researchers and experts are interested in these developments, but they have not gained attention from the general public due to issues with user-friendliness and accuracy, preventing commercialization.'),\n",
       " Document(metadata={'doc_id': '2e7b6471-e138-40db-bb13-a286245d771d'}, page_content='LLM NLP advancements were significantly propelled by OpenAI\\'s GPT-3 launch in 2020, which, through training on vast datasets, improved the accuracy and comprehensiveness of NLP responses, advancing the pursuit of \"true\" AI.'),\n",
       " Document(metadata={'doc_id': '19c22dbf-04ce-4c8e-9c93-4b349d82ad82'}, page_content='LLMs have made NLP models accessible to non-technical users through natural-language prompts, democratizing the technology. This shift was driven by advancements such as neural networks, attention mechanisms, transformers, and progress in unsupervised and self-supervised learning. Understanding these concepts is essential for grasping how LLMs function and for developing new LLMs. Neural networks are machine learning algorithms inspired by the human brain.'),\n",
       " Document(metadata={'doc_id': '2594ae9a-9959-4c1f-9882-800188bd7d3d'}, page_content=\"Artificial neural networks are composed of neurons organized into layers, including an input layer that receives data, and hidden layers that perform functions such as data identification, classification, and generation, which contribute to the model's output.\"),\n",
       " Document(metadata={'doc_id': '337fab1d-3ffb-4e84-8dd8-e3c21a60fb15'}, page_content='The output layer of LLMs produces predictions based on input, originally utilizing simpler neural network architectures like RNNs and LSTMs, which effectively consider context and relationships between distant words in a sequence.'),\n",
       " Document(metadata={'doc_id': '2abd27c6-236c-46a5-8520-5549bfe4baee'}, page_content='Neural networks, particularly RNNs and LSTMs, excel in NLP tasks like sentiment analysis and text classification by memorizing past data for more accurate outputs. Their key advantage is the ability to autonomously learn and create rules from data, unlike traditional rule-based systems that require predefined rules.'),\n",
       " Document(metadata={'doc_id': '52aa09d6-0a86-47e6-8078-f12c57f63dbe'}, page_content='Representation learning refers to the process by which neural networks identify hidden patterns or features in data, inspired by human learning mechanisms.'),\n",
       " Document(metadata={'doc_id': 'fdb21d9c-f731-4170-b1e4-360745ac964a'}, page_content='The model identifies that plural nouns typically have the suffix \"-s\" from the training tokens and applies this representation to new data following a similar distribution.'),\n",
       " Document(metadata={'doc_id': '89880459-67f0-42fb-aa14-73de83605840'}, page_content='The model is expected to accurately classify tokens such as “chairs” and “table” as plural or singular, even if it has not previously encountered these words.'),\n",
       " Document(metadata={'doc_id': 'c4fdbaed-2c7b-4751-a0a4-197cda82320a'}, page_content='The model adjusts its parameters for irregular nouns, ensuring that plural nouns are matched with plural verbs, which enhances generalization capabilities compared to rule-based systems and improves task performance.'),\n",
       " Document(metadata={'doc_id': '4d966bd1-505e-4dff-a47d-30d6c1053cc3'}, page_content='The effectiveness of representation extraction is significantly influenced by the number of neurons and layers in a network.'),\n",
       " Document(metadata={'doc_id': '80516c05-a691-42bb-9837-4fa0497ea7a8'}, page_content='Larger neural networks with multiple hidden layers allow for more complex representations, which is why deep learning is preferred in modern large language models.'),\n",
       " Document(metadata={'doc_id': '2ff4099e-0840-4dd5-be2a-a57d368f343c'}, page_content='The comparison highlights that deep learning neural networks consist of multiple hidden layers, allowing them to process complex inputs and outputs more effectively than single-layer networks. The development of deep neural networks has become viable due to advancements in hardware capable of managing extensive computational demands.'),\n",
       " Document(metadata={'doc_id': '7a7f49ab-e736-4634-97d2-d9e050868ae3'}, page_content='The introduction of the first GPU by NVIDIA around 1999 and the popularization of GPUs in deep learning by the successful CNN AlexNet in 2012 enabled significant advancements in the field.'),\n",
       " Document(metadata={'doc_id': 'b6156e2f-2500-4a95-aa9e-7b2ccbd65653'}, page_content='GPUs have significantly advanced deep learning, leading to the development of transformers, a new type of neural network that has transformed NLP. While RNNs and LSTMs have benefits, they struggle with long data sequences and context management, making them less effective for complex tasks like machine translation. Transformers, introduced in 2017 by Vaswani et al., have addressed these limitations.'),\n",
       " Document(metadata={'doc_id': 'b50fcc46-febd-4d3a-af0d-4b3bb38d81ba'}, page_content='The paper \"Attention is All You Need\" introduces the Transformer architecture, based solely on attention mechanisms, which eliminates the need for recurrence and convolutions in processing sequential data. This approach made transformers superior for machine translation and various NLP tasks by enabling models to focus on significant tokens in the input. Attention weights are calculated for each token, determining their importance in generating output. Self-attention layers assess the relevance of tokens to each other, allowing the model to emphasize or de-emphasize words as needed, thereby improving context handling and overall performance.'),\n",
       " Document(metadata={'doc_id': '1bda9394-c503-431d-b5bd-27b99f2922d0'}, page_content='Please provide the table or text chunk you would like summarized.'),\n",
       " Document(metadata={'doc_id': '18b9a229-9632-4c89-8eb6-fda5bab16772'}, page_content='Tokens with few or irrelevant relationships to other tokens are deemed less important or unimportant for specific tasks.'),\n",
       " Document(metadata={'doc_id': 'aa057d1b-4e1a-48ef-921f-bdcb9d7c54d8'}, page_content='Understood. Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': 'b35c7ef9-c814-4f64-9baa-457df89ac5f8'}, page_content='Attention mechanisms improve model accuracy and speed in generating outputs compared to RNNs and LSTMs, which process data sequentially and take longer, as they handle inputs one word at a time.'),\n",
       " Document(metadata={'doc_id': 'eff0a2f5-4611-471f-a095-5f14b17d2b8f'}, page_content='Processing a single Wikipedia page with LSTMs and RNNs would take a significant amount of time.'),\n",
       " Document(metadata={'doc_id': 'f19d6c72-5d6f-40aa-84ff-4dea45ef2679'}, page_content='Transformers process data in parallel, allowing them to read all input tokens simultaneously, resulting in faster performance on NLP tasks compared to LSTMs and RNNs.'),\n",
       " Document(metadata={'doc_id': '28b6ea3c-9d2b-419a-9ec3-d3914695cbda'}, page_content='Sequential data processing in LSTMs and RNNs ensures word order recognition by processing one word at a time, while transformers process data non-sequentially and lack initial awareness of word order.'),\n",
       " Document(metadata={'doc_id': 'd4366bd1-b541-4b15-8f62-49ecfa763dc4'}, page_content='Mary owned a small lamb.'),\n",
       " Document(metadata={'doc_id': '43aa9f64-6f24-4901-9114-319b319064a1'}, page_content='Mary had a little lamb.'),\n",
       " Document(metadata={'doc_id': '779b2b13-729f-4eb1-8f6f-c7fe4d60ee51'}, page_content='Mary had a little lamb.'),\n",
       " Document(metadata={'doc_id': '2ba9a51a-ce2b-4404-9d8c-fdd7f7f51f92'}, page_content='Sentence (2) illustrates how minor word order changes can alter meaning, while sentence (3) highlights that more significant changes can lead to nonsensical constructions. Transformers address this by utilizing positional encodings, which are vectors added to tokens to preserve their positional information.'),\n",
       " Document(metadata={'doc_id': '5be6a82e-1cd8-431f-aa83-9c7dcfbb139f'}, page_content='Parameters can be either fixed or trainable based on the preference for model refinement during training.'),\n",
       " Document(metadata={'doc_id': 'ea0f6217-1fcc-48ed-b906-125d64edddf9'}, page_content=\"The document is a beginner's guide to large language models (LLMs), likely containing illustrations and explanations to help new users understand the concepts and workings of LLMs.\"),\n",
       " Document(metadata={'doc_id': 'c91289f7-30fc-4e66-887d-8489aff82bba'}, page_content='Sequential data processing in LSTM is enhanced by mechanisms introduced in transformer models, notably Google’s BERT, released in 2018, which utilizes attention mechanisms for improved language representation.'),\n",
       " Document(metadata={'doc_id': 'cc8a1034-346d-4906-acc2-e468c0939e36'}, page_content='The model is a masked language model (MLM) trained on sentences with masked tokens.'),\n",
       " Document(metadata={'doc_id': '0f34aa56-3f87-41dc-ac22-330fd7f07501'}, page_content='BERT predicts masked tokens by analyzing surrounding words, using context from both sides, making it bidirectional and more accurate than unidirectional models, which only consider left-side context.'),\n",
       " Document(metadata={'doc_id': '68fa84d9-0639-49fa-a006-be96e2e6a40a'}, page_content='Bidirectional models like BERT outperform unidirectional models in NLP tasks by improving context and meaning understanding, exemplified in enhancing Google search accuracy.'),\n",
       " Document(metadata={'doc_id': '74d56a50-557f-4b63-b36a-b1f9ca93113b'}, page_content=\"Other companies are developing their own large language models influenced by Google's BERT and advancements in the field.\"),\n",
       " Document(metadata={'doc_id': 'da46bccd-0c64-418a-ada6-75f0e30c1461'}, page_content='The table lists models developed before the release of GPT-3 in June 2020.'),\n",
       " Document(metadata={'doc_id': 'e643d0e8-e0fc-4096-bd29-aaaa7158918b'}, page_content='Table 2 outlines the timeline of significant releases of large language models (LLMs).'),\n",
       " Document(metadata={'doc_id': '3720238c-937b-404d-ad58-124074f3eb3c'}, page_content='The table outlines the release timeline of two large language models: GPT-2 by OpenAI in 2019, which generates human-like text for various NLP tasks, and RoBERTa by Facebook in 2019, which enhances NLP task performance through extensive training on larger datasets.'),\n",
       " Document(metadata={'doc_id': '44b45a10-034b-4358-acdd-97691db0fc89'}, page_content=\"DeBERTa, developed by Microsoft in 2020, enhances NLP tasks by separating the encoder and decoder components of the model. GPT-3, created by OpenAI in 2020, is an advanced version of GPT-2, leveraging a larger dataset for superior output quality and demonstrating the potential of large language models (LLMs) in few-shot learning. This success has led to the development of even larger models such as Megatron-Turing with 530 billion parameters, PaLM with 540 billion, and WuDao 2.0 with 1.75 trillion parameters. Additionally, BERT's impact stemmed from its bidirectional training and use of unsupervised learning techniques.\"),\n",
       " Document(metadata={'doc_id': '4239e17d-b2db-4025-a176-b9aa731eb8ef'}, page_content='Unsupervised learning involves machine learning algorithms detecting patterns in unlabeled datasets without human intervention.'),\n",
       " Document(metadata={'doc_id': '3bba9dd2-d700-4411-85ee-0afec7c8ad11'}, page_content='BERT independently extracted patterns from plain-language Wikipedia pages during training, exemplifying pure AI capabilities.'),\n",
       " Document(metadata={'doc_id': 'faf099b4-2255-4222-8891-f939c1eed72d'}, page_content='Unsupervised learning models utilize feedback loops for performance improvement by receiving feedback on predictions. In contrast, self-supervised learning models do not use feedback loops but instead rely on automatically generated supervisory signals from data for training.'),\n",
       " Document(metadata={'doc_id': '1ec0861e-25f2-4bca-9b7e-23d326789f68'}, page_content='Unsupervised and self-supervised learning techniques autonomously generate labels and extract features, reducing the need for human intervention, in contrast to supervised learning.'),\n",
       " Document(metadata={'doc_id': 'd1a6bed8-b620-4c91-a772-623c51711929'}, page_content='Companies can train models efficiently without the need for extensive data labeling or human feedback on outputs.'),\n",
       " Document(metadata={'doc_id': '0c95300d-1a5c-4845-925a-8006b2e0072f'}, page_content='Self-supervised learning is the primary method for pre-training large language models, often suggested for enterprises. GPT and BERT are both advanced NLP models but have different architectures and applications.'),\n",
       " Document(metadata={'doc_id': 'f98e094b-a818-4f2f-b89f-930eebac7614'}, page_content='GPT is a generative model focused on predicting the next word in a sentence, suitable for language generation tasks like text completion and summarization, using a unidirectional transformer. In contrast, BERT is a discriminative model designed for classifying sentences or tokens, using a bidirectional transformer to consider both left and right context, making it effective for tasks such as sentiment analysis and named entity recognition. Both models are influential in the field of NLP.'),\n",
       " Document(metadata={'doc_id': '31323d18-a0f8-4ef9-86cc-8e2cc17e5499'}, page_content='Researchers and practitioners often combine different models to achieve optimal results for specific tasks.'),\n",
       " Document(metadata={'doc_id': '2a411e86-9586-4ce6-a910-2c7955705ad0'}, page_content='Enterprises can leverage large language models to automate various language-related tasks, including content generation, data analysis, and customer support. These models enhance efficiency and accuracy, enabling faster code writing in tech firms and reducing errors in fraud detection for banks.'),\n",
       " Document(metadata={'doc_id': '5952ddc2-4f00-4e7f-8a4a-86dd26746023'}, page_content='LLMs can automate tedious tasks, enabling employees to focus on more critical activities and enhancing productivity. In healthcare, LLMs can generate synthetic clinical data to expedite medical research. Other applications include sentiment analysis for audience insights and customer churn prediction to retain clients.'),\n",
       " Document(metadata={'doc_id': '291e7607-bf8b-4c5c-847d-b482a48800cb'}, page_content='Enterprises can leverage large language models (LLMs) to create new, conversation-based services, including specialized AI companions. However, they should be cautious of common challenges associated with LLMs, which apply to customization, fine-tuning, or building models from scratch.'),\n",
       " Document(metadata={'doc_id': '57370704-364e-4f02-978f-fec035e12b55'}, page_content='Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': '3e6809b9-8ee7-4c5c-b82c-dc6a2c534158'}, page_content='Large language models are susceptible to adversarial examples, which are specially crafted inputs that can mislead them, posing security risks for sensitive sectors such as healthcare and finance.'),\n",
       " Document(metadata={'doc_id': 'e33f3a93-ceab-4df7-bf2a-67d58a863629'}, page_content='Please provide the text or table chunk you would like summarized.'),\n",
       " Document(metadata={'doc_id': '69ec9b9a-574d-4121-8975-b68119c6715a'}, page_content='Large language models often lack interpretability, making it challenging to understand their decision-making processes and assess their accuracy and bias. This issue is particularly significant in high-stakes applications like fraud detection and in industries such as healthcare and finance that demand transparency.'),\n",
       " Document(metadata={'doc_id': 'c49d92a9-89f4-4dc7-947e-917a7899e929'}, page_content='The provided content seems to be incomplete; please provide the relevant table or text for summary.'),\n",
       " Document(metadata={'doc_id': 'd2de6dd6-cf3a-4eab-8f04-d0266c588d48'}, page_content='Large language models can produce non-specific and generic responses.'),\n",
       " Document(metadata={'doc_id': 'b443c8bd-692d-44a5-af08-1fab9d86fc8c'}, page_content='LLMs may struggle with understanding human input, but techniques like Reinforcement Learning from Human Feedback (RLHF) can enhance their performance. However, they may still reproduce training data, leading to ethical concerns and potential copyright issues for businesses.'),\n",
       " Document(metadata={'doc_id': '2d4577d3-c635-4811-b314-fc551d2f8ac7'}, page_content=\"Please provide the table or text chunk you'd like summarized.\"),\n",
       " Document(metadata={'doc_id': 'e9f880ce-3035-4c01-81b9-d347c0447896'}, page_content='Using large language models (LLMs) for important decision-making raises ethical concerns, particularly regarding their role in candidate selection and the potential displacement of human workers in white-collar jobs.'),\n",
       " Document(metadata={'doc_id': '4e4e7afc-6bc5-4ff0-9d28-226876a17656'}, page_content='Large language models (LLMs) can generate inappropriate and harmful content due to their training on extensive internet text, which may include toxic or biased information. Enterprises looking to build proprietary LLMs face challenges related to computing power, storage, expertise, and funding. Instead of building from scratch, many businesses, especially those not focused on AI, prefer to customize existing pre-trained models (PLMs), which involves selecting an appropriate foundation model as the first step.'),\n",
       " Document(metadata={'doc_id': '2ab6622f-5022-48d0-90d1-bdc687286789'}, page_content='Key factors to consider include ideal model size, training tasks and datasets, and LLM providers.'),\n",
       " Document(metadata={'doc_id': '432efd20-0617-4ec6-8da2-318c974e8b5b'}, page_content='It seems there was an error in your request, as no specific table or text was provided for summarization. Please provide the text or table you would like summarized.'),\n",
       " Document(metadata={'doc_id': '80ed85b5-2b4e-412d-99f8-d5a9fb093f82'}, page_content='Base models can be fine-tuned on specific corpora for tailored applications, such as adapting text classification models for sentiment analysis or training with legal records to enhance legal terminology proficiency.'),\n",
       " Document(metadata={'doc_id': '4bd47768-64b4-4586-9c47-1b9bf3b3cca5'}, page_content='Models can be optimized using Reinforcement Learning from Human Feedback (RLHF), which updates them based on human feedback on predictions. RLHF is effective, as demonstrated by its use in ChatGPT. Enterprises may also customize base models using parameter-efficient techniques like adapters and p-tuning.'),\n",
       " Document(metadata={'doc_id': '80c9ccf6-1cd3-47e9-a26e-52b3e8d21d36'}, page_content='Customization of models enhances accuracy when the base model is trained on similar tasks, like using a text classification model for sentiment analysis. Large language models (LLMs) excel in various language tasks but require careful evaluation based on training data quality, model size, and inference speed. For effective learning, training data must be diverse, representative, and annotated for supervised learning.'),\n",
       " Document(metadata={'doc_id': '43f3651c-6320-4905-9bdf-afba3fc6f197'}, page_content='Larger models typically yield better performance but demand more computational resources, prompting researchers to balance model size and performance based on task requirements and available resources.'),\n",
       " Document(metadata={'doc_id': '37bdbb70-8a17-4900-b00c-e91d121d8f35'}, page_content='Larger models are more susceptible to overfitting, negatively impacting generalization on new data, and inference speed is a key consideration for real-world deployment of LLMs.'),\n",
       " Document(metadata={'doc_id': 'd75302a1-154a-492e-b56d-b97b515a2ebf'}, page_content='Faster inference time allows LLMs to efficiently process large data volumes.'),\n",
       " Document(metadata={'doc_id': 'd6897eba-0793-4a63-bd74-0195c933bee2'}, page_content=\"Techniques like pruning, quantization, and distillation are used to enhance LLM efficiency. LLM performance is assessed via benchmarks like GLUE, SuperGLUE, and CoQA. Significant companies in LLM development include OpenAI, Google, and Meta, with notable models like GPT-3 and PaLM. LLM apps, highlighted by OpenAI's ChatGPT, rapidly gained popularity, with other startups like Jasper and Copy.ai also achieving substantial user bases.\"),\n",
       " Document(metadata={'doc_id': '9d9919b6-49bd-44f9-bade-1714bb01d8ad'}, page_content='Figure 4 illustrates a natural-language prompt that users can input into Copy.ai to create a blog post outline.'),\n",
       " Document(metadata={'doc_id': '5c215fdc-f925-406f-9cbc-5dce1cb21d2e'}, page_content=\"The content relates to a beginner's guide on large language models, featuring a prompt example and a blog outline section.\"),\n",
       " Document(metadata={'doc_id': 'b19e841a-f327-480e-865c-8a2822e25bdf'}, page_content=\"The blog discusses the applications of OpenAI's GPT-3 in various writing tools, comparing it to GPT-3 powered apps like Copy.ai, Lex, Jasper, and Frase.io. It highlights the differences between OpenAI's GPT-3 and third-party applications, forecasts the future of these writing tools, and mentions successful LLM-powered apps like Grammarly and GitHub Copilot. The upcoming part will explore additional uses of LLMs in enterprises for specialized applications, providing examples of customization for industries such as finance, healthcare, and telecommunications to encourage innovative uses of LLMs.\"),\n",
       " Document(metadata={'doc_id': '2f8de51e-3de2-4454-a989-bc030a8059e2'}, page_content='The document is an informational guide about large language models, provided by NVIDIA, which disclaims any warranties regarding product functionality, condition, or information accuracy.'),\n",
       " Document(metadata={'doc_id': 'b81a94e5-6487-46c8-8125-67a0b0175dad'}, page_content=\"NVIDIA disclaims liability for any consequences resulting from the use of its information and does not commit to developing or delivering any material. The company reserves the right to modify the document without notice, and customers should verify the latest information before placing orders. Products are sold under NVIDIA's standard terms and conditions, and any customer terms are explicitly rejected.\"),\n",
       " Document(metadata={'doc_id': 'ac61beb3-951b-466b-8bbe-eda1fc9ad056'}, page_content='This document disclaims any contractual obligations related to NVIDIA products, which are not authorized for use in critical applications that may lead to personal injury or property damage. Customers assume all risk associated with the use of these products and are responsible for testing and ensuring suitability for their intended applications. No warranties or representations are made regarding the products, and NVIDIA disclaims liability for any damages resulting from their use. Reproduction of the document’s information requires prior written approval from NVIDIA. All materials are provided \"as is\" without warranty. NVIDIA\\'s liability is limited as per the Terms of Sale for the products.'),\n",
       " Document(metadata={'doc_id': '0f3fb82d-4711-4150-99ec-21c21360c9b3'}, page_content='The text indicates that company and product names may be trademarks of their respective companies, and it is copyrighted by NVIDIA Corporation in 2023.'),\n",
       " Document(metadata={'doc_id': '9f89062c-c4be-4e29-837b-9942b4813f80'}, page_content='All rights to the content are reserved.'),\n",
       " Document(metadata={'doc_id': '409259a3-b2d4-4eb6-a72a-7b55ab842b24'}, page_content='NVIDIA Corporation discusses large language models (LLMs) and their foundations in transformer architecture. LLMs are self-supervised, trained to predict the next word in a sequence and require vast amounts of text and computational resources. The evolution of language modeling is outlined, starting from static word embeddings in 1990 to transformers in 2017 and contextual embeddings by 2018. Static embeddings like word2vec have limitations, which LLMs aim to overcome by using transformers that utilize attention mechanisms.'),\n",
       " Document(metadata={'doc_id': 'b80d68c1-c87b-40a8-8237-8ab1d6523645'}, page_content='Static word embeddings do not capture the variability of word meanings in different contexts, as illustrated by the example of \"it.\" Contextual embeddings are needed to represent meanings accurately.'),\n",
       " Document(metadata={'doc_id': '87837ea1-759f-4286-a70c-59f6380443e1'}, page_content='Contextual embeddings assign different vector representations to words based on their context, with the attention mechanism allowing for selective integration of information from neighboring words to compute these embeddings. This approach helps determine what a pronoun refers to in a sentence by weighing information from its context.'),\n",
       " Document(metadata={'doc_id': '65e93e3a-ed1b-4539-96c4-10aad2f6b03d'}, page_content='Attention is a technique that enhances token representations by considering prior words based on their relevance to the current token. This is achieved through weighted sums of embeddings, influenced by similarity scores. Each embedding has a query and key, allowing for contextual adjustments, resulting in different meanings for words depending on their context.'),\n",
       " Document(metadata={'doc_id': '8dad2e5a-16f5-46d1-9288-f679004da384'}, page_content='Transformers and Large Language Models utilize a mechanism based on attention to compute embeddings, with multiple transformer blocks processing input tokens. Language models predict the next word through a pretraining phase on vast text datasets, followed by application to specific tasks. Model components include composite embeddings, a language modeling head for output probabilities over a vocabulary, and a sequence of feedforward and normalization layers.'),\n",
       " Document(metadata={'doc_id': 'e2000480-39e3-4293-9f3b-b36d62894c6d'}, page_content='No table or text provided for summarization.'),\n",
       " Document(metadata={'doc_id': '04bac082-0a34-467c-87be-14596e4739c3'}, page_content='A corpus of text is a structured collection of written or spoken material used for linguistic analysis and research.'),\n",
       " Document(metadata={'doc_id': '0f6dc0e7-0a88-4f7d-b5b2-151c43333d6e'}, page_content='The text references a process that occurs at each time step t, but no additional details are provided.'),\n",
       " Document(metadata={'doc_id': 'abdc4fa1-dc2c-4079-87b1-96a9061e6d67'}, page_content='The text discusses the training of a language model using gradient descent to minimize prediction error with a focus on cross-entropy loss. The model should assign high probabilities to true next words, and the loss increases when probabilities are too low. It outlines the training process for transformer models using pretraining data from various sources like the web, highlighting that the model learns general linguistic patterns, such as recognizing the presence of canines.'),\n",
       " Document(metadata={'doc_id': '8f31e3a5-7cb5-4b28-b463-8112bf9e9d61'}, page_content='The text discusses large language models (LLMs) and their training, emphasizing the importance of pretraining on extensive text for knowledge acquisition. It outlines various architectures for LLMs, including decoders (e.g., GPT, Claude), encoders (e.g., BERT), and encoder-decoders (e.g., Flan-T5). The concept of conditional generation, where text is generated based on previous text, is highlighted.'),\n",
       " Document(metadata={'doc_id': '1e93e94e-8d0a-4cea-94b5-c9b55eb7230f'}, page_content='Completion Text is a category related to input/output processing, often used in natural language tasks such as text generation, summarization, and dialogue systems. It involves generating coherent and contextually relevant content based on given prompts.'),\n",
       " Document(metadata={'doc_id': 'e804f822-7420-4cb1-966d-c846f6a49bef'}, page_content='The text describes the use of conditional generation for tasks like sentiment analysis, exemplified by the string \"I like Jackie Chan\" and setting up a prompt for the language model to identify the sentiment.'),\n",
       " Document(metadata={'doc_id': 'b2820a33-4e26-4097-8580-3c80408d8dc4'}, page_content='The text discusses using language models for tasks like sentiment analysis and question answering, illustrating this with examples of conditional generation and a specific query about the author of \"The Origin of Species.\"'),\n",
       " Document(metadata={'doc_id': '9902190f-50cd-4335-9b01-8d7298ad2e0c'}, page_content='Please provide the table or text chunk for summarization.'),\n",
       " Document(metadata={'doc_id': 'd155f1a0-1d39-4420-a0d9-c9a2f94f5a01'}, page_content='The table suggests predicting the next word after the question \"Who wrote the book \\'The Origin of Species\\'?\" based on the context provided.'),\n",
       " Document(metadata={'doc_id': '01830fe6-9b2a-464b-b86e-8a68f8f48625'}, page_content='Various types of encoders are available.'),\n",
       " Document(metadata={'doc_id': 'c87232e9-4e2f-48ee-9da0-ff5b2056ef71'}, page_content='Masked Language Models (MLMs), particularly the BERT family, predict words based on surrounding context and are often fine-tuned for classification tasks. Encoder-Decoder models are used for tasks like machine translation and speech recognition. Additional techniques include instruction fine-tuning, preference alignment, and prompt engineering.'),\n",
       " Document(metadata={'doc_id': 'd0855928-1203-444f-9155-669f83655ceb'}, page_content='CS224N is a course focused on Natural Language Processing with deep learning methods, covering topics such as word embeddings, neural networks, sequence models, and attention mechanisms. It emphasizes practical applications and includes hands-on projects.'),\n",
       " Document(metadata={'doc_id': '1703c3bb-5e28-43a2-8439-4c310667ae5a'}, page_content='Large Language Models (LLMs) can be applied to new tasks using pretrained models but pose risks such as hallucination, where AI generates false or misleading information. This raises concerns about the reliability of chatbots and their potential to represent information incorrectly.'),\n",
       " Document(metadata={'doc_id': '382a5180-7bce-4c87-97a2-53bf8714f607'}, page_content='Air Canada lost a court case after its chatbot generated false information about policies, with the airline claiming the chatbot should be held liable.'),\n",
       " Document(metadata={'doc_id': 'cdec7be5-01a3-4160-b2ee-68b279b76391'}, page_content='Current research on addressing hallucination involves Retrieval-Augmented Generation (RAG), where information is retrieved from high-quality sources to aid language model responses. Additionally, legal actions are underway against OpenAI for alleged mass copyright infringement related to the use of copyrighted novels and articles, including a lawsuit from The Times regarding AI training on millions of its articles.'),\n",
       " Document(metadata={'doc_id': 'c276e32c-b76a-43f2-aa26-7e3ddb3b8eab'}, page_content=\"The text discusses concerns surrounding AI technologies, focusing on the impact of their misuse, such as the trauma experienced by contractors screening harmful content for OpenAI's chatbot and the proliferation of misinformation in U.S. elections from chatbots. Additionally, it highlights the growing interest in the ethics of large language models (LLMs), evidenced by increasing submissions to the FAccT conference from various sectors from 2018 to 2022. Lastly, it outlines educational goals related to AI, including skills in regular expressions, classification, search engines, and chatbot development.\"),\n",
       " Document(metadata={'doc_id': '10ba17e4-1505-4e2a-b44d-381c3015280e'}, page_content='CS224N: Natural Language Processing with Deep Learning focuses on algorithmic internals such as transformers, GPT, parsing, and machine translation, covering detailed technical aspects of these applications.'),\n",
       " Document(metadata={'doc_id': '75ad9d0a-23c4-498b-b3eb-8f9d556ecff9'}, page_content='CS224C focuses on applying machine learning and social science theories to analyze human behavior and societal issues, including topics like hate speech and misinformation. CS224S is centered on spoken language technology, particularly dialogue systems. CS336 covers the comprehensive process of language model creation, from data gathering to model training and evaluation, with an application requirement.'),\n",
       " Document(metadata={'doc_id': '7c14a80c-dcd9-4182-bfe1-c12d1cf27fda'}, page_content='CS 246 focuses on mining massive data sets. Upcoming courses include CS 224V, which covers topics related to conversational virtual assistants and LLMs, such as knowledge growth, hallucination prevention, evaluation, task control, persuasion, multilingual support, and interface integration. CS 329X emphasizes human-centered design in NLP, focusing on human-in-the-loop algorithms, fairness, and accessibility.'),\n",
       " Document(metadata={'doc_id': '1c3c7c95-89e2-4e4e-b72b-a1300f8b831d'}, page_content='CS329R focuses on combining natural language processing techniques with social psychology regarding race to create systems that tackle important societal challenges. Additionally, it offers engaging courses in linguistics and communication, both this spring and in the following year.')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d4805629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '3519cb02-3c5d-476c-b73a-fb09c51d3262'}, page_content=\"I'm unable to provide a description of the image as requested. However, if you need assistance with the concepts related to the transformer architecture or specific aspects of your research paper, feel free to ask!\"),\n",
       " Document(metadata={'doc_id': 'e94d0e79-ab7b-4450-bdf6-37a3540f2b88'}, page_content=\"I can't determine the content of the image you provided, but if it's part of a research paper explaining transformers architecture, it likely includes visuals like diagrams, equations, and possibly bar plots demonstrating various aspects of the architecture's performance or comparison to other models.\\n\\nYou might expect to see:\\n\\n1. **Diagrams**: Illustrations of the transformer model itself, detailing components like the encoder and decoder, attention mechanisms, and layer structures.\\n\\n2. **Bar Plots**: Graphs comparing performance metrics, such as accuracy or training time across different transformer architectures or variations, highlighting key differences.\\n\\n3. **Legend or Labels**: Each graph would likely have clearly labeled axes indicating the metrics being compared, with a legend explaining what each bar represents.\\n\\n4. **Color Coding**: Different colors for bars may signify different models or datasets used in the experiments.\\n\\nThese elements collectively contribute to the understanding of the transformer architecture’s efficacy and design choices.\"),\n",
       " Document(metadata={'doc_id': '7734506b-b15c-40ed-b3cd-42ae97f64417'}, page_content='The image presents a visually engaging representation of concepts related to \"Agentic AI.\" \\n\\n### Left Side:\\n- **Background**: A dark green chalkboard-like backdrop.\\n- **Illustration**: A silhouette of a human head, facing right, is depicted in white outline. This head is integrated with several directional arrows and lines that symbolize thought processes or information flow. \\n  - **Question Mark**: A yellow question mark is prominently displayed within the head, indicating inquiry or decision-making.\\n  - **Arrows**: Lines emanate from the head and point in various directions—some arrows are upward, while others curve and move sideways, denoting complexity in decision-making or the intertwining of various thought pathways.\\n\\n### Right Side:\\n- **Text Section**: This space is divided into two segments:\\n  - **Top Section**: The title \"AGENTIC AI\" is displayed in bold, white capital letters, creating a strong focal point.\\n  - **Bottom Section**: A subtitle reads, \"A Comprehensive Framework for Autonomous Decision-Making Systems in Artificial Intelligence,\" in a more detailed font but still prominent, set against a darker purple background. This further elaborates on the theme of the research paper.\\n\\n### Overall Impression:\\nThe design combines a playful yet serious approach, integrating a conceptual framework with a visual cue of human cognition, aimed at illustrating complex ideas in AI decision-making in an accessible manner. The combination of graphics and text functions effectively to draw attention and communicate the essence of the research.'),\n",
       " Document(metadata={'doc_id': '6a583262-655c-4c22-ae1b-7f0f44cebafe'}, page_content='The image presents a bar plot comparing the key performance metrics of two types of AI systems: Traditional AI and Agentic AI. \\n\\n### Key Features of the Bar Plot:\\n\\n1. **Title**: The title at the top states \"Key Performance Metrics of Agentic AI Systems,\" indicating the focus of the data presented.\\n\\n2. **Axes**:\\n   - The **Y-axis** represents the performance metrics measured in percentages, ranging from 0 to 100.\\n   - The **X-axis** categorizes four key performance metrics: \\n     - Decision Accuracy\\n     - Response Time\\n     - Adaptation Rate\\n     - System Reliability\\n\\n3. **Bars**:\\n   - **Color Coding**: \\n     - The bars for Traditional AI are colored **blue**.\\n     - The bars for Agentic AI are colored **orange**.\\n   - Each metric has two bars side by side:\\n     - The blue bars consistently show lower values for Traditional AI metrics.\\n     - The orange bars typically exceed the blue ones for all metrics.\\n   \\n4. **Data Values**:\\n   - **Decision Accuracy**: \\n     - Traditional AI: 82.5%\\n     - Agentic AI: 94.8%\\n   - **Response Time**:\\n     - Traditional AI: 75.3%\\n     - Agentic AI: 92.1%\\n   - **Adaptation Rate**:\\n     - Traditional AI: 68.7%\\n     - Agentic AI: 88.9%\\n   - **System Reliability**:\\n     - Traditional AI: 85.2%\\n     - Agentic AI: 93.4%\\n\\n5. **Overall Comparison**: The plot clearly shows that Agentic AI outperforms Traditional AI across all performance metrics, with particularly significant gains seen in Decision Accuracy and Response Time.\\n\\nThis detailed visual representation effectively conveys the advantages of Agentic AI systems compared to their traditional counterparts.'),\n",
       " Document(metadata={'doc_id': 'b64abc2d-f6ad-49fd-a59e-6a4464af7250'}, page_content='The image is a bar graph illustrating various metrics of improvement, expressed as percentages. The title at the top reads \"Improvement (%)\" in a large, bold font. \\n\\nThere are five bars, each representing a specific metric:\\n\\n1. **Operational Efficiency** - This bar is the tallest, reaching 65.8%. It is depicted in a solid blue color and is positioned farthest to the left.\\n   \\n2. **Cost Reduction** - The second bar, slightly shorter, reaches 42.3%. It is positioned next to the Operational Efficiency bar, also in blue.\\n\\n3. **Customer Satisfaction** - This bar stands at 58.7%, taller than the Cost Reduction bar but shorter than Operational Efficiency and is placed next in line.\\n\\n4. **Innovation Rate** - The fourth bar is the second tallest, measuring 71.2%. It is prominently displayed following the Customer Satisfaction bar.\\n\\n5. **Time-to-Market** - The shortest bar, reaching 48.9%, is positioned on the far right and is also colored blue.\\n\\nEach bar is labeled at the bottom with the respective metric name, and the percentage values are displayed prominently at the top of each bar. The overall design presents a clear comparison of these five performance indicators, allowing for fast visual assessment of their relative improvements. The background is a light gradient, which highlights the bars effectively.'),\n",
       " Document(metadata={'doc_id': 'c0bdb18d-f35d-40e3-ae9d-6440991de32e'}, page_content='The image depicts a Creative Commons logo, which includes an encircled \"CC\" symbol, indicating that the content is licensed under Creative Commons. The logo features three distinct symbols:\\n\\n1. **BY (Attribution)**: This is represented by an icon of a person, indicating that users must give appropriate credit to the creator of the work.\\n  \\n2. **NC (Non-Commercial)**: Shown as a dollar sign with a slash through it, this symbol indicates that the work cannot be used for commercial purposes.\\n\\nThe logo is presented in a monochromatic scheme with a gray background and black symbols. Each component is clearly defined, ensuring readability and clarity. This logo is often found in research papers to inform readers of the licensing terms related to the use of the paper\\'s content.'),\n",
       " Document(metadata={'doc_id': 'd09b21a0-7c3b-442a-8b2e-61432bbd1ced'}, page_content=\"The image is a detailed diagram illustrating the architecture of a transformer model, commonly used in natural language processing. Here's a breakdown of its components:\\n\\n1. **Overall Structure**:\\n   - The diagram features two main sections representing the encoder and decoder parts of the transformer architecture, interconnected with arrows to indicate the flow of data.\\n   - The overall design is modular, showing cyclical patterns that repeat for multiple layers denoted as \\\\( N \\\\times \\\\), indicating the number of layers in both the encoder and decoder.\\n\\n2. **Encoder Side**:\\n   - Begins with **Input Embedding** at the bottom, which receives the model's input data.\\n   - The **Positional Encoding** is added to the input embeddings to provide information about the position of words in the sequence.\\n   - The first block consists of **Multi-Head Attention** followed by an **Add & Norm** layer, which normalizes the output.\\n   - This is followed by a **Feed Forward** neural network, again leading into another **Add & Norm** layer.\\n   - The entire block structure repeats \\\\( N \\\\) times on the encoder side.\\n\\n3. **Decoder Side**:\\n   - Similar in design to the encoder but starts with **Positional Encoding** and **Input Embedding**, shifting prior values for the outputs.\\n   - Features a **Masked Multi-Head Attention** mechanism to prevent the model from looking at future tokens during training, alongside an **Add & Norm** layer.\\n   - The subsequent layer mirrors the encoder with another **Multi-Head Attention** block that incorporates outputs from the encoder.\\n   - Each flow culminates in another **Add & Norm** followed by a **Feed Forward** structure.\\n\\n4. **Output Layer**:\\n   - At the top, the flow culminates in a **Linear** layer fed by prior layers' outputs, followed by a **Softmax** function to convert logits into probabilities for the output predictions.\\n\\n5. **Connections**:\\n   - Arrows throughout the diagram indicate input and output relationships between different components, demonstrating the model's flow from inputs to encoded representations and finally to predictions.\\n\\nThe diagram clearly illustrates the flow of information and the modular approach to building both the encoder and decoder in the transformer architecture.\"),\n",
       " Document(metadata={'doc_id': 'a4737277-7f72-4449-be6c-506b4b9eaf3f'}, page_content='The image presents two structured components of the Transformer architecture: \"Scaled Dot-Product Attention\" and \"Multi-Head Attention.\" \\n\\n### Scaled Dot-Product Attention (Left Side)\\n- **Components**: This section contains a sequence of boxes representing mathematical operations.\\n  - **Input Vectors**: At the bottom, there are three labeled vectors: `Q` (Query), `K` (Key), and `V` (Value).\\n  - **MatMul**: The first operation (labeled `MatMul`) applies matrix multiplication to `Q` and `K`, producing an intermediate result.\\n  - **Scale**: Above that, a box labeled `Scale` indicates a scaling operation, likely to adjust the magnitude of the result.\\n  - **Mask (opt.)**: The next box is labeled `Mask (opt.)`, suggesting an optional masking operation which alters the attention weights to control which parts of the sequences are attended to.\\n  - **SoftMax**: The SoftMax function follows, transforming the masked scores into probabilities.\\n  - **Final MatMul**: The last operation is another `MatMul`, combining the attention scores with the `V` vector to produce the final context vector.\\n\\n### Multi-Head Attention (Right Side)\\n- **Architecture Overview**: This section details the Multi-Head Attention mechanism.\\n  - **Linear Layers**: Three `Linear` boxes at the bottom indicate linear transformations applied to the `V`, `K`, and `Q` vectors, respectively.\\n  - **Concat**: Above, a `Concat` box signifies that the outputs of multiple attention heads will be concatenated.\\n  - **Scaled Dot-Product Attention**: Central to this section is another `Scaled Dot-Product Attention` box (highlighted in purple) that emphasizes the main computation for each head.\\n  - **h (Heads)**: Arrows indicate that this attention process is replicated for multiple heads, each processing their separate linear transformations of `V`, `K`, and `Q`.\\n\\nThis diagram serves as a concise visual representation of the sequential operations involved in both attention mechanisms, illustrating how data flows through the architecture to produce meaningful outputs in the context of natural language processing tasks.'),\n",
       " Document(metadata={'doc_id': '5376a41e-bf44-4525-8418-9cf1101c32b3'}, page_content='The image appears to feature a stylized graphic with an eye shape at its center. Within the eye, there are sequences of binary code (0s and 1s) arranged in three lines, creating a visual representation of digital information. The background is a dark color, possibly black or deep blue, which contrasts with the lighter shades of the eye and the binary numbers, giving it a modern and sleek appearance. \\n\\nThis design could symbolize themes related to vision or perception in the context of artificial intelligence and machine learning, particularly in relation to how transformers process data. The use of binary code may imply a focus on data representation and computing, reinforcing the theme of technology in the discussion of transformers. \\n\\nThe simplicity and abstract nature of the image suggest it may serve as a conceptual illustration rather than a detailed data visualization like graphs or bar plots.'),\n",
       " Document(metadata={'doc_id': '084a64e6-8907-4f4c-9036-f874d1421b55'}, page_content=\"I can't interpret the image you provided, but I can help describe the typical components you might find in a research paper explaining the transformers architecture.\\n\\nIn such papers, you would often see graphs or diagrams illustrating concepts like:\\n\\n1. **Transformers Architecture**:\\n   - Diagrams that show the layers of the Transformer, including the encoder and decoder stacks.\\n   - Connections between the layers, highlighting self-attention mechanisms.\\n\\n2. **Attention Mechanism**:\\n   - Visuals depicting the attention scores, often represented as heatmaps.\\n   - Bar plots comparing the effectiveness of self-attention versus other mechanisms.\\n\\n3. **Performance Metrics**:\\n   - Bar plots illustrating the performance of various models on benchmark datasets, with axes labelled with model types and performance scores (e.g., accuracy, F1 scores).\\n\\n4. **Training Curves**:\\n   - Line graphs showing loss and accuracy over epochs during training, indicating model convergence.\\n\\nIf you need specific information or details regarding a graph or component of the architecture, feel free to let me know!\"),\n",
       " Document(metadata={'doc_id': '23c2efcf-c5a2-4f82-83c4-9051c56c45b2'}, page_content='The image appears to display the Creative Commons licensing symbol. It features a circular logo on the left, representing the Creative Commons attribution (CC BY) license. The graphic consists of a light gray background with a black bar at the bottom. Within this black area, a small figure is depicted, symbolizing attribution.\\n\\nThere are no graphs, bar plots, or specific data visualizations related to the transformers architecture in this image. Instead, it focuses solely on the licensing information related to the content of the research paper.'),\n",
       " Document(metadata={'doc_id': 'e9006cb0-232f-4f3b-b1d3-0c0ad1a09bf3'}, page_content='The image presents a comparison of \"Real\" and \"Synthetic\" visuals, divided into four sections: two on the top representing real images and two on the bottom for synthetic images.\\n\\n**Top Row (Real):**\\n- **Left Image:** A corner of a room features a light green wall with a wooden counter displaying a vase of flowers. The arrangement consists of various colored blooms, likely in a clear glass or a simple ceramic vase, placed on a round table covered with a cloth.\\n  \\n- **Middle Image:** This depicts two giraffes in a natural setting. They are standing amidst a dense landscape of trees and dry grass, embodying a typical wildlife scene. The giraffes are partially obscured by foliage, emphasizing their natural habitat.\\n\\n- **Right Image:** Here, a snowy outdoor scene shows a group of people engaging in winter sports. Some individuals are skiing, while others, possibly children, are learning or playing in the snow. Trees are visible in the background, enhancing the winter landscape.\\n\\n**Bottom Row (Synthetic):**\\n- **Left Image:** Featuring a glass vase holding a bouquet of artificial flowers, this image closely mimics the look of the real flower arrangement above, but the textures and colors may appear overly perfect or vibrant.\\n\\n- **Middle Image:** Two synthetic giraffes, designed to resemble the real ones from the top row, are portrayed standing in a lush, green environment with stylized trees. The colors might appear more vivid or cartoon-like.\\n\\n- **Right Image:** A synthetic version of a winter scene shows human figures dressed in colorful winter attire walking across a striped surface, possibly suggesting a stylized or artificial landscape. While it conveys a similar theme to the top-right image, the portrayal may exhibit less realism and more artistic flair.\\n\\nThis layout illustrates contrasts between authentic and synthetic images, likely to discuss the capabilities of transformer models in generating realistic imagery.'),\n",
       " Document(metadata={'doc_id': 'e67bc250-0e3e-4d86-9c21-f7a3801e1c09'}, page_content='The image consists of a 3x3 grid containing photographs, each framed in red boxes to highlight specific elements. Here’s a detailed description of each section:\\n\\n1. **Top Left (Food Scene)**: This image shows a close-up of a table setting featuring a pizza topped with various ingredients, including what appears to be red sauce, cheese, and possibly vegetables. Accompanying the pizza are two champagne flutes filled with bubbly liquid, suggesting a celebratory or social occasion.\\n\\n2. **Top Center (Transportation)**: The second image captures a tram on a street, characterized by its distinct red and yellow color scheme. The scene includes a city backdrop with buildings lining the street, giving a sense of urban life and public transportation.\\n\\n3. **Top Right (Wildlife)**: This image features a zebra standing in a natural setting. The zebra’s striking black and white stripes are prominent, and it is positioned in a sandy environment, likely a savannah or similar habitat, indicating a wildlife scene.\\n\\n4. **Middle Left (Interior Scene)**: The fourth image shows a glimpse of an indoor space, possibly an office or meeting room. There are colored items on a wall that may be various charts or images, and a wooden table with chairs is visible.\\n\\n5. **Middle Center (Abstract)**: The fifth image is less clear, with an abstract pattern or noise resembling digital interference or static. The colors and shapes do not provide identifiable content and may indicate an experimental or illustrative purpose.\\n\\n6. **Middle Right (Data Visualization)**: The final image resembles a heatmap or some form of data visualization. The star-like pattern is colored in gradients from blue to yellow, suggesting it might represent numerical data or frequency distribution.\\n\\nThis composition illustrates a range of subjects from culinary delights to urban transport, wildlife, indoor settings, abstract concepts, and data representations, effectively demonstrating the variability in visual data or potential applications within the context of transformers architecture in machine learning.'),\n",
       " Document(metadata={'doc_id': '40940fe9-ce02-4097-a921-df638f6fa4c1'}, page_content='The image illustrates a flowchart depicting the process of transfer learning using a Convolutional Neural Network (CNN) model. \\n\\n1. **Left Side**:\\n   - There are two sections at the top labeled \"ImageNet\" and \"RSI,\" each showing a collage of images. These represent the datasets used, with ImageNet being a widely used benchmark dataset containing various classes of images.\\n\\n2. **CNN Model**:\\n   - The upper and lower parts of the diagram feature a similar structure representing the CNN model. Each section consists of multiple vertical bars or blocks, which symbolize the layers of the CNN. These layers are colored in alternating segments (e.g., red, yellow, blue), indicating different operations or feature extraction processes taking place within the model.\\n\\n3. **Transfer Learning Arrow**:\\n   - A downward arrow labeled \"Transfer learning\" connects the upper CNN model to the lower section, indicating the process of adapting the pretrained model for a different task.\\n\\n4. **Output Classes**:\\n   - At the upper end of the flowchart, there’s a note that states \"1000 classes (Head removed).\" This suggests that the original model, trained on ImageNet, categorizes images into 1000 classes, and the classification head has been removed as part of the transfer learning process.\\n\\n5. **New Head**:\\n   - The lower section ends with an output indicating \"2 classes (New head).\" This implies that after fine-tuning the CNN on the RSI dataset, it is now configured to classify into just two specific categories: \"Real\" and \"Synthetic.\"\\n\\nOverall, the image effectively conveys the workflow of adapting a CNN model from one dataset to another through transfer learning, highlighting key components and outputs.'),\n",
       " Document(metadata={'doc_id': 'fbcdbd09-0cbe-4234-bd0e-96e698a90243'}, page_content='The image appears to be a collage featuring various examples of research papers related to different synthesis techniques in computer vision. The layout is organized into three main columns, each highlighting specific categories of synthesis methods.\\n\\n1. **Left Column (Image-to-Image Synthesis)**:\\n   - The first section lists several models along with their references, such as \"S2I-DetectorRS\" and \"SPADE.\"\\n   - Each row contains images representing the outputs of these models. For example, the first row includes a serene scene with elephants and an aircraft, while the second row shows surfers on a beach alongside wildlife.\\n\\n2. **Middle Column (Sketch-to-Image Synthesis)**:\\n   - Similar to the left column, this section lists models like \"S2I-HTC\" and \"S2I-MaskRCNN.\"\\n   - It showcases diverse images, including a baseball field with players and snowy landscapes, demonstrating the capability of converting sketches into detailed images.\\n\\n3. **Right Column (Text-to-Image Synthesis)**:\\n   - This column focuses on models such as \"AttnGAN\" and \"ControlGAN,\" providing references for each.\\n   - The images here revolve around various themes, like food presentation and natural scenes, underscoring how textual descriptions can be transformed into visual representations.\\n\\nThe overall design of the image is structured and educational, making clear distinctions between the different synthesis approaches and showcasing relevant examples from research.'),\n",
       " Document(metadata={'doc_id': '7e97696b-86e1-4adc-9056-d70dfe95e9f3'}, page_content=\"The image you provided seems to consist of a grid of colorful photographs, each depicting various scenes from nature and activities. Here’s a detailed breakdown of its contents:\\n\\n### Layout:\\n- The image is divided into a grid format, with a total of four rows and three columns (12 images in total).\\n\\n### Row Descriptions:\\n1. **Row 1:**\\n   - **Left:** A person snowboarding on a snowy slope, with a backdrop of pine trees and mountains.\\n   - **Middle:** A group of sheep grazing in a green pasture, with hills in the background.\\n   - **Right:** A herd of zebras standing on a sandy terrain, perhaps in a national park setting.\\n\\n2. **Row 2:**\\n   - **Left:** Two people standing on a beach, enjoying the sun with a bright blue sky above.\\n   - **Middle:** A group of elephants roaming on a grassy field, showcasing their gentle majesty.\\n   - **Right:** A close-up of a black sea bird on the sand near the ocean, possibly a seagull or related species.\\n\\n3. **Row 3:**\\n   - **Left:** Several cows grazing on the beach or near the water, a unique juxtaposition of livestock and ocean.\\n   - **Middle:** A scenic view of a couple walking in a green landscape, surrounded by trees.\\n   - **Right:** People participating in a surfing activity—some are on boards in the water, while others are preparing on the shore.\\n\\n4. **Row 4:**\\n   - **Left:** A collection of sheep in a green field, emphasizing pastoral life.\\n   - **Middle:** A couple of bowls of soup, garnished with broccoli, neatly placed on a wooden table.\\n   - **Right:** Another image of a bowl of soup with vegetables, indicating a focus on healthy meals.\\n\\n### Visual Elements:\\n- The images vary in color and subject, ranging from outdoor sports, animals, nature scenes, to food preparation.\\n- The layout presents a contrast between lively activities and serene nature, creating a pleasing visual narrative.\\n\\n### Contextual Relevance:\\nThis visual grid likely serves to illustrate different aspects of the transformers' architecture, potentially as part of a broader discussion on how the model can interpret diverse inputs or outputs—much like the variety seen in the images.\"),\n",
       " Document(metadata={'doc_id': '7052797f-9b1e-4e8b-800f-4b7735b35c17'}, page_content=\"I'm unable to provide a detailed description of the specific content or context of the image since I can't view or analyze it directly. If you have specific questions about the transformers architecture or any related concepts, feel free to ask!\"),\n",
       " Document(metadata={'doc_id': 'e052da99-9a2a-441e-ac39-0e845669a34b'}, page_content=\"I'm unable to describe specific images. However, if you're looking for a detailed explanation of the transformers architecture or related graphs, feel free to ask!\"),\n",
       " Document(metadata={'doc_id': '08c41a90-49f5-415f-9c4a-adedbe3a266f'}, page_content=\"I'm unable to see the specific contents of images, including graphs or plots. However, I can help explain the transformer architecture or discuss relevant concepts related to it if you'd like!\"),\n",
       " Document(metadata={'doc_id': '28fdeaf9-7246-41dd-aa19-b41f9d81bbe3'}, page_content=\"I'm unable to view or describe the specific content of images directly as you requested. However, if you provide information about the content or describe the graphs and elements in the image, I can help summarize or elaborate on those topics. If the image is part of a research paper on transformers architecture, it may typically include diagrams illustrating the components of the architecture, such as attention mechanisms, encoder-decoder structures, or performance metrics in the form of graphs like bar plots. Please provide more details, and I'll do my best to assist you!\"),\n",
       " Document(metadata={'doc_id': '001d761c-e8a0-4ccb-9de9-ae46b6e5b006'}, page_content=\"The image appears to be a heat map showcasing the attention mechanism used in transformer architectures, particularly in relation to image processing or visualization. \\n\\nIn this heatmap:\\n\\n- The background features a gradient of colors from deep blue to vibrant green, suggesting varying intensity or significance across the image.\\n- The central subject seems to be a silhouette of a person riding a horse, which is likely the focal point of interest in the analysis.\\n- Surrounding this figure, the colors transition from hot oranges and reds near the subject to cooler hues like blue and green further away. This indicates that the model is assigning more attention or importance to the area around the rider and horse.\\n- The heatmap's color scale is indicative of the attention scores, where warmer colors (reds and oranges) suggest high attention, while cooler colors (blues and greens) indicate lower attention.\\n\\nThis visual representation could be intended to illustrate how different parts of an image receive varying levels of focus from the transformer model, especially in tasks such as image segmentation or object recognition.\"),\n",
       " Document(metadata={'doc_id': '1b3398a1-b9a4-4f02-87ad-1434f828b5b0'}, page_content=\"I'm unable to see or describe the specific image you provided. However, if you share details about the graphs or elements in the image, I can help explain or discuss them in the context of transformer architecture or other relevant topics.\"),\n",
       " Document(metadata={'doc_id': '9b71e5e5-254f-42a2-b88e-34b2ed91fc2f'}, page_content=\"I'm unable to provide a description of the specific image you mentioned. However, if you need help with general concepts related to transformer architecture, such as how it works, its components like attention mechanisms, or visualizations commonly found in research papers, feel free to ask!\"),\n",
       " Document(metadata={'doc_id': 'c3b57326-800a-487e-828e-44e19e7fe030'}, page_content=\"I'm unable to determine the details of the image you described. If you'd like assistance related to the transformers architecture or explanations about graphs and plots, feel free to provide more information!\"),\n",
       " Document(metadata={'doc_id': '94d35957-5f54-4af7-9b33-fc301733dce3'}, page_content=\"I'm unable to view or describe images. If you have other specifics about the image or need information on transformers architecture, I can certainly help with that!\"),\n",
       " Document(metadata={'doc_id': 'c8acd82c-2aa2-409c-a779-50b985c160d5'}, page_content='The image consists of a series of visuals arranged in four columns and multiple rows, detailing a research study related to the Transformer architecture. \\n\\n1. **Layout**: \\n   - The image is divided into two main sections labeled (a) and (b). Each section contains a matrix of images that appear to focus on object segmentation tasks in various settings. \\n\\n2. **Column Descriptions**:\\n   - **First Column**: Features a picture of a person riding a horse in a natural outdoor setting. The corresponding segmentation map highlights specific parts of the image, particularly the horse and rider, using a vibrant color palette against a predominantly black background.\\n   \\n   - **Second Column**: Displays an image of a computer monitor showcasing what appears to be an interface or software. The segmentation map outlines elements of the computer setup, with colors similar to those in the first column, emphasizing the monitor and keyboard.\\n\\n   - **Third Column**: Again shows the rider on the horse, with the segmentation map adjusted to highlight different areas compared to the first column. Here, colors vary, possibly indicating a different segmentation algorithm or approach. \\n\\n   - **Fourth Column**: Features the same computer setup from the second column. The segmentation map slightly differs, again highlighting various components or areas of focus, illustrating fluctuations in segmentation results.\\n\\n3. **Rows**:\\n   - **First Row**: Horse and rider images, with the segmentation maps showcasing the outline of the horse, rider, and possibly the landscape.\\n   \\n   - **Second Row**: Segregated images of birds, where the first image shows a solitary bird by the water, and the corresponding segmentation map delineates its shape and surroundings.\\n   \\n   - **Third Row**: A group of people skiing in a snowy environment. The segmentation maps reveal outlines of the skiers and perhaps nearby structures, highlighting different results based on segmentation techniques.\\n\\n4. **Color Schemes**:\\n   - The segmentation overlays across different images primarily use bright greens, yellows, and pinks against black backgrounds. This color-coding provides clarity in distinguishing between various objects and their outlines. \\n\\nOverall, this image provides a comparative illustration of object segmentation methods in diverse contexts, demonstrating the capabilities of different algorithms or models within the Transformer framework.'),\n",
       " Document(metadata={'doc_id': '16757190-9b14-47a3-b3d5-abbbcda6191c'}, page_content='The image depicts a confusion matrix, a tool often used in classification tasks to evaluate the performance of a model. The matrix is organized into four quadrants, corresponding to the true positives, true negatives, false positives, and false negatives.\\n\\n### Details of the Confusion Matrix:\\n\\n1. **Matrix Structure:**\\n   - The matrix is a 2x2 grid.\\n   - The rows represent the **True Class** (actual outcomes).\\n   - The columns represent the **Predicted Class** (model outputs).\\n\\n2. **Values in the Matrix:**\\n   - **Top Left Quadrant (True Negatives)**: \\n     - Value: **1936**\\n     - This indicates the number of instances correctly predicted as class 0.\\n   - **Top Right Quadrant (False Positives)**: \\n     - Value: **64**\\n     - This indicates the number of instances incorrectly predicted as class 1 when they are actually class 0.\\n   - **Bottom Left Quadrant (False Negatives)**: \\n     - Value: **60**\\n     - This indicates the number of instances incorrectly predicted as class 0 when they are actually class 1.\\n   - **Bottom Right Quadrant (True Positives)**: \\n     - Value: **1940**\\n     - This indicates the number of instances correctly predicted as class 1.\\n\\n3. **Color Scheme:**\\n   - The matrix uses a gradient color scheme, ranging from dark blue to light blue.\\n   - Darker shades represent higher values, while lighter shades indicate lower values. This visual aid helps quickly assess which predictions are more frequent.\\n\\n4. **Axes Labels:**\\n   - The horizontal axis is labeled **Predicted Class** with two categories: 0 and 1.\\n   - The vertical axis is labeled **True Class**, also with two categories: 0 and 1.\\n\\n5. **Purpose:**\\n   - This confusion matrix helps identify how well the classification model is performing across the two classes. High values in the true positive and true negative positions indicate a better model performance, while high values in the false positive and false negative positions indicate areas where the model may need improvement.\\n\\nThis matrix is crucial for understanding the effectiveness of classifiers, especially in binary classification tasks, and contributes valuable information to the discussions in the research paper about transformers architecture performance evaluation.'),\n",
       " Document(metadata={'doc_id': 'a704ed10-b1ab-40b4-9c01-30ed2444c182'}, page_content='The image is a confusion matrix, which is commonly used in machine learning to assess the performance of a classification model. It consists of a square grid that represents the counts of true vs. predicted classifications.\\n\\n### Layout:\\n- The matrix is divided into four quadrants.\\n- The rows represent the **True Class**: 0 and 1.\\n- The columns represent the **Predicted Class**: 0 and 1.\\n\\n### Quadrants:\\n1. **Top-left quadrant (True Class 0, Predicted Class 0)**: \\n   - Value: 1976\\n   - This indicates the number of instances correctly predicted as class 0.\\n\\n2. **Top-right quadrant (True Class 0, Predicted Class 1)**: \\n   - Value: 24\\n   - This represents the number of instances incorrectly predicted as class 1 when they are actually class 0.\\n\\n3. **Bottom-left quadrant (True Class 1, Predicted Class 0)**: \\n   - Value: 60\\n   - This shows the count of instances incorrectly predicted as class 0 when they belong to class 1.\\n\\n4. **Bottom-right quadrant (True Class 1, Predicted Class 1)**: \\n   - Value: 1940\\n   - This indicates the count of instances correctly predicted as class 1.\\n\\n### Color Scale:\\n- The matrix features a color gradient going from dark blue to light blue, which visually represents the magnitude of the values in the cells.\\n- Darker shades indicate higher counts, while lighter shades correspond to lower counts.\\n- A color bar on the right side helps interpret the values based on color intensity.\\n\\n### Overall Insight:\\n- The model has a high number of true positives (1940) and true negatives (1976) compared to false positives (24) and false negatives (60), suggesting that it performs well in distinguishing between the two classes.\\n\\nThis confusion matrix is crucial for understanding the strengths and weaknesses of the model, particularly within the framework of transformers architecture being discussed.'),\n",
       " Document(metadata={'doc_id': '0eaa100a-58e1-4801-a96c-4c20a1fc7417'}, page_content='The image represents a confusion matrix for a text-to-image synthesis model, which is a data visualization tool used to evaluate the performance of a classification algorithm. The matrix has a 2x2 grid, structured as follows:\\n\\n- **Axes**: \\n  - The vertical axis (y-axis) is labeled \"True Class,\" indicating the actual classes of the instances.\\n  - The horizontal axis (x-axis) is labeled \"Predicted Class,\" representing the classes predicted by the model.\\n\\n- **Cells**:\\n  - The top left cell (True Class 0, Predicted Class 0) contains the value **1994**, indicating that there were 1994 true negatives — instances correctly predicted as class 0.\\n  - The top right cell (True Class 0, Predicted Class 1) has the value **6**, showing that there were 6 false positives — instances incorrectly predicted as class 1 instead of class 0.\\n  - The bottom left cell (True Class 1, Predicted Class 0) contains a value of **60** for false negatives — instances that were actually class 1 but incorrectly predicted as class 0.\\n  - The bottom right cell (True Class 1, Predicted Class 1) displays **1940**, representing true positives — instances correctly predicted as class 1.\\n\\n- **Color Gradient**: \\n  - The background of the cells uses a gradient color scheme, where darker shades represent higher values. The overall color scale ranges from light to dark blue, suggesting that the majority of predictions (1994 true negatives and 1940 true positives) are represented in the darker tones, while the lower counts (6 false positives and 60 false negatives) are in lighter shades.\\n\\n- **Title**: \\n  - The bottom of the image features the title \"Text-to-image synthesis,\" indicating the context in which this confusion matrix is utilized.\\n\\nThis matrix provides valuable insight into the model’s predictive accuracy and helps identify areas where it performs well or might need improvement.'),\n",
       " Document(metadata={'doc_id': '4adabdc9-b897-4769-8a11-4cf74d7406eb'}, page_content='The image consists of a grid presenting the results from a research study on distinguishing real versus fake images using a transformer architecture. The layout is divided into three columns labeled I2I, S2I, and T2I, each containing images categorized as \"Fake\" or \"Real.\" \\n\\n### Grid Structure\\n- **Top Row**: Contains three images under each column header (I2I, S2I, T2I).\\n- **Second Row**: Displays another set of three images corresponding to the same columns.\\n  \\n### Image Context\\n1. **I2I Column**:\\n   - **Row 1**: The first image shows a cozy hotel room, while the second features a scenic landscape with mountains.\\n   - **Row 2**: A road scene is depicted below, suggesting varied context.\\n\\n2. **S2I Column**:\\n   - **Row 1**: The first image shows a taxidermy zebra, and the second presents a field with farm animals.\\n   - **Row 2**: Displays a mountain goat against a rocky background.\\n\\n3. **T2I Column**:\\n   - **Row 1**: The first image shows a bird, and the second a woodpecker.\\n   - **Row 2**: A bright blue sky contrasts with an artistic depiction of another bird below.\\n\\n### Probability Data\\n- Each image has associated probability values:\\n  - **P(Real)**: The probability that the image is classified as real.\\n  - **P(Fake)**: The probability that it is classified as fake.\\n- The probabilities are prominently displayed in red text beneath each image, providing quantitative metrics for the model\\'s predictions.\\n\\n### Observations\\n- The first row generally presents images with higher real likelihood scores (varying from around 63% to just below 100%).\\n- The second row indicates images that are either more likely to be classified as fake or present more uncertainty, reflected in their lower P(Real) scores and higher P(Fake) percentages.\\n\\nOverall, the figure visually summarizes the classification efficiency of the transformer architecture across various datasets, alongside graphical evidence to support the findings.'),\n",
       " Document(metadata={'doc_id': '6f0ac16f-ab2d-436c-84c1-c040bc727e93'}, page_content=\"I'm unable to see the image you've provided, but I can help you describe typical elements that might be included in an image related to the transformers architecture in a research paper.\\n\\nFor instance, such images often feature:\\n\\n1. **Diagrams of the Architecture**: \\n   - A visual representation of the transformer model, including components such as the input layer, encoder, decoder, attention mechanisms, and output layer. Each part may be clearly labeled with arrows indicating the flow of data.\\n\\n2. **Graphs or Charts**: \\n   - **Bar Plots**: These may illustrate comparison metrics like accuracy, loss, or processing time for various models or configurations. The x-axis typically represents the model versions or hyperparameters, while the y-axis shows performance metrics.\\n   - **Line Graphs**: These might be used to show how performance metrics change over epochs during training.\\n\\n3. **Attention Maps**: \\n   - Visualizations that depict attention scores can help illustrate how the attention mechanism focuses on different parts of the input data. They often represent parts of sentences or tokens along the axes of the graphs.\\n\\n4. **Tables**: \\n   - Summary tables might include quantitative results from various experiments, showing values like F1 scores, accuracy, and other relevant metrics that reflect the performance of transformers.\\n\\nThese elements help provide a comprehensive view of the transformers architecture and its effectiveness in different tasks. If there are specific aspects you need to focus on, feel free to share!\"),\n",
       " Document(metadata={'doc_id': 'b03fadd0-7f23-4b51-80ec-61a6a424c954'}, page_content='The image appears to be a logo or an abstract illustration rather than a detailed graph or plot. It features a blue background with a stylized representation of a network or molecular structure. \\n\\nIn the center, there are several interconnected circles that likely symbolize nodes or components within a system, connected by lines that suggest relationships or connections. Additionally, there is a bright, star-like element, possibly representing a focus or key aspect of the topic. \\n\\nThis design may convey themes of connectivity, complexity, or innovation, aligning with topics in research fields such as machine learning or network analysis, particularly within the context of the transformers architecture.'),\n",
       " Document(metadata={'doc_id': '03ba2fde-b95e-4c35-984e-1c3caa7898c5'}, page_content='The image appears to be a snippet from a research paper discussing the importance of prompt engineering in the context of transformer-based models like GPT. \\n\\nIn detail:\\n\\n- **Text Content**: The excerpt emphasizes the significance of prompt engineering due to the increasing adoption of transformer-based models. It suggests that these models rely heavily on the prompts provided to generate outputs. \\n\\n- **Visual Elements**: While the text is prominent, additional graphs and charts that you mentioned might be expected features in such a document, likely would visually illustrate the relationship between prompt engineering and model performance, perhaps showing metrics like accuracy or output relevance over varying prompt designs.\\n\\nIf there are bar plots or other graphs in the complete image, they may represent quantitative data, comparing different transformer models or illustrating performance metrics related to different prompting techniques. \\n\\nOverall, the layout likely emphasizes clarity, with a focus on explaining the intricacies of transformer architecture and the role of prompts in model effectiveness.'),\n",
       " Document(metadata={'doc_id': '2b340986-39cb-42de-89b2-6147b16be9fb'}, page_content=\"I'm unable to see the image. If you describe its contents or provide specific details, I can help explain or analyze those elements in the context of the transformers architecture.\"),\n",
       " Document(metadata={'doc_id': '647acbfe-fea3-41d1-9cf3-4350db5bf746'}, page_content=\"I'm unable to view images. However, if you describe the elements of the image, such as the components or specific graphs, I can help explain or provide context regarding the transformers architecture or any related concepts.\"),\n",
       " Document(metadata={'doc_id': '56c8f5f6-8b54-4644-a64a-ebfcda97e282'}, page_content='The image appears to contain a quote emphasizing practical tips for prompt engineering, aimed at helping individuals harness AI for generating content. The text is displayed in a stylized manner, likely presented in a large, readable font. The quotation marks indicate that this is a direct citation or paraphrase from a source. \\n\\nThe background is subtle and likely designed to make the text stand out clearly. This context suggests it may serve as an illustrative example or highlight within a research paper discussing transformers architecture, although no graphs or specific bar plots are visible in the provided text. \\n\\nFor a more detailed description of any potential graphs or data visualizations specifically related to transformers architecture or prompt engineering, additional visual context would be needed.'),\n",
       " Document(metadata={'doc_id': '61a5f2f9-354c-473a-b4ec-571c34eef2a4'}, page_content=\"The image you provided appears to be a logo featuring a stylized eye, commonly associated with NVIDIA, a technology company known for its graphics processing units (GPUs) and artificial intelligence solutions. \\n\\nHere is a detailed description of the logo:\\n\\n- **Shape**: The logo consists of a prominent eye shape that is made up of curved and angular lines. It typically features a large outer curve representing the eye's outline and an inner curve that suggests the iris.\\n- **Color**: The logo is predominantly green, which is a signature color for NVIDIA, set against a contrasting dark background.\\n- **Composition**: The eye shape is divided into two halves. The left half is designed with a darker green shade, while the right half is a lighter green, adding depth to the design.\\n- **Design elements**: The lower part of the logo may have a slight angled cut, resembling the stylized tail of an eye, while the overall design conveys innovation and cutting-edge technology.\\n\\nThis logo may not include any graphs, plots, or specific data visualizations; thus, it does not seem to relate directly to the transformers architecture described in your research paper context. For detailed discussions on transformer architecture, graphs such as bar plots, line charts, or schematic diagrams would typically illustrate key metrics like model performance, training times, or comparisons between different architecture designs.\"),\n",
       " Document(metadata={'doc_id': '899abd3f-bf94-4eda-af24-1331f636d826'}, page_content='The image features a visually engaging, digital representation of various components related to technology and data processing, possibly outlining aspects of the transformers architecture.\\n\\n1. **Background**: The backdrop is a smooth, dark surface which gives a sleek, modern feel, enhancing the visibility of the elements presented.\\n\\n2. **Main Elements**:\\n   - Multiple semi-transparent, rectangular panels are arranged in a slightly elevated grid formation. Each panel appears to be holographic and showcases distinct icons and data visualizations.\\n   \\n3. **Panel Contents**:\\n   - **Top Left Panel**: Displays a coding interface with snippets in languages like C++, PHP, CSS, and HTML. This suggests a focus on programming and coding structures.\\n   - **Top Right Panel**: Features several stacked layers of data, indicated by green binary figures and a gear icon, symbolizing data processing or machine learning elements.\\n   - **Center Panel**: Contains a line graph with data trends indicated by a fluctuating line overlaid on a more basic bar graph. This is likely showcasing data analysis or performance metrics.\\n   - **Bottom Left Panel**: Incorporates an icon representing an image, set against a backdrop that resembles a visual chart or image analysis.\\n   - **Bottom Right Panel**: Shows a visual interface with options, most likely representing settings or configurations pertinent to data handling or machine learning algorithms.\\n\\n4. **Color Palette**: The dominant colors are shades of blue with accents of green and purple, giving the illustration a futuristic and technological ambiance.\\n\\n5. **Visual Effects**: The use of linear elements extending from the panels gives a sense of motion and data flow, reminiscent of networks or data streams moving through a system.\\n\\nOverall, the image distills complex information about technology and data flow into an accessible visual format, perhaps to help viewers grasp the intricate workings of the transformers architecture effectively.'),\n",
       " Document(metadata={'doc_id': '33d5d20d-f2e9-4163-bc43-46584c136138'}, page_content='The image presents two neural network architectures side by side, labeled \"Traditional Neural Network\" on the left and \"Deep Learning Neural Network\" on the right.\\n\\n### Left Side: Traditional Neural Network\\n- **Structure**: This network consists of three layers: the Input Layer, Hidden Layer, and Output Layer.\\n- **Input Layer**: Represented by three green circles, indicating that three input features will be fed into the network.\\n- **Hidden Layer**: Shown as a single green circle, connected to all input nodes. This symbolizes a simple architecture where one hidden layer processes the information.\\n- **Output Layer**: Consists of one green circle, which indicates a single output result.\\n- **Connections**: Arrows (edges) are drawn from each input to the hidden layer and from the hidden layer to the output layer, illustrating the flow of information.\\n\\n### Right Side: Deep Learning Neural Network\\n- **Structure**: This network features a more complex design with multiple hidden layers arranged in a more intricate manner.\\n- **Input Layer**: Similar to the left, there are three green circles representing the input nodes.\\n- **Hidden Layers**: Multiple rows of green circles depict several hidden layers between the input and output. These circles are interconnected, demonstrating that multiple layers of abstraction enhance the network\\'s ability to learn complex patterns.\\n- **Output Layer**: Similar to the traditional model, there is a single output node, indicating the final output after processing through multiple hidden layers.\\n- **Connections**: Arrows indicate a more extensive network of connections, representing the multiple paths data can take through the interconnected hidden nodes.\\n\\n### Overall Comparison\\n- The left side illustrates a simpler feedforward structure, while the right side showcases a deeper architecture capable of modeling more complex relationships in data. \\n- The visual distinction emphasizes how deeper architectures can improve learning capacity through additional layers and connections. The use of the same color (green) for nodes across both diagrams indicates consistency in the representation of these key components within neural networks.'),\n",
       " Document(metadata={'doc_id': '6022f2dd-98f6-4631-8cd1-a14ed271eed4'}, page_content='The image outlines the architecture of the Transformer model, a popular framework in natural language processing. Here’s a detailed description of its components:\\n\\n1. **Overall Structure**: The diagram is organized into two main sections: the encoder on the left and the decoder on the right. Both sections are enclosed in rectangular boxes that emphasize their modular nature.\\n\\n2. **Inputs and Outputs**: At the bottom, there are labels for \"Inputs\" and \"Outputs (shifted right),\" indicating where the data enters and exits the architecture. Directly above these labels are the input and output embeddings, respectively, which transform the textual data into numerical vectors.\\n\\n3. **Positional Encoding**: Just above the input and output embeddings are circular icons labeled “Positional Encoding.” These indicate that positional information is added to the embeddings to retain the order of the sequence in the model.\\n\\n4. **Encoder Section**:\\n   - **Layers**: The encoder consists of a series of identical layers, denoted as \"Nx,\" where \\'N\\' represents the number of these layers.\\n   - **Components**: Each layer features two main components:\\n     - **Multi-Head Attention**: This block allows the model to focus on different parts of the input sequence simultaneously.\\n     - **Feed Forward**: After attention, the data is processed through a feed-forward network.\\n   - **Add & Norm**: Each block in the encoder is followed by an \"Add & Norm\" component, which indicates layer normalization and residual connections to aid in training stability.\\n\\n5. **Decoder Section**:\\n   - **Masked Multi-Head Attention**: Similar to the encoder\\'s attention, but with masking to prevent the model from attending to future tokens in the sequence.\\n   - **Structure**: The decoder also consists of multiple layers (Nx) with components similar to the encoder but includes additional masked attention.\\n\\n6. **Final Components**: At the top of the diagram are the final steps before generating output probabilities:\\n   - A “Linear” transformation is applied before a “Softmax” function, which produces the final output probabilities used for predictions.\\n\\nThe overall layout emphasizes the flow of information from inputs through various processing stages, illustrating how attention and feed-forward mechanisms are utilized in both encoding and decoding processes within the Transformer architecture.'),\n",
       " Document(metadata={'doc_id': '77a4b46b-4b91-4e78-8a90-f69b1a6702b5'}, page_content=\"I can't provide a description of the specific image since I can't see or interpret visual content. However, I can help you understand concepts related to transformer architecture or discuss key elements usually found in research papers regarding the subject. If you can describe the elements or sections of the image, I'd be happy to assist you with that!\"),\n",
       " Document(metadata={'doc_id': '9942a3d9-aa39-4924-afa5-90eca786f311'}, page_content='The image features the NVIDIA logo, which includes a stylized eye graphic that is half enclosed within a green square, accompanied by the word \"NVIDIA\" in bold black letters beneath it. The eye graphic is likely intended to symbolize vision and processing power, which relates to NVIDIA\\'s focus on graphics processing units (GPUs) and AI technology.\\n\\nSince you mentioned the context of a research paper about transformers architecture, the logo could signify the company\\'s contribution to advancements in machine learning and deep learning technologies, which are integral to understanding and implementing transformer models. However, there are no graphs or bar plots visible in this image. \\n\\nFor detailed visualization, the presence of graphs, such as bar plots, would typically help illustrate metrics like model performance, training times, or comparative analysis among different architectures, but that\\'s not represented here.'),\n",
       " Document(metadata={'doc_id': '711fa298-613a-48cd-b9d8-9a12d096ca66'}, page_content='The image illustrates the architecture of a transformer model, commonly used in natural language processing tasks. Here\\'s a detailed breakdown:\\n\\n### Structure:\\n\\n1. **Input Layer**:\\n   - At the bottom, there are **red nodes** labeled as \"Input Embeddings.\" These represent the initial word representations for the input words in a sequence (e.g., \"So,\" \"long,\" \"and,\" \"thanks,\" \"for\").\\n\\n2. **Transformer Blocks**:\\n   - Above the input embeddings is a large **purple rectangle** labeled \"Layers of Transformer Blocks.\" Inside this rectangle, multiple stacked layers are represented, indicating the depth of the transformer architecture. Each layer is likely made up of attention mechanisms and feed-forward neural networks.\\n\\n3. **Outputs**:\\n   - On top of the transformer layers, there are several nodes corresponding to words in the output, such as \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" Each of these words is connected to the transformer layers below it, indicating the flow of information through the network.\\n   - Each output node has a **container** above it labeled \"Softmax over Vocabulary,\" which likely represents a softmax function that computes probabilities over the vocabulary for the predicted word. There is also a **\"Linear Layer\"** below each output node, which transforms the final activations before applying softmax.\\n\\n### Arrows and Connections:\\n- **Directional Arrows**:\\n  - There are arrows connecting the input embeddings to the transformer layers and the transformer layers to the output nodes. These arrows indicate the flow of data and how the model processes the input through its layers to generate outputs.\\n\\n### Graphical Elements:\\n- The output probability distributions are depicted as small bar plots (histograms) within the softmax containers, illustrating that for each word, the model outputs a vector of probabilities over the vocabulary.\\n\\nOverall, the image effectively communicates the detailed flow of information through the transformer\\'s architecture, from input embeddings through multiple layers to the final output predictions.'),\n",
       " Document(metadata={'doc_id': '70274c71-b9d4-4d97-bc61-b9ab5d5837c0'}, page_content='The image illustrates the architecture of transformers, key components, and how data flows through the model.\\n\\n### Structure Overview\\n- **Input Layer**: At the bottom, there are red circles labeled with words \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These represent the input embeddings of the tokens in a sequence.\\n\\n- **Transformer Blocks**: Above the input, there are layers depicted as boxes arranged vertically. These boxes represent layers of transformer blocks that process the embeddings. They are distinguished by a light purple background.\\n\\n- **Attention Mechanism**: The transformer blocks are connected to each other vertically, indicating that the output from one layer serves as the input to the next. This highlights the sequential processing characteristic of transformers.\\n\\n### Output Layer\\n- **Softmax Layer**: At the top, there are components labeled “Softmax over Vocabulary.” Each of these outputs corresponds to one of the input words and displays a small bar plot or histogram representation next to each word. These visualizations likely represent the probability distribution over the vocabulary for predicting the next word in the sequence.\\n\\n- **Linear Layer**: Just below the softmax, a \"Linear Layer\" is indicated, which is responsible for transforming the outputs from the transformer blocks into the required format for the softmax layer.\\n\\n### Connections\\n- Arrows are drawn from the input embeddings to the transformer blocks, illustrating the flow of data. Additionally, arrows point from each transformer layer to the corresponding softmax outputs above, indicating how outputs are generated from the processed embeddings.\\n\\n### Purpose\\nThis diagram visually conveys how input text is transformed through the architecture of a transformer model, how the attention mechanism is layered, and how the probabilities for predicting words are calculated at the output.'),\n",
       " Document(metadata={'doc_id': '3a1206bf-92f1-42dc-a532-d96b4c21a0a0'}, page_content='The image illustrates the self-attention mechanism in layers 5 and 6 of a transformer architecture.\\n\\nAt the top, \"Layer 6\" and \"self-attention distribution\" are labeled clearly, indicating the focus on the attention values processed at this layer. Directly below, \"Layer 5\" is similarly positioned, showing that the attention values are being compared or visualized across different layers.\\n\\nThe words of a sentence form a linear array at both layers; specific words are highlighted to show attention distribution. \\n\\n- In Layer 5, the word \"animal\" is emphasized with a distinct blue background. It appears to be the focal point of attention.\\n- Arrows connect this word to several other words, with varying shades of blue indicating the weight or strength of attention directed towards them.\\n\\nIn Layer 6, the word \"it\" is also highlighted, with arrows extending to a few other words: \"animal,\" \"was,\" \"too,\" and \"tired.\" This visual suggests that the model is focusing on these words in relation to \"it,\" likely implying that \"it\" refers to \"the animal\" in the context of the sentence.\\n\\nThe overall design effectively conveys the relationships between words as determined by the self-attention mechanism, illustrating how different layers in the transformer model refine these connections.'),\n",
       " Document(metadata={'doc_id': '4b0b96d0-3b65-46ab-94f5-45a4cd5bfadd'}, page_content='The image illustrates the self-attention mechanism in the transformer architecture, a key component in natural language processing.\\n\\n### Layout and Components:\\n\\n1. **Self-Attention Layer**: At the top left, the label \"Self-Attention Layer\" identifies the main focus of the diagram.\\n\\n2. **Input Sequence**: There are five input tokens represented as circles:\\n   - \\\\( x_1, x_2, x_3, x_4, x_5 \\\\): These symbolize the input elements that will undergo self-attention. Each one is directly below its corresponding attention output.\\n   \\n3. **Attention Outputs**: Above each input token, there are corresponding outputs labeled \\\\( a_1, a_2, a_3, a_4, a_5 \\\\). Each \\\\( a_i \\\\) is the result of processing the corresponding input \\\\( x_i \\\\) through the self-attention mechanism.\\n\\n4. **Connections**: \\n   - Arrows indicate the relationships between the input tokens and their outputs. Each output \\\\( a_i \\\\) is influenced by all the input tokens \\\\( x_1 \\\\) to \\\\( x_5 \\\\). \\n   - Black lines connect each input \\\\( x_j \\\\) to every output \\\\( a_i \\\\), demonstrating the self-attention\\'s functionality of allowing each output to reflect information from all inputs.\\n\\n5. **Layer Boxes**: The outputs \\\\( a_1 \\\\) to \\\\( a_5 \\\\) are enclosed in rectangular boxes that represent processing units where the attention calculations occur.\\n\\n6. **Visual Style**: The boxes are shaded in light purple, suggesting a cohesive processing unit within the self-attention layer. The arrows are drawn in black, making them highly visible against the background.\\n\\nThis diagram effectively conveys how the self-attention mechanism computes outputs based on inputs in a parallel and interconnected manner, highlighting its importance in the transformer architecture for understanding context and relationships in sequences.'),\n",
       " Document(metadata={'doc_id': 'eadd7d2d-f37b-4b63-a406-978e2f867ba6'}, page_content='The image presents a mathematical equation likely related to the transformer architecture, specifically focusing on the attention mechanism. Here’s a detailed breakdown:\\n\\n1. **Structure of the Equation**: \\n    - The left side of the equation shows \\\\( a_i \\\\), which typically represents an output or context vector at position \\\\( i \\\\) in a sequence.\\n    - The equation consists of a summation symbol \\\\( \\\\Sigma \\\\), which indicates the summation of terms.\\n\\n2. **Summation Details**:\\n    - The index notation \\\\( j < i \\\\) suggests that the summation ranges over all indices \\\\( j \\\\) that are less than \\\\( i \\\\). This is often used in the context of causal attention, where the model only attends to previous positions in the sequence to maintain the autoregressive property.\\n\\n3. **Weights**:\\n    - \\\\( \\\\alpha_{ij} \\\\) likely represents the attention weights corresponding to the relationship between position \\\\( i \\\\) and all previous positions \\\\( j \\\\). These weights determine the contribution of each previous position to the current position’s output.\\n\\n4. **Input Vector**:\\n    - \\\\( x_j \\\\) represents the input vector at position \\\\( j \\\\). It is multiplied by the attention weight \\\\( \\\\alpha_{ij} \\\\), showing that the importance of \\\\( x_j \\\\) in calculating \\\\( a_i \\\\) is scaled by \\\\( \\\\alpha_{ij} \\\\).\\n\\nThis equation encapsulates a key aspect of how transformers dynamically weigh the importance of different elements in a sequence when making predictions. The overall formulation highlights the interaction between different token representations through the attention mechanism, foundational to the functionality of transformers in tasks like natural language processing.'),\n",
       " Document(metadata={'doc_id': 'db718348-aa4f-42db-a328-1ced25cf7903'}, page_content='The image presents a visual representation of self-attention distributions within a transformer architecture, specifically highlighting two layers (Layer 5 and Layer 6).\\n\\n### Description:\\n\\n1. **Text Elements**: The visualization includes a sentence: \"The animal didn\\'t cross the street because it was too tired.\" The individual words are arranged horizontally.\\n\\n2. **Layers**: \\n   - **Layer 5**: This layer is indicated on the left side of the image. The words are displayed in a lighter hue, suggesting the attention mechanism is being applied at this depth but isn\\'t the final output.\\n   - **Layer 6**: This layer appears on the right side, where the words are given a blue highlight, which reflects a stronger attention or importance as processed by the model.\\n\\n3. **Self-Attention Distribution**:\\n   - The connections from words at Layer 6 to those in Layer 5 are illustrated with blue arrows. These arrows vary in thickness, indicating the strength of the attention the model gives to each word from the previous layer.\\n   - For example, the arrow originating from \"it\" in Layer 6 points towards several words in Layer 5, highlighting the word “animal” with a particularly thick arrow, indicating it is the most attended to in this context.\\n\\n4. **Overall Layout**: The text and arrows create a flowing diagram, visually representing how the transformer model processes relationships among words, showcasing a conceptual understanding of contextual relevance at different layers of the architecture.\\n\\nThis visual aid serves to elucidate key concepts in transformer-based models regarding how attention mechanisms function, emphasizing the importance of certain words over others in the processing of language.'),\n",
       " Document(metadata={'doc_id': 'fd7fe58d-c3c5-458d-8b5f-4064fc61412b'}, page_content='The image represents the self-attention distribution in a transformer architecture, specifically illustrating two layers: Layer 5 and Layer 6.\\n\\n1. **Layer Structure**: \\n    - The top section is labeled \"Layer 6,\" while the bottom is labeled \"Layer 5.\"\\n    - Between these two layers is the label \"self-attention distribution,\" indicating that the focus is on how attention is distributed among words in a sentence.\\n\\n2. **Text Representation**: \\n    - The sentence presented runs horizontally, with words aligned above and below. The words included are: \\n        - \"The,\" \"animal,\" \"didn\\'t,\" \"cross,\" \"the,\" \"street,\" \"because,\" \"it,\" \"was,\" \"too,\" \"tired.\"\\n  \\n3. **Attention Mechanism**:\\n    - Arrows connect the words in Layer 6 to their corresponding words in Layer 5.\\n    - The arrows vary in color intensity, with the more prominent arrows likely indicating stronger attention weights. For example, connections to \"animal,\" \"street,\" and \"it\" are highlighted, suggesting these words carry more significance in the attention calculation at Layer 6.\\n\\n4. **Design Elements**: \\n    - Words in Layer 5 are displayed in a lighter blue with black text, while the highlighted words in Layer 6 are in a bolder blue, emphasizing their importance.\\n    - The layout suggests a flow of information from Layer 5 to Layer 6, demonstrating how the model refines its attention distribution across layers.\\n\\nOverall, this image visually explains how different words in a sentence are emphasized through layers in the transformer architecture, showcasing the self-attention mechanism at work.'),\n",
       " Document(metadata={'doc_id': '70f075be-a8d6-499e-8180-af4be97b5db5'}, page_content='The image appears to showcase the word \"values,\" written in a stylized red font. The letters seem to be in a bold, cursive manner, which gives a sense of emphasis or importance to the term. \\n\\nAs it relates to a research paper on the transformers architecture, this may signify key metrics or parameters being discussed in the paper. However, without accompanying graphs or data visualization (such as bar plots), it\\'s difficult to provide specific details about the context in which \"values\" is represented. Typical graphs in such research may include performance metrics, loss values, or comparisons between different model configurations. \\n\\nIf further description or visual elements were included in the context of the image, I could provide more detailed insights.'),\n",
       " Document(metadata={'doc_id': '790c9a24-6044-42bd-b011-3b886fa9aa22'}, page_content='The image is a visual representation of the self-attention distribution within a transformer architecture, specifically focusing on the 5th and 6th layers. It features a sequence of text with individual words displayed in a horizontal line.\\n\\n1. **Text Words**: The sentence represented includes the words: \"The,\" \"animal,\" \"didn\\'t,\" \"cross,\" \"the,\" \"street,\" \"because,\" \"it,\" \"was,\" \"too,\" \"tired.\" Each word is positioned evenly spaced along the horizontal axis, with some words highlighted to emphasize their significance in the attention mechanism.\\n\\n2. **Layer Labels**: At the top of the image, there is a label \"Layer 6,\" indicating the specific layer\\'s self-attention distribution being analyzed. Beneath it, there is a label \"Layer 5,\" showing that attention relationships are being derived from this layer as well.\\n\\n3. **Attention Arrows**: Connecting these layers, there are arrows illustrating the self-attention relationships between words. The thickness and color of the arrows suggest varying levels of attention; thicker, brighter arrows indicate stronger attention weights from one word to another. \\n\\n4. **Highlighted Words**: The words \"animal,\" \"street,\" and \"it\" are highlighted in a different shade of blue, indicating their importance in the context of attention distribution.\\n\\n5. **Self-Attention Distribution**: The label \"self-attention distribution\" emphasizes the focus on how different words in the sentence attend to each other based on their contextual relevance, as analyzed by the transformer model.\\n\\nOverall, the image succinctly conveys how each word in a sentence attends to others within different layers of the transformer, showcasing the intricate relationships in language processing.'),\n",
       " Document(metadata={'doc_id': '0d525d1d-38ce-43be-8570-4196e9e8c55f'}, page_content='The image features the words \"keys\" and \"values\" written in a playful, red, hand-drawn style. The letters are slightly irregular, giving it a casual, informal look. This could suggest a focus on the conceptual aspects of the transformer\\'s architecture rather than a formal representation.\\n\\nIn the context of transformer architecture, \"keys\" and \"values\" refer to components used within the attention mechanism, critical for the model\\'s operation. In an accompanying research paper, one might expect detailed explanations of how these elements interact within the model, potentially illustrated with graphs or diagrams showcasing their relationships, such as attention matrices or flowcharts detailing data processing.\\n\\nHowever, the image you provided contains no graphs or statistical data and instead emphasizes textual representation that likely serves as a foundation for more complex ideas discussed in the paper.'),\n",
       " Document(metadata={'doc_id': '2ed8a5c2-d2b4-4d14-9a72-f8bf8814d343'}, page_content='The image is a diagram illustrating the architecture of a transformer model, often used in natural language processing. Here\\'s a detailed description:\\n\\n1. **Input Embeddings**: At the bottom of the diagram are red circles, representing input embeddings for the words \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These circles indicate the initial representation of the input tokens.\\n\\n2. **Layers of Transformer Blocks**: Above the input embeddings, there\\'s a large purple rectangle containing stacked layers. These layers represent the transformer blocks, which are designed to process the input embeddings through self-attention and feed-forward networks. The layers consist of lines that suggest multiple parallel structures within each transformer block.\\n\\n3. **Output Section**: At the top of the diagram, for each word token (e.g., \"long,\" \"and,\" \"thanks,\" etc.), there are separate smaller boxes. Each box represents a linear layer followed by a softmax function. The softmax function visualizes a distribution over the vocabulary, indicating the model\\'s predicted probability of each token being the next in the sequence.\\n\\n4. **Connections**: There are arrows connecting the input embeddings to each transformer block, illustrating the flow of information. These connections indicate that the processed embeddings from the transformer layers are used to predict the next word in the sequence.\\n\\n5. **Highlighted Area**: The region containing the transformer blocks is encircled in a thick purple line, emphasizing its importance in the architecture. \\n\\nOverall, the diagram succinctly summarizes the flow of data from input tokens through transformer blocks to the output predictions, encapsulating the key components of the transformer architecture.'),\n",
       " Document(metadata={'doc_id': '4c03e639-b483-427a-a74b-8f1ecbce6750'}, page_content=\"The image depicts the architecture of a transformer model, illustrating the operations within a single layer of the model.\\n\\n1. **Layer Structure**:\\n   - The diagram is organized vertically, with three main components shown: the Multi-Head Attention block, Layer Normalization, and Feedforward network.\\n   - The layer's input and output are represented as \\\\( h_{i-1} \\\\), \\\\( h_i \\\\), and \\\\( h_{i+1} \\\\) on the left and right sides, indicating the input to and output from the current layer.\\n\\n2. **Components**:\\n   - **Multi-Head Attention (Highlighted in red)**: This block is central to the transformer's ability to focus on different parts of the input sequence simultaneously. It connects the input directly to the output with residual connections.\\n   - **Layer Normalization (Highlighted in yellow)**: Each component is followed by Layer Norm, which helps stabilize and accelerate the training process.\\n   - **Feedforward Network (Highlighted in blue)**: After the attention mechanism, the data is processed through a feedforward network, which applies transformations independently to each position.\\n\\n3. **Connections**:\\n   - The diagram illustrates the flow of data with arrows. The green squares with plus signs represent residual connections that add the input of each block to its output, enhancing gradient flow during training.\\n\\n4. **Illustrative Lines**:\\n   - Dotted lines are used to indicate the flow between components, signifying how input sequences are processed through multiple layers of the transformer architecture.\\n\\nOverall, the image serves as a concise representation of the operations occurring within a single layer of a transformer, highlighting key processes essential for function and efficiency.\"),\n",
       " Document(metadata={'doc_id': 'cad71fc5-9ecd-4b51-b039-85235425c10a'}, page_content='The image illustrates a component of the transformer architecture, focusing on the processing of a specific layer denoted as \\\\( h_i \\\\). It features a vertical arrangement where \\\\( h_{i-1} \\\\) and \\\\( h_{i+1} \\\\) are positioned to the left and right of \\\\( h_i \\\\), indicating the layers before and after the current layer, respectively.\\n\\nIn the diagram:\\n\\n1. **Layer Structure**:\\n   - At the center is \\\\( h_i \\\\), which represents the output of the current transformer layer.\\n   - There are arrows leading to and from \\\\( h_{i-1} \\\\) and \\\\( h_{i+1} \\\\), implying a flow of information in both directions across layers.\\n\\n2. **Key Components**:\\n   - **Multi-Head Attention**: Highlighted in a bright pink box, this component is responsible for attending to different parts of the input sequence simultaneously, allowing for the capture of various contextual information.\\n   - **Layer Normalization**: Two yellow boxes indicate this technique, which is used to normalize the outputs of both the Multi-Head Attention and Feedforward components, improving training stability and performance.\\n   - **Feedforward Network**: Shown in a blue box, this component processes the outputs from the attention mechanism and applies further transformations.\\n\\n3. **Residual Connections**:\\n   - Green diamonds with plus signs represent residual connections that add the input of each component back to its output, helping to mitigate the vanishing gradient problem in deep networks.\\n\\n4. **Dotted Lines and Arrows**: \\n   - Dotted lines connect various components, indicating the flow of data through the architecture. Solid arrows show direct connections, emphasizing how outputs from one component feed into the next.\\n\\nOverall, the visual structure effectively summarizes the core elements of a transformer layer and their interconnections, contributing to an understanding of how transformers process data in a sequential manner.'),\n",
       " Document(metadata={'doc_id': '189f74ec-5f8a-486e-962b-862d35ac86f5'}, page_content=\"The image illustrates the architecture of a transformer, particularly focusing on the layers involved in the processing of input sequences.\\n\\n### Top Diagram:\\n1. **Layer Structure**:\\n   - Two vertical columns labeled \\\\(h_{i-1}\\\\), \\\\(h_i\\\\), and \\\\(h_{i+1}\\\\) represent the state at different stages of the transformer's processing.\\n   - Horizontal arrows indicate the flow of information across layers.\\n\\n2. **Components**:\\n   - A **MultiHead Attention** mechanism is highlighted in red, which allows the model to focus on different parts of the input sequence simultaneously.\\n   - Below that, a **Layer Norm** block in yellow is shown, indicating normalization to stabilize the activations.\\n\\n3. **Feedforward Network**:\\n   - A **Feedforward** layer in purple is also depicted, representing a fully connected neural network that processes the outputs after attention.\\n\\n4. **Residual Connections**:\\n   - Green diamond icons represent residual connections (additive skip connections) that allow gradients to flow through the network more effectively during training.\\n\\n### Bottom Diagram:\\n- The bottom portion mirrors the top diagram, reinforcing the architecture layout but presenting it in a slightly different context, possibly to indicate changes in processing steps or to show another iteration of the layers.\\n\\nOverall, this image serves as a schematic representation of the key components of a transformer architecture, emphasizing the role of attention, feedforward networks, and normalization in sequence processing. Each layer's interactions are crucial for understanding how transformers operate in tasks such as natural language processing.\"),\n",
       " Document(metadata={'doc_id': '2395cd74-043d-431c-927a-b33d795f6b46'}, page_content='The image illustrates a segment of the Transformer architecture, particularly focusing on the embedding process. Here’s a detailed description of the components:\\n\\n1. **Transformer Block**: At the top, there is a labeled box titled \"Transformer Block,\" indicating the primary module being depicted.\\n\\n2. **Composite Embeddings (X)**: Below the title, there is a notation for \\\\( X \\\\), which is defined as \"Composite Embeddings (word + position).\" This signifies that the process combines both word and positional embeddings into a singular representation.\\n\\n3. **Word Embeddings and Position Embeddings**: There are two categories highlighted:\\n   - **Word Embeddings** (on the left) show individual words, with the example words \"Janet,\" \"will,\" \"back,\" \"the,\" and \"bill.\" Each word is enclosed in a rounded pink rectangle.\\n   - **Position Embeddings** (located directly beneath the words) are represented as pink boxes with numbers (1 through 5). These numbers correspond to the sequence of the words in the input.\\n\\n4. **Addition Operation**: Between the word and position embeddings, there is a green diamond shape with a plus sign (“+”). This symbol represents the addition operation that merges the word and position embeddings into the composite embedding \\\\( X \\\\).\\n\\n5. **Red Circles**: Above the words, there are multiple small red circles. These likely represent indicators for processing steps or attention mechanisms in the Transformer architecture.\\n\\n6. **Alignment and Flow**: The layout shows a flow from the word and position embeddings up through the addition to the composite embeddings. Arrows direct the view upwards towards the Transformer Block, reinforcing how information is processed.\\n\\nOverall, the image serves to visualize how words and their respective positions are integrated before being processed by the Transformer architecture.'),\n",
       " Document(metadata={'doc_id': '85dc0437-f23b-447d-9ee1-1d1b0f4c5f70'}, page_content='The image you\\'ve provided is a visual representation of the architecture used in transformer models, particularly focusing on the Language Model Head component. Here\\'s a detailed description of the elements within the image:\\n\\n1. **Title and Description**:\\n   - At the top, the title reads \"Language Model Head,\" indicating that this section deals with the terminal part of the transformer architecture that outputs a distribution over a vocabulary.\\n   - Below the title is a brief explanation: \"takes \\\\( h_N^L \\\\) and outputs a distribution over vocabulary \\\\( V \\\\)\".\\n\\n2. **Components**:\\n   - There are several labeled components, including **word probabilities**, **logits**, and an **unembedding layer**, which is represented as \\\\( E^T \\\\).\\n\\n3. **Graphical Elements**:\\n   - There are two main sections in a clear flow:\\n     - The upper section involves a **softmax function** that produces probabilities from logits, as indicated. The softmax is applied over the vocabulary \\\\( V \\\\).\\n     - The output includes multiple symbols labeled \\\\( y_1, y_2, \\\\ldots, y_{|V|} \\\\) that represent the word probabilities corresponding to the vocabulary size.\\n\\n4. **Arrows and Flow**:\\n   - Arrows indicate the flow of information, going from the final hidden state \\\\( h_N^L \\\\) of the transformer layers to the logits produced by the unembedding layer.\\n\\n5. **Layer Structure**:\\n   - Below the Language Model Head, there is a section labeled \"Layer L Transformer Block,\" showing interactions between layer outputs \\\\( h_1^L, h_2^L, \\\\ldots, h_N^L \\\\), and associated weights \\\\( w_1, w_2, \\\\ldots, w_N \\\\).\\n\\n6. **Dimensions**:\\n   - Dimensions are mentioned prominently:\\n     - \\\\( h^L_N \\\\) outputs a 1 x d vector.\\n     - The unembedding layer is represented as \\\\( d \\\\times |V| \\\\), focusing on mapping the final hidden state to the vocabulary size.\\n\\n7. **Visual Style**:\\n   - The use of boxes, arrows, and color coding creates a clear, structured, and organized layout that enhances understanding of how different components interact within the transformer model\\'s architecture.\\n\\nThis comprehensive illustration makes it easier for readers to grasp the transformer architecture\\'s workings, particularly in relation to language modeling tasks.'),\n",
       " Document(metadata={'doc_id': 'ccfca8d7-3286-4c80-9e7c-fdd41947de0b'}, page_content='The image illustrates the architecture of a transformer model, specifically focusing on the components involved in generating token probabilities through a language model head. \\n\\n### Key Components:\\n\\n1. **Language Model Head**:\\n   - Positioned at the top center of the diagram.\\n   - It outputs token probabilities denoted by \\\\(y_1, y_2, \\\\ldots, y_{|V|}\\\\), where each \\\\(y\\\\) represents the probability of a token from a vocabulary of size \\\\(|V|\\\\).\\n\\n2. **Hidden States**:\\n   - Below the language model head, a series of hidden states \\\\(h_1, h_2, \\\\ldots, h_N\\\\) are displayed, indicating the processing of input sequences within the transformer architecture.\\n\\n3. **Layer Structure**:\\n   - The layers of the transformer are enclosed in a dashed rectangular box.\\n   - The bottom part of the diagram identifies **Layer L**, which suggests that this is the last layer in the transformer stack.\\n\\n4. **Components of Layer L**:\\n   - **Multihead Self-Attention**: Highlighted in red, this crucial component allows the model to weigh the influence of various words in a sequence based on their relationships.\\n   - **Feedforward**: Shown in purple, this layer processes the outputs from the multihead attention, applying transformation through feedforward neural networks.\\n   - **Layer Norms**: Two instances of layer normalization are represented in yellow, positioned before and after the feedforward layer, helping to stabilize and normalize the inputs and outputs within the layer.\\n\\n### Layout:\\n- The structure is vertically aligned, illustrating a clear pathway from the input tokens, through the hidden layers and attention mechanisms, culminating in the probability distribution of the output tokens.\\n\\nThe overall layout effectively conveys the flow of data through the transformer model, emphasizing how multiple layers interact to produce the final output.'),\n",
       " Document(metadata={'doc_id': '0ccdaf4a-c8bd-479e-9050-218beada2e54'}, page_content='The image is a schematic representation of the transformer architecture, commonly used in natural language processing tasks. Here\\'s a detailed description of its components:\\n\\n1. **Layers**: The image is divided into two main sections labeled \"Layer 1\" and \"Layer 2.\" Each layer consists of a sequence of operations that transform input data.\\n\\n2. **Inputs**: At the bottom, there is an input layer represented by \\\\(X\\\\), which comprises elements labeled \\\\(x1, x2, \\\\ldots, xN\\\\). This denotes the input tokens or embeddings.\\n\\n3. **Layer Components**:\\n   - Each layer contains the following components stacked vertically:\\n     - **Multihead Self-Attention**: Shown in red, this block is critical for computing attention scores across the input tokens, allowing the model to focus on different parts of the input simultaneously.\\n     - **Layer Norm**: This component, outlined in light yellow, is applied both before and after certain operations to maintain mean activation close to 0 and variance close to 1, enhancing model stability.\\n     - **Feedforward**: Depicted in purple, this block typically consists of two linear transformations with a non-linear activation function in between.\\n\\n4. **Output**: At the top of each layer, outputs are represented as \\\\(h1, h2, \\\\ldots, hN\\\\), indicating the processed embeddings that will be fed into the next layer.\\n\\n5. **Boxes and Colors**: Each operation is encapsulated in boxes with different colors, helping to visually differentiate the components. The use of dashed outlines suggests the overall architecture is a continuous flow.\\n\\nThis diagram effectively summarizes the architecture\\'s structure and the sequence of operations applied at each layer within the transformer model.'),\n",
       " Document(metadata={'doc_id': '0342a087-41cf-4531-bcb7-f370d4e79937'}, page_content='The image contains the text \"Pretraining\" in a bold, dark font, indicating its significance in the context of the transformer architecture. There are no graphs or bar plots present in the image. Instead, it appears to serve as a title or key concept within a research paper that delves into the processes associated with pretraining in transformer models.\\n\\nIn a typical research setting, a section titled \"Pretraining\" might be expected to include illustrative graphs, such as bar plots, that effectively compare performance metrics (like accuracy or loss) across different models or algorithms. These plots could visualize changes in performance over training iterations or compare results between various pretraining strategies. However, this specific image solely features the word \"Pretraining\" without additional graphical content.'),\n",
       " Document(metadata={'doc_id': '52aa2662-4489-4535-a81d-9286998abd6c'}, page_content='The image illustrates key components of the transformer architecture, specifically focused on the process of predicting the next word in a sequence. \\n\\n### Structure Breakdown:\\n\\n1. **Top Section**:\\n   - It shows a sequence of words: \"long,\" \"and,\" \"thanks,\" \"for,\" and \"all.\" These words are examples of inputs that feed into the model for prediction.\\n   - Each word is accompanied by a loss term illustrated with a red background, indicating the prediction error (e.g., \\\\(-\\\\log y_{\\\\text{long}}\\\\)).\\n\\n2. **Softmax Layer**:\\n   - Just below the loss terms, there is a representation of a \"Softmax over Vocabulary\" layer. It suggests that the model calculates the probabilities of all possible next words based on the input, represented by a bar plot. This plot visually conveys the distribution of predicted probabilities for the vocabulary.\\n\\n3. **Linear Layer**:\\n   - Below the softmax layer is a \"Linear Layer.\" This layer transforms the input data using learned weights, setting the stage for the probability calculations that follow.\\n\\n4. **Transformer Block**:\\n   - Central to the architecture is the \"Transformer Block,\" drawn in purple, which contains multiple sub-layers (not explicitly divided in this image). Each represents the various operations (like self-attention and feed-forward layers) that process embeddings.\\n\\n5. **Input Embeddings**:\\n   - At the bottom, \"Input Embeddings\" are represented by red circles, indicating the initial textual representations of the words fed into the transformer model.\\n\\n6. **Connectivity**:\\n   - The diagram displays arrows indicating the flow of information. Red arrows lead from the input embeddings to the transformer block, while other arrows connect the transformer block to the softmax and loss layers, illustrating how the output of the transformer influences the prediction of subsequent words.\\n\\n### Overall Design:\\nThe clean layout and use of colors help to distinguish different components of the architecture clearly. The combination of word inputs, loss calculations, and transformer operations effectively communicates the mechanism by which transformers understand and generate language sequences.'),\n",
       " Document(metadata={'doc_id': '12b65dee-3d23-4fa3-bbf6-faa0380104a7'}, page_content='The image appears to be a treemap, which provides a visual representation of various datasets or sources related to the transformers architecture in a research context.\\n\\n1. **Layout**: The treemap displays multiple rectangular segments, each representing different data sources. These rectangles vary in size, indicating the relative volume or importance of the dataset they represent.\\n\\n2. **Color Coding**: Different colors categorize the datasets. The main colors include:\\n   - **Blue**: Represents a significant set of datasets, such as PubMed Central, ArXiv, and others, suggesting a strong emphasis on academic and research-related sources.\\n   - **Green**: Contains datasets like Pile-CC and StackExchange, hinting at additional diverse content.\\n   - **Orange and Yellow**: Highlight smaller datasets like Bibliotik, PG-19, and titles like Subtitles, indicating varied media sources.\\n\\n3. **Labels**: Each rectangle is labeled with the name of the dataset, such as \"PubMed Central,\" \"Wikipedia,\" \"Github,\" and others. Some labels suggest specific types of data (e.g., Creative Commons, educational or entertainment content).\\n\\n4. **Proportions**: The sizes of the rectangles reflect the significance of each dataset, either in terms of data volume or relevance to the research topic. Larger rectangles imply a more extensive dataset, while smaller ones may indicate niche sources.\\n\\n5. **Hierarchy**: The arrangement signifies a hierarchical structure, where larger rectangles are often positioned in a way to denote primary sources, while smaller ones branch off, indicating their secondary importance.\\n\\nThis visualization is useful for quickly understanding the composition of the datasets utilized in the transformers architecture research.'),\n",
       " Document(metadata={'doc_id': '1c8452a9-eedc-4f77-8eb0-1a1e9be68bd1'}, page_content=\"The image illustrates a simplified diagram of the transformer architecture, typically used in natural language processing and machine learning contexts. Here is a detailed description:\\n\\n1. **Rectangular Blocks**: The image features several vertical rectangular blocks, colored in a pale red hue. These blocks represent individual steps in the transformer model's architecture, which processes input data.\\n\\n2. **Arrows**: From the left side, there are several black arrows directed towards the red blocks. These arrows signify the flow of data or information between various components of the transformer architecture. \\n\\n3. **Connections**: The most notable aspect of the diagram is the presence of black arrows connecting the different red blocks. These arrows are directed diagonally and horizontally, indicating that information can flow in multiple directions. This illustrates the concept of attention mechanisms, whereby each block can attend to information from not just its immediate predecessor, but also from non-adjacent blocks, which is central to the functioning of transformers.\\n\\n4. **Layered Structure**: The arrangement of the blocks suggests a layering or sequential processing of information. This layered structure helps to capture contextual relationships in the data being processed.\\n\\nThe overall design is minimalistic but effectively conveys the complex interconnections that define the transformer model, emphasizing its ability to handle dependencies across varying distances in the input data.\"),\n",
       " Document(metadata={'doc_id': '810ca1f2-10cf-4bb8-a96a-3c756b382d14'}, page_content='The image depicts a schematic representation of the transformer architecture, often used in deep learning for natural language processing and other tasks. \\n\\n### Key Features of the Image:\\n\\n- **Rectangular Nodes**: The image features several vertical rectangular boxes, colored in a bright cyan. These likely represent input data or embeddings being processed through the transformer model.\\n\\n- **Black Arrows**: Between the cyan rectangles, there are multiple black arrows emanating from specific points, which represent the connections between different layers of the architecture. These arrows may indicate the flow of data or the attention mechanisms in play.\\n\\n- **Layer Structure**: The layout suggests a stacked configuration, typical of transformer models where input flows through multiple layers. The arrows crossing over multiple rectangles highlight the interconnectedness and parallel processing capabilities inherent in the transformer design.\\n\\n- **Abstract Representation**: The use of abstract shapes and colors indicates an emphasis on conceptual understanding rather than intricate detail. This aligns with visual explanations often found in research papers where complex architectures need simplifying for readability.\\n\\nOverall, the image succinctly communicates the core ideas of how data flows through the transformer architecture, emphasizing both the input layers and the interconnections that facilitate complex computation.'),\n",
       " Document(metadata={'doc_id': '93f66b0e-7e9a-4a92-a5d7-5b89108ba8ff'}, page_content=\"The image illustrates a simplified representation of the transformer architecture, commonly used in natural language processing and machine learning. It features two groups of rectangular nodes, with the left side depicted in light blue and the right in light red.\\n\\n### Key Components:\\n\\n- **Blue Nodes (Input Layer)**: The left side contains several blue rectangles that signify input embeddings or information segments. This likely represents the initial input layer where data is processed.\\n\\n- **Red Nodes (Output Layer)**: The right side includes red rectangles representing the output layer or transformed outputs generated by the model. \\n\\n- **Connections**: \\n  - There are multiple directed arrows (black lines) showing connections between the blue and red nodes, indicating the flow of information from the input layer to the output layer. \\n  - Some arrows connect the blue nodes with each other, suggesting internal relationships or processing steps within the input layer before reaching the output layer.\\n\\n### Arrangement:\\n- The blue nodes are aligned in a vertical stack on the left, while the red nodes are arranged in a similar vertical orientation on the right.\\n- The arrows are organized in a way that makes clear the directionality of the flow, with the outputs receiving information from multiple inputs, a characteristic of transformer models that rely on attention mechanisms to integrate contextual information.\\n\\n### Overall Purpose:\\nThis diagram serves to visualize the transformer architecture's layered approach, highlighting how inputs are processed and transformed into outputs through interconnected nodes. The focus here is on illustrating the relationships and dynamic flow of data within the architecture.\"),\n",
       " Document(metadata={'doc_id': '1e925e49-ba21-4509-9e18-386e31cda2b3'}, page_content=\"The image illustrates a conceptual representation of the Transformer architecture, commonly used in natural language processing and other fields. \\n\\n1. **Structure**: The layout consists of several vertically-aligned rectangular blocks, colored in a light red hue. Each block likely represents a layer or component of the Transformer model, such as an encoder or decoder.\\n\\n2. **Connections**: Black arrows connect the blocks, indicating the flow of information or data between different layers. The arrows vary in direction and are likely meant to represent the attention mechanism utilized in Transformers, where each layer can attend to multiple other layers.\\n\\n3. **Directionality**: Some arrows point upward towards the blocks above, while others point diagonally or horizontally, suggesting a complex interrelationship among layers that allows for parallel processing of input sequences.\\n\\n4. **Overall Implication**: The diagram emphasizes the model's architecture and the relationships between its components. The focus is on how the layers work together to process data effectively, highlighting the non-sequential nature of the Transformer model compared to traditional recurrent networks.\\n\\nIn summary, this image serves as a visual aid to understand the layered structure and connectivity of the Transformer architecture, crucial for comprehending its functionality in processing sequences.\"),\n",
       " Document(metadata={'doc_id': 'b5280e5a-1a3c-4759-b06f-1a958cd13538'}, page_content='The image is a schematic representation of the Transformer architecture, commonly used in natural language processing. Here\\'s a detailed breakdown:\\n\\n1. **Input Layer**: At the bottom, the image displays labeled \"Input embeddings\" with several red circles that appear to represent tokens or words from an input sequence. Each token is accompanied by a corresponding blue label indicating the word, such as \"So,\" \"long,\" \"and,\" \"thanks,\" \"for,\" \"all,\" and \"the.\"\\n\\n2. **Transformer Blocks**: Above the input layer, there is a large, shaded purple rectangle labeled \"Transformer Blocks.\" This indicates that the input embeddings are processed through multiple layers of Transformer blocks. Inside the rectangle, parallel horizontal lines or layers represent the individual processing steps or components within those blocks.\\n\\n3. **Connections and Flow**: Arrows connect the input embeddings to the Transformer blocks, suggesting the flow of data as the tokens are input into the model. Several arrows lead to specific components within the blocks, illustrating how the model pays attention to different words.\\n\\n4. **Output and Softmax**: To the right, there\\'s a section indicating the model output. Two bold red words, \"all\" and \"the,\" are highlighted. Arrows point from these output words to a smaller box that likely symbolizes a sampling mechanism (such as Softmax) where probabilities are determined. The label \"Sample from Softmax\" indicates that the model generates the next word based on learned probabilities from the context.\\n\\n5. **Linear Layer**: Adjacent to the Softmax sampling box, another box labeled \"linear layer\" suggests that before sampling, the model’s output goes through a linear transformation, which is a common component in neural networks.\\n\\n6. **Overall Structure**: The design visually communicates how input words are transformed sequentially through the architecture, culminating in the generation of output words. The arrows emphasize connections and dependencies between words, illustrating the self-attention mechanism that allows the model to consider the entire context of the input sequence when generating predictions.\\n\\nThis illustration effectively summarizes the key components and data flow within the Transformer architecture, making it accessible for readers familiarizing themselves with the concept.'),\n",
       " Document(metadata={'doc_id': '5f05853a-a64d-455f-a538-4066bd6dc4a6'}, page_content='The image depicts a graphical representation of the transformer architecture, commonly used in machine learning models. It features several vertical rectangles colored in light blue, which represent different layers or components of the transformer.\\n\\nIn the center, there are multiple black arrows that form a complex network of connections between the layers. These arrows symbolize the attention mechanism of the transformer, illustrating how information flows and is processed between different parts of the model. The layering suggests a multi-layered architecture, which is characteristic of transformers.\\n\\nOverall, the design is minimalist and focuses on conveying the structure and connectivity within the transformer model, highlighting its complexity and efficiency in processing data.'),\n",
       " Document(metadata={'doc_id': '597e944b-da72-400c-87c6-61030e45e8a9'}, page_content=\"The image illustrates a diagrammatic representation of the transformer architecture, which is crucial for understanding how transformer models process data.\\n\\n1. **Structure**: The diagram is divided into two main sections. The left side features a series of blue rectangles, while the right side consists of red rectangles. \\n\\n2. **Blue Rectangles**: The blue rectangles likely represent input embeddings or initial layers within the transformer model. These nodes appear to be connected through black arrows, which indicate directed relationships or flows of information between them.\\n\\n3. **Red Rectangles**: On the right side, the red rectangles likely signify the output layers or subsequent stages of processing in the transformer architecture. \\n\\n4. **Connections**: There are multiple black arrows originating from the blue rectangles and connecting to the red rectangles. This suggests that each node on the left can influence multiple nodes on the right, showcasing the attention mechanism where inputs can attend to different parts of the output.\\n\\n5. **Directional Arrows**: The arrows depict the flow of information from one layer to the next, emphasizing the model's capability to transform inputs into outputs by attending to various inputs in a non-sequential manner.\\n\\nOverall, the image demonstrates the core concept of the transformer architecture, highlighting how different layers interact and the flow of data through the network.\"),\n",
       " Document(metadata={'doc_id': '8c690fc8-d032-4e6d-9836-b9d4ecd93eb3'}, page_content=\"The image features a stylized and colorful representation of a person's face, divided into three distinct panels. Each panel showcases a similar portrait, but with differing color schemes that create a striking pop-art effect.\\n\\n1. **First Panel (Left)**: The background is a bright blue, contrasting with the vibrant red and yellow tones used for the face. The features are somewhat abstract, but recognizable, with prominent highlights and shadows enhancing the facial structure.\\n\\n2. **Second Panel (Middle)**: This panel presents a red background, accentuating the face in a blend of yellow and orange. The use of color creates a warm feel, and the detailing remains consistent with the first, continuing the theme of abstraction.\\n\\n3. **Third Panel (Right)**: Here, the background shifts to a vivid green, while the colors of the face remain bright, with a mix of yellow and red. This panel maintains the same artistic style, showcasing similar facial features in an abstract manner.\\n\\nOverall, the image employs bold colors and contrasts, creating an engaging visual experience that resonates with themes often found in pop art. The consistent facial representation across all three panels suggests an exploration of identity or perception through varied color interpretations.\"),\n",
       " Document(metadata={'doc_id': 'd54bdf28-49cf-4349-9be5-87a375ee299c'}, page_content='The image appears to depict a modern building façade featuring the iconic logo of \"The New York Times.\" The logo is displayed prominently against a backdrop of horizontal metal slats, which create a pattern that adds a dynamic element to the scene. The slats allow light to filter through, creating shadows that enhance the text\\'s visibility.\\n\\nIn the foreground, there may be an overlay containing text, possibly part of the research paper\\'s context or title. This text highlights an aspect related to training in a technical framework, likely connected to the transformers architecture being discussed in the paper.\\n\\nThe overall composition combines architectural features with textual elements, blending modern design with scholarly content. The use of black and white enhances the visual impact, giving it a clean and professional appearance, suitable for an academic setting.'),\n",
       " Document(metadata={'doc_id': '31dc607c-afa8-47dc-94f3-ec26d26ef0ac'}, page_content='The image depicts a bar plot representing the number of research papers published from 2018 to 2022, categorized by different sectors. \\n\\n### Graph Details:\\n\\n- **Y-Axis**: Labeled \"Number of Papers,\" indicating the quantity of research papers.\\n- **X-Axis**: Denotes the years from 2018 to 2022, with tick marks for each year.\\n\\n### Bars:\\n- Each bar represents the total number of papers published in a given year, divided into segments that correspond to different categories.\\n  \\n1. **2018**: \\n   - Total: 71 papers\\n   - Purple segment (Education): 63 papers\\n   - Teal segment (Industry): 8 papers\\n\\n2. **2019**: \\n   - Total: 166 papers\\n   - Purple (Education): 139 papers\\n   - Teal (Industry): 27 papers\\n\\n3. **2020**: \\n   - Total: 244 papers \\n   - Purple (Education): 200 papers\\n   - Teal (Industry): 44 papers\\n\\n4. **2021**: \\n   - Total: 302 papers \\n   - Purple (Education): 227 papers\\n   - Teal (Industry): 53 papers\\n\\n5. **2022**: \\n   - Total: 772 papers \\n   - Purple (Education): 503 papers\\n   - Teal (Industry): 181 papers\\n   - Light teal segment (Government): 70 papers\\n   - Dark teal segment (Nonprofit): 181 papers\\n   - Dark blue segment (Other): 70 papers\\n\\n### Color Coding:\\n- **Purple**: Represents the Education sector.\\n- **Teal (dark)**: Represents the Industry sector.\\n- **Teal (light)**: Represents the Government sector.\\n- **Dark teal**: Represents the Nonprofit sector.\\n- **Dark blue**: Represents other categories.\\n\\n### Data Labels:\\n- Each segment within the bars is labeled with the specific number of papers corresponding to that category for clarity.\\n\\n### Overall Trend:\\nThe graph indicates a significant increase in the number of research papers published over the years, particularly in the Education sector, culminating in a sharp rise in 2022.')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ff914",
   "metadata": {},
   "source": [
    "### Check retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "67793f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "docs = retriever.invoke(\n",
    "    \"who are the authors of the paper?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fbb5205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marra, F.; Gragnaniello, D.; Verdoliva, L.; Poggi, G.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Marra, F.; Gragnaniello, D.; Verdoliva, L.; Poggi, G.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Ribeiro, M.T,; Singh, S.; Guestrin, C.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Ribeiro, M.T,; Singh, S.; Guestrin, C.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4bb2c",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "cdd3019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base64 import b64decode\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            #context_text += text_element.text\n",
    "            context_text += text_element\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "aacfafc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer is a new network architecture based solely on attention mechanisms, which eliminates the need for recurrence and convolutions. It is designed for sequence transduction tasks, particularly in machine translation, where it has demonstrated superior performance in terms of quality while being faster to train compared to traditional models that rely on recurrent or convolutional layers. The Transformer employs multi-headed self-attention and has achieved state-of-the-art results on translation tasks, such as English-to-German and English-to-French. The architecture is noted for its parallelizability and efficiency, making it a promising approach for future applications beyond text, including images, audio, and video.'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848735b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
